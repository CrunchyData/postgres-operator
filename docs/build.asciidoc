= PostgreSQL Operator Build and Setup
:toc:
v2.1, {docdate}

== Table of Contents

== Overview

This document describes how to build from source code the
Postgres Operator.  If you don't want to build the images
from source, you can download them from the following:

 * Dockerhub (crunchydata/lspvc and crunchydata/postgres-operator images)
 * link:https://github.com/CrunchyData/postgres-operator/releases[Github Releases]  (pgo client and client configuration files, extracted to your $HOME)

Further details can be found in the link:design.asciidoc[PostgreSQL Operator Design] document on
how the operator is built and how it operates.

== Requirements

=== Prerequisites

These versions of Kubernetes and OpenShift are required due to the use of CustomResourceDefinitions which first emerged in
these versions.

* *Kubernetes 1.7.0+*
* *OpenShift Origin 1.7.0+*

The operator is developed with the Golang versions great than or equal to version 1.8  See
link:https://golang.org/dl/[Golang website] for details on installing golang.

Pre-compiled versions of the Operator *pgo* client are provided for the x86_64 and Mac OSX
hosts.

As of version 2.0, the Operator uses the following PostgreSQL containers:

* link:https://hub.docker.com/r/crunchydata/crunchy-postgres/[PostgreSQL 9.6+ Container] version 1.6.0 or later (e.g. centos7-9.6.6-1.7.0)
* link:https://hub.docker.com/r/crunchydata/crunchy-backup/[PostgreSQL Backup Container] version 1.6.0 or later (e.g. centos7-9.6.6-1.7.0)
* link:https://hub.docker.com/r/crunchydata/crunchy-upgrade/[PostgreSQL Upgrade Container] version 1.6.0 or later (e.g. centos7-9.6.6-1.7.0)

The Operator project builds and operates with the following containers:

* link:https://hub.docker.com/r/crunchydata/lspvc/[PostgreSQL PVC Listing Container]
* link:https://hub.docker.com/r/crunchydata/postgres-operator/[postgres-operator Container]
* link:https://hub.docker.com/r/crunchydata/apiserver/[apiserver Container]
* link:https://hub.docker.com/r/crunchydata/csvload/[csvload Container]

This Operator is developed on and has also been tested on the following operating systems:

* *CentOS 7*
* *RHEL 7*

=== Kubernetes Environment

To test the *postgres-operator*, it is required to have a Kubernetes cluster
environment.  The Operator is tested on Kubeadm Kubernetes installed clusters.  Other
Kubernetes installation methods have been known to work as well.

link:https://kubernetes.io/docs/setup/independent/install-kubeadm/[Installing kubeadm - Official Kubernetes Documentation]


=== Create Namespace

This example is based on a kubeadm installation with the admin
user being already created. The example below assumes the cluster name is *kubernetes* and the cluster default user is *kubernetes-admin*.
....
kubectl create -f $COROOT/examples/demo-namespace.json
kubectl get namespaces
....

On kubeadm, you'll need to let non-root users have access to the
kubeconfig admin directory and files as follows:
....
sudo chmod o+rwx /etc/kubernetes/
sudo chmod o+rwx /etc/kubernetes/admin.conf
....

then set your context to the new demo namespace
....
kubectl config set-context demo --namespace=demo --cluster=kubernetes --user=kubernetes-admin
kubectl config use-context demo
kubectl config current-context
....

Add a cluster role binding to allow the new namespace default service
account permissions to run the postgres-operator and create
the Custom Resource Definitions:

....
kubectl create clusterrolebinding serviceaccounts-cluster-admin \
  --clusterrole=cluster-admin \
    --group=system:serviceaccounts
....

*WARNING*:  the above RBAC command is very permissive, adjust this
to a scope that you require for your environment.

See link:https://kubernetes.io/docs/admin/authorization/rbac/[here] for more
details on how to enable RBAC roles and modify the scope of the permissions
to suit your needs.

There are 2 places you will need to update to specify your
namespace:

In the operator configuration file, $COROOT/conf/apiserver/pgo.yaml, you will add
the *demo* value for the *Namespace*:
....
Namespace:  demo
....

likewise, specify your *CO_NAMESPACE* environment variable will specify *demo*;

....
export CO_NAMESPACE=demo
....

== Installation

=== Create Project and Clone

Install some of the required dependencies:
....
yum -y install git gettext
....

In your .bashrc file, include the following:
....
export GOPATH=$HOME/odev
export GOBIN=$GOPATH/bin
export PATH=$PATH:$GOBIN
export COROOT=$GOPATH/src/github.com/crunchydata/postgres-operator
export CO_BASEOS=centos7
export CO_VERSION=2.2
export CO_IMAGE_TAG=$CO_BASEOS-$CO_VERSION
export CO_NAMESPACE=demo
export CO_CMD=kubectl
export CO_APISERVER_URL=http://postgres-operator:8080
....

The value of CO_APISERVER_URL is used by the *pgo* client to connect
to the postgres-operator *apiserver*.  This URL should include
either a DNS name for the postgres-operator service or it's Service
IP address.

Next, set up a project directory structure and pull down the project:
....
mkdir -p $HOME/odev/src $HOME/odev/bin $HOME/odev/pkg
mkdir -p $GOPATH/src/github.com/crunchydata/
cd $GOPATH/src/github.com/crunchydata
git clone https://github.com/CrunchyData/postgres-operator.git
cd postgres-operator
....

=== Pull Postgres Containers

The Operator works with the Crunchy Container Suite
containers, you can pre-pull them as follows:

For PostgreSQL version 9.6:
....
docker pull crunchydata/crunchy-postgres:centos7-9.6.6-1.7.0
docker pull crunchydata/crunchy-backup:centos7-9.6.6-1.7.0
docker pull crunchydata/crunchy-upgrade:centos7-9.6.6-1.7.0
....

For PostgreSQL version 10.1:
....
docker pull crunchydata/crunchy-postgres:centos7-10.1-1.7.0
docker pull crunchydata/crunchy-backup:centos7-10.1-1.7.0
docker pull crunchydata/crunchy-upgrade:centos7-10.1-1.7.0
....

At this point, you can choose one of two options to install the postgres-operator
itself:

* link:https://github.com/CrunchyData/postgres-operator/blob/master/docs/build.asciidoc#get-prebuilt-images[Get Pre-built Images]
* link:https://github.com/CrunchyData/postgres-operator/blob/master/docs/build.asciidoc#build-from-source[Build from source]

=== Get Prebuilt Images

At this point if you want to avoid building the images and binary
from source, you can pull down the Docker images as follows:
....
docker pull crunchydata/lspvc:centos7-2.2
docker pull crunchydata/csvload:centos7-2.2
docker pull crunchydata/postgres-operator:centos7-2.2
docker pull crunchydata/apiserver:centos7-2.2
....

Next get the *pgo* client, go to the Releases page and download the tar ball, uncompress it into your $HOME directory:
....
cd $HOME
wget https://github.com/CrunchyData/postgres-operator/releases/download/2.1/postgres-operator.2.2.tar.gz
tar xvzf ./postgres-operator.2.2.tar.gz
....

Lastly, add the *pgo* client into your PATH.

You are now ready to Deploy the operator to your Kube system.

=== Build from Source

Install a golang compiler, this can be done with either
your package manager or by following directions
from https://golang.org/dl/.  The operator is currently built
using golang version 1.8.X but also runs using golang version 1.9.X

Then install the project library dependencies, the godep dependency manager is used
as follows:
....
cd $COROOT
go get github.com/tools/godep
make setup
....

NOTE:  you will see errors and warnings from the *make setup* target, you
can ignore these when building release 2.1

==== Compiling the PostgreSQL Operator
....
cd $COROOT
make all
which pgo
....

=== Configure Persistent Storage

The default Operator configuration is defined to use a HostPath
persistence configuration.

There are example scripts provided that will create PV and PVC resources
that can be used in your testing.

These utilize HostPath and NFS volume types. Other types are not currently
supported, but can be manually defined.

To create sample HostPath Persistent Volumes and CLaims use the following scripts:
....
cd $COROOT/pv
./create-pv.sh
kubectl create -f ./crunchy-pvc.json
kubectl create -f ./csv-pvc.json
....

Note that this example will create a PVC called *crunchy-pvc* that is
referenced in the examples and *pgo.yaml* configuration file as the
desired PVC to use when databases and clusters are created.

=== Configure Basic Authentication

In Operator version 2.2, Basic Authentication is required by the *apiserver*.
You will configure the *pgo* client to specify a basic authentication
username and password by creating a file in the user's home
directory named *.pgouser* that looks similar to this:
....
testuser:testpass
....

This example specifies a username of *testuser* and a password of
*testpass*.  These values will be read by the *pgo* client and passed
to the *apiserver* on each REST API call.

For the *apiserver*, a list of usernames and passwords is
specified in the *apiserver-conf* ConfigMap.  The values specified
in a deployment are found in the following location:
....
$COROOT/conf/apiserver/pgouser
....

The sample configuration for *pgouser* is as follows:
....
username:password
testuser:testpass
....

Modify these values to be unique to your environment.

If the username and password passed by clients to the *apiserver* do
not match, the REST call will fail and a log message will be produced
in the *apiserver* container log.  The client will receive a 401 http
status code if they are not able to authenticate.

If the *pgouser* file is not found in the home directory of the *pgo* user
then the next searched location is */etc/pgo/pgouser*, and if not found
there then lastly the *PGOUSER* environment variable is searched for
a path to the basic authentication file.

=== Configuration

The *apiserver* uses the following  configuration files found in $COROOT/conf/apiserver to determine how the Operator will provision PostgreSQL containers:
....
$COROOT/conf/apiserver/pgo.yaml
$COROOT/conf/apiserver/pgo.lspvc-template.json
$COROOT/conf/apiserver/pgo.csvload-template.json
....

Note that the default *pgo.yaml* file assumes you are going to use *HostPath* Persistent Volumes for
your storage configuration.  Adjust this file for NFS or other storage configurations.

Note that the *pgo.yaml* configuration file assumes your Kubernetes configuration file is located in */etc/kubernetes/admin.conf*.  Update this kubeconfig
path to match your local Kubernetes configuration file location.

More in-depth explanations of postgres operator configurations are available
in the link:config.asciidoc[Configuration] document.

=== Deploy the PostgreSQL Operator
*NOTE*: This will create and use */data* on your
local system as the persistent store for the operator to use
for its persistent volume.
....
cd $COROOT
make deployoperator
kubectl get pod -l 'name=postgres-operator'
....

When you first run the operator, it will create the required
CustomResourceDefinitions. You can view these as follows:

....
kubectl get crd
....

Strategies for deploying the operator can be found in the link:design.asciidoc[PostgreSQL Operator Design] document.

=== Makefile Targets

The following table describes the Makefile targets:
.Makefile Targets
[width="40%",frame="topbot",options="header,footer"]
|======================
|Target | Description
|all        | compile all binaries and build all images
|setup        | fetch the dependent packages required to build with
|deployoperator        | deploy the Operator (apiserver and postgers-operator) to Kubernetes
|main        | compile the postgres-operator
|runmain        | locally execute the postgres-operator
|pgo        | build the pgo binary
|runpgo        | run the pgo binary
|runapiserver        | run the apiserver binary outside of Kube
|clean        | remove binaries and compiled packages, restore dependencies
|operatorimage        | compile and build the postgres-operator Docker image
|apiserverimage        | compile and build the apiserver Docker image
|lsimage        | build the lspvc Docker image
|csvloadimage        | build the csvload Docker image
|release        | build the postgres-operator release
|======================

=== Verify Installation

When you first run the operator, it will look for the presence of the
predefined custom resource definitions, and create them if not found.
The best way to verify a successful deployment of the Operator is by
viewing these custom resource definitions:

....
kubectl get crd
kubectl get pgclusters
kubectl get pgbackups
kubectl get pgupgrades
kubectl get pgpolicies
kubectl get pgpolicylogs
....

At this point, you should be ready to start using the *pgo* client!

== Performing a Smoke Test

A simple *smoke test* of the postgres operator includes testing
the following:

 * get version information (*pgo version*)
 * create a cluster (*pgo create cluster testcluster*)
 * scale a cluster (*pgo scale testcluster --replica-count=1*)
 * show a cluster (*pgo show cluster testcluster*)
 * show all clusters (*pgo show cluster all*)
 * backup a cluster (*pgo backup testcluster*)
 * show backup of cluster (*pgo show backup testcluster*)
 * show backup pvc of cluster (*pgo show pvc backup-testcluster-pvc*)
 * restore a cluster (*pgo create cluster restoredcluster --backup-pvc=testcluster-backup-pvc --backup-path=testcluster-backups/2017-01-01-01-01-01 --secret-from=testcluster*)
 * test a cluster (*pgo test restoredcluster*)
 * minor upgrade a cluster (*pgo upgrade testcluster*)
 * major upgrade a cluster (*pgo upgrade testcluster --upgrade-type=major*)
 * delete a cluster (*pgo delete cluster testcluster*)
 * create a policy from local file (*pgo create policy policy1 --in-file=./examples/policy/policy1.sql*)
 * create a policy from git repo (*pgo create policy gitpolicy --url=https://github.com/CrunchyData/postgres-operator/blob/master/examples/policy/gitpolicy.sql*)
 * repeat testing using emptydir storage type
 * repeat testing using create storage type
 * repeat testing using existing storage type
 * create a series of clusters  (*pgo create cluster myseries --series=2*)
 * apply labels at cluster creation (*pgo create cluster xraydb --series=2 --labels=project=xray*)
 * apply a label to an existing set of clusters (*pgo label --label=env=research --selector=project=xray*)
 * create a user for a given cluster (*pgo user --add-user=user0 --valid-days=30 --managed --db=userdb --selector=name=xraydb0*)
 * load a csv file into a cluster (*pgo load --load-config=./sample-load-config.json --selector=project=xray*)
 * extend a user's password allowed age (*pgo user --change-password=user1 --valid-days=10 --selector=name=xraydb1*)
 * drop user access (*pgo user --delete-user=user2 --selector=project=xray*)
 * check password age (*pgo user --expired=10 --selector=project=xray*)
 * backup an entire project (*pgo backup --selector=project=xray*)
 * delete an entire project (*pgo delete cluster --selector=project=xray*)

More detailed explanations of the commands can be found in the link:user-guide.asciidoc[User Guide].
