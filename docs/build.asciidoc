= PostgreSQL Operator Build and Setup
:toc:
v2.0.0, {docdate}

== Table of Contents

== Overview

This document describes how to build from source code the
Postgres Operator.  If you don't want to build the images
from source, you can download them from the following:

 * Dockerhub (crunchydata/lspvc and crunchydata/postgres-operator images)
 * link:https://github.com/CrunchyData/postgres-operator/releases[Github Releases]  (pgo client and client configuration files, extracted to your $HOME)

Further details can be found in the link:design.asciidoc[PostgreSQL Operator Design] document on
how the operator is built and how it operates.

== Requirements

=== Prerequisites

These versions of Kubernetes and OpenShift are required due to the use of CustomResourceDefinitions which first emerged in
these versions.

* *Kubernetes 1.7.0+*
* *OpenShift Origin 1.7.0+*

The operator is developed with the following specific version of Golang; you can find this distribution on the official
link:https://golang.org/dl/[Golang website]. Because Go binaries essentially have Go runtime bundled with them, it is
important to build on a platform that is compatible with the target deployment platform.

* *Golang 1.8.x*

The Operator makes use of the following containers:

* link:https://hub.docker.com/r/crunchydata/crunchy-postgres/[PostgreSQL 9.5+ Container]
* link:https://hub.docker.com/r/crunchydata/crunchy-backup/[PostgreSQL Backup Container]
* link:https://hub.docker.com/r/crunchydata/crunchy-upgrade/[PostgreSQL Upgrade Container]
* link:https://hub.docker.com/r/crunchydata/lspvc/[PostgreSQL PVC Listing Container]
* link:https://hub.docker.com/r/crunchydata/postgres-operator/[postgres-operator Container]

This Operator has also been tested on the following operating systems:

* *CentOS 7*
* *RHEL 7*

=== Kubernetes Environment

To test the *postgres-operator*, it is required to have a Kubernetes cluster
environment. A few different links are listed below that describe different
methods of installation.

link:https://kubernetes.io/docs/setup/independent/install-kubeadm/[Installing kubeadm - Official Kubernetes Documentation]

link:http://linoxide.com/containers/setup-kubernetes-kubeadm-centos/[Installing Kubernetes 1.5 with kubeadm on CentOS]

link:https://blog.openebs.io/setting-up-kubernetes-1-5-5-cluster-with-vagrant-dda11e33b5bc[Setting up Kubernetes 1.5.5 Cluster with Vagrant]

link:https://github.com/kubernetes/minikube[Minikube]

==== minikube

*Note*: If you set up your Kubernetes environment using *minikube*, it is important to
run the following command:

....
minikube addons disable default-storageclass
....

Minikube uses a hostpath provisioner that does not support supplementalGroups; because of this,
if you don't disable the default-storageclass, odd permission errors occur that stop the
PostgreSQL database from operating correctly.

==== kubeconfig

The kubeadm installation will create */etc/kubernetes/admin.conf* for
the kubeconfig file you will use to execute the *postgres-operator*. This
needs to be readable from your user account - to enable this, change
the permissions:
....
sudo chmod +r /etc/kubernetes/admin.conf
....

==== RBAC Permissions

*NOTE* - as of Kubernetes 1.6, RBAC security is enabled on most Kubernetes
installations.  With RBAC, the *postgres-operator* needs permissions
granted to it to enable CustomResourceDefinitions viewing.  You can grant the
*default* Service Account a cluster role as one way to enable
permissions for the operator. This coarse level of granting permissions
is not recommended for production. This command will enable
the *default* Service Account to have the *cluster-admin* role:
....
kubectl create clusterrolebinding permissive-binding \
	--clusterrole=cluster-admin \
	--user=admin \
	--user=kubelet \
       	--group=system:serviceaccounts:default
....

See link:https://kubernetes.io/docs/admin/authorization/rbac/[here] for more
details on how to enable RBAC roles.

== Installation

=== Create Project and Clone

Install some of the required dependencies:
....
yum -y install git gettext
....

In your .bashrc file, include the following:
....
export GOPATH=$HOME/odev
export GOBIN=$GOPATH/bin
export PATH=$PATH:$GOBIN
export COROOT=$GOPATH/src/github.com/crunchydata/postgres-operator
export CO_BASEOS=centos7
export CO_VERSION=2.0
export CO_IMAGE_TAG=$CO_BASEOS-$CO_VERSION
export CO_NAMESPACE=myproject
export CO_CMD=kubectl
....

It will be necessary to log out and back in for the changes to your .bashrc
file to take effect.

Next, set up a project directory structure and pull down the project:
....
mkdir -p $HOME/odev/src $HOME/odev/bin $HOME/odev/pkg
mkdir -p $GOPATH/src/github.com/crunchydata/
cd $GOPATH/src/github.com/crunchydata
git clone https://github.com/CrunchyData/postgres-operator.git
cd postgres-operator
....

At this point, you can choose one of two options to install the postgres-operator
itself:

* link:https://github.com/CrunchyData/postgres-operator/blob/master/docs/build.asciidoc#get-packaged-dependencies[Get packaged dependencies]
* link:https://github.com/CrunchyData/postgres-operator/blob/master/docs/build.asciidoc#build-from-source[Build from source]

=== Pull Postgres Containers

The Operator works with the Crunchy Container Suite
containers, you can pre-pull them as follows:
....
docker pull crunchydata/crunchy-postgres:centos7-10.0-1.6.0
docker pull crunchydata/crunchy-backup:centos7-10.0-1.6.0
docker pull crunchydata/crunchy-upgrade:centos7-10.0-1.6.0
....

For PostgreSQL version 9.6:
....
docker pull crunchydata/crunchy-postgres:centos7-9.6.5-1.6.0
docker pull crunchydata/crunchy-backup:centos7-9.6.5-1.6.0
docker pull crunchydata/crunchy-upgrade:centos7-9.6.5-1.6.0
....

=== Get Packaged Dependencies

At this point if you want to avoid building the images and binary
from source, you can pull down the Docker images as follows:
....
docker pull crunchydata/lspvc:centos7-2.0
docker pull crunchydata/csvload:centos7-2.0
docker pull crunchydata/postgres-operator:centos7-2.0
....

Next get the *pgo* client, go to the Releases page and download the tar ball, uncompress it into your $HOME directory:
....
cd $HOME
wget https://github.com/CrunchyData/postgres-operator/releases/download/2.0/postgres-operator.2.0.tar.gz
tar xvzf ./postgres-operator.2.0.tar.gz
....

Lastly, add the *pgo* client into your PATH.

You are now ready to Deploy the operator to your Kube system.

=== Build from Source

Install a golang compiler, this can be done with either
your package manager or by following directions
from https://golang.org/dl/.  The operator is currently built
using golang version 1.8.X


Then install the project library dependencies, the godep dependency manager is used
as follows:
....
cd $COROOT
go get github.com/tools/godep
make setup
....

==== Compiling the PostgreSQL Operator
....
cd $COROOT
make all
which pgo
....

=== Deploy the PostgreSQL Operator
*NOTE*: This will create and use */data* on your
local system as the persistent store for the operator to use
for its persistent volume.
....
cd $COROOT
make deployoperator
kubectl get pod -l 'name=postgres-operator'
....

When you first run the operator, it will create the required
CustomResourceDefinitions. You can view these as follows:

....
kubectl get crd
....

There are example scripts provided that will create PV and PVC resources
that can be used in your testing. These utilize HostPath and NFS volume
types. Other types are not currently supported, but can be manually defined.
If you do elect to use dynamic storage, it is not necessary to create the
PersistentVolume object in Kubernetes.

See the following scripts:
....
examples/operator/create-pv-nfs.sh
examples/operator/create-pv.sh
kubectl create -f examples/operator/crunchy-pvc.json
....

Note that this example will create a PVC called *crunchy-pvc* that is
referenced in the examples and *pgo* configuration file as the
desired PVC to use when databases and clusters are created.

Strategies for deploying the operator can be found in the link:design.asciidoc[PostgreSQL Operator Design] document.

=== Makefile Targets

The following table describes the Makefile targets:
.Makefile Targets
[width="40%",frame="topbot",options="header,footer"]
|======================
|Target | Description
|setup        | fetch the dependent packages required to build with
|deployoperator        | deploy the Operator to Kubernetes
|main        | compile the postgres-operator
|runmain        | locally execute the postgres-operator
|pgo        | build the pgo binary
|runpgo        | run the pgo binary
|clean        | remove binaries and compiled packages, restore dependencies
|operatorimage        | compile and build the postgres-operator Docker image
|lsimage        | build the lspvc Docker image
|csvloadimage        | build the csvload Docker image
|release        | build the postgres-operator release
|======================
=== Configuration

The *pgo* client requires three configuration files be copied
to your $HOME as follows:
....
cp $COROOT/examples/pgo.yaml.emptydir $HOME/.pgo.yaml
cp $COROOT/examples/pgo.lspvc-template.json $HOME/.pgo.lspvc-template.json
cp $COROOT/examples/pgo.csvload-template.json $HOME/.pgo.csvload-template.json
....

Alternatively, the configuration files can be located in these locations:

* . (current directory)
* $HOME
* /etc/pgo/

The .pgo.yaml file location is checked in that order.

Edit the .pgo.yaml file and change the following settings to match your current configuration:
....
KUBECONFIG:  /etc/kubernetes/admin.conf
....

The location of the LSPVC_TEMPLATE value is set by default to $HOME/$(whoami) as a location.
If this is not accurate, please change this value to match where the file is located.

....
LSPVC_TEMPLATE:  /home/*yourid*/.pgo.lspvc-template.json
....

Note that this configuration file assumes your Kubernetes configuration file is
located in */etc/kubernetes/admin.conf*.  Update this kubeconfig
path to match your local Kubernetes configuration file location.

More in-depth explanations of postgres operator configurations are available
in the link:config.asciidoc[Configuration] document.

=== Verify Installation

When you first run the operator, it will look for the presence
of the predefined custom resource definitions, and create them if not found. The best way to
verify pgo is up and running successfully is by viewing these custom resource definitions:

....
kubectl get crd
kubectl get pgclusters
kubectl get pgbackups
kubectl get pgupgrades
kubectl get pgpolicies
kubectl get pgpolicylogs
....

At this point, you should be ready to start using the *pgo* client!

== Performing a Smoke Test

A simple *smoke test* of the postgres operator includes testing
the following:

 * create a cluster (*pgo create cluster testcluster*)
 * scale a cluster (*pgo scale testcluster --replica-count=1*)
 * show a cluster (*pgo show cluster testcluster*)
 * show all clusters (*pgo show cluster all*)
 * backup a cluster (*pgo backup testcluster*)
 * show backup of cluster (*pgo show backup testcluster*)
 * show backup pvc of cluster (*pgo show backup testcluster --show-pvc*)
 * restore a cluster (*pgo create cluster restoredcluster --backup-pvc=testcluster-backup-pvc --backup-path=testcluster-backups/2017-01-01-01-01-01 --secret-from=testcluster*)
 * test a cluster (*pgo test restoredcluster*)
 * minor upgrade a cluster (*pgo upgrade testcluster*)
 * major upgrade a cluster (*pgo upgrade testcluster --upgrade-type=major*)
 * delete a cluster (*pgo delete cluster testcluster*)
 * create a policy from local file (*pgo create policy policy1 --in-file=./examples/policy/policy1.sql*)
 * create a policy from git repo (*pgo create policy gitpolicy --url=https://github.com/CrunchyData/postgres-operator/blob/master/examples/policy/gitpolicy.sql*)
 * repeat testing using emptydir storage type
 * repeat testing using create storage type
 * repeat testing using existing storage type
 * create a series of clusters  (*pgo create cluster myseries --series=2*)
 * apply policies at cluster creation (*pgo create cluster xraydb --series=2 --labels=project=xray --policies=xrayapp,rlspolicy*)
 * apply a label to an existing set of clusters (*pgo label --label=env=research --selector=project=xray*)
 * create a user for a given cluster (*pgo user --add-user=user0 --valid-days=30 --managed --db=userdb --selector=name=xraydb0*)
 * load a csv file into a cluster (*pgo load --load-config=./sample-load-config.json --selector=project=xray*)
 * extend a user's password allowed age (*pgo user --change-password=user1 --valid-days=10 --selector=name=xraydb1*)
 * drop user access (*pgo user --delete-user=user2 --selector=project=xray*)
 * check password age (*pgo user --expired=10 --selector=project=xray*)
 * backup an entire project (*pgo backup --selector=project=xray*)
 * delete an entire project (*pgo delete cluster --selector=project=xray*)

More detailed explanations of the commands can be found in the link:user-guide.asciidoc[User Guide].
