---
title: "5.0.3"
date:
draft: false
weight: 897
---


Crunchy Data announces the release of [Crunchy Postgres for Kubernetes](https://www.crunchydata.com/products/crunchy-postgresql-for-kubernetes/) 5.0.3.

Crunchy Postgres for Kubernetes is powered by [PGO](https://github.com/CrunchyData/postgres-operator), the open source [Postgres Operator](https://github.com/CrunchyData/postgres-operator) from [Crunchy Data](https://www.crunchydata.com). [PGO](https://github.com/CrunchyData/postgres-operator) is released in conjunction with the [Crunchy Container Suite](https://github.com/CrunchyData/container-suite).

Crunchy Postgres for Kubernetes 5.0.3 includes the following software versions upgrades:

- PostgreSQL 14 is now available.
- pgBackRest is updated to version 2.35.
- Patroni is updated to version 2.1.1.

Read more about how you can [get started]({{< relref "quickstart/_index.md" >}}) with Crunchy Postgres for Kubernetes. We recommend [forking the Postgres Operator examples](https://github.com/CrunchyData/postgres-operator-examples/fork) repo.

## Features

- Some network filesystems are sensitive to Linux user and group permissions. Process GIDs can now be configured through `PostgresCluster.spec.supplementalGroups` for when your PVs don't advertise their [GID requirements](https://kubernetes.io/docs/tasks/configure-pod-container/configure-persistent-volume-storage/#access-control).
- A replica service is now automatically reconciled for access to PostgreSQL replicas within a cluster.
- The PostgreSQL primary service and PgBouncer service can now each be configured to have either a `ClusterIP`, `NodePort` or `LoadBalancer` service type. Suggested by Bryan A. S. (@bryanasdev000).
- Pod Topology Spread Constraints can now be specified for PostgreSQL instances, the pgBackRest dedicated repository host as well as PgBouncer.
-  Existing `PGDATA`, Write-Ahead Log (WAL) and pgBackRest repository volumes can now be migrated from PGO v4 to PGO v5 by specifying an `existingVolumes` data source when creating a PostgresCluster.
- There is now a migration guide avaialble for moving Postgres clusters between PGO v4 to PGO v5.
- Custom resource requests and limits can now be configured for all `init` containers, therefore ensuring the desired Quality of Service (QoS) class can be assigned to the various Pods comprising a cluster.
- Custom resource requests and limits can now be configured for all Jobs created for a PostgresCluster.
- A Pod Priority Class can now be configured for the various Pods created for a PostgresCluster.
- An `imagePullPolicy` can now be configured for the various Pods created for a PostgresCluster.
- The pgAudit extension is now enabled by default in all clusters.
- Additional validation has been added to the various PVC definitions within the PostgresCluster spec to ensure successful PVC reconciliation.
- A custom SQL script can be configured to run when a PostgresCluster is initialized.

## Changes

- The supplemental group `65534` is no longer applied by default. Upgrading the operator will perform a rolling update on all PostgresClusters to remove it.

  If you need this GID for your network filesystem or you want to postpone the rollout, you need to perform the following steps when upgrading:

  1. Before deploying the new operator, deploy the new CRD. You can get the new CRD from the [Postgres Operator Examples](https://github.com/CrunchyData/postgres-operator-examples/fork) repository and executing the following command:
     ```console
     $ kubectl apply -k kustomize/install
     ```

  2. Add the group to your existing PostgresClusters:
     ```console
     $ kubectl edit postgrescluster/hippo

     kind: PostgresCluster
     …
     spec:
       supplementalGroups:
       - 65534
     …
     ```

     _or_

     ```console
$ kubectl patch postgrescluster/hippo --type=merge --patch='{"spec":{"supplementalGroups":[65534]}}'
     ```

     _or_

    by modifying `spec.supplementalGroups` in your manifest.

  3. Deploy the new operator. If you are using an up-to-date version of the manifest, you can run:
     ```console
     $ kubectl apply -k kustomize/install
     ```

  If you don't need the supplemental group, you can remove it and trigger a rolling update when convenient:

  ```console
  $ kubectl patch postgrescluster/hippo --type=merge --patch='{"spec":{"supplementalGroups":null}}'
  ```
- A dedicated pgBackRest repository host is now only deployed if a `volume` repo is configured.  This means that users with cloud-based (`s3`, `gcs` and/or `azure`) repos only will no longer see a dedicated repository host, nor will `SSHD` rub in any form within that PG cluster.  As a result of this change, the `spec.backups.pgbackrest.repoHost.dedicated` section is removed from the PostgresCluster spec, and all settings within it have been consolidated under section `spec.backups.pgbackrest.repoHost` (which is now only applicable if a dedicated repo host is actually deployed within the PG cluster).  Therefore, when upgrading please update the PostgresCluster spec to ensure any settings from section `spec.backups.pgbackrest.repoHost.dedicated` are moved into section `spec.backups.pgbackrest.repoHost`.
- The PGO documentation now includes an "Administrative Tasks" section, which includes instructions for manually restarting PostgreSQL and rotating TLS certificates.
- PgBouncer now uses SCRAM when authenticating into PostgreSQL.

## Fixes

- Validation for the PostgresCluster spec is updated to ensure at least one repo is always defined for section `spec.backups.pgbackrest.repos`.
- A restore will now complete successfully If `max_connections` and/or `max_worker_processes` is configured to a value higher than the default when backing up the PostgreSQL database. Reported by Tiberiu Patrascu (@tpatrascu).
- The installation documentation now properly defines how to set the `PGO_TARGET_NAMESPACE` environment variable for a single namespace installation.
- Ensure the full allocation of shared memory is available to Postgres containers.
