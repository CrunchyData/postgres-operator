= PostgreSQL Operator Documentation 
:toc:
v2.4, {docdate}

== Overview

This document describes how to build from source code the
Postgres Operator.  If you don't want to build the images
from source, you can download them from the following:

 * Dockerhub (crunchydata/lspvc and crunchydata/postgres-operator images)
 * link:https://github.com/CrunchyData/postgres-operator/releases[Github Releases]  (pgo client and client configuration files, extracted to your $HOME)

Further details can be found in the link:design.asciidoc[PostgreSQL Operator Design] document on
how the operator is built and how it operates.

== Requirements

=== Prerequisites

These versions of Kubernetes and OpenShift are required due to the use of CustomResourceDefinitions which first emerged in
these versions.

* *Kubernetes 1.7.0+*
* *OpenShift Origin 1.7.0+*

The operator is developed with the Golang versions great than or equal to version 1.8  See
link:https://golang.org/dl/[Golang website] for details on installing golang. 

Pre-compiled versions of the Operator *pgo* client are provided for the x86_64 and Mac OSX
hosts.

As of version 2.0, the Operator uses the following PostgreSQL containers:

* link:https://hub.docker.com/r/crunchydata/crunchy-postgres/[PostgreSQL 9.6+ Container] version 1.6.0 or later (e.g. centos7-10.1-1.7.0)
* link:https://hub.docker.com/r/crunchydata/crunchy-backup/[PostgreSQL Backup Container] version 1.6.0 or later (e.g. centos7-10.1-1.7.0)
* link:https://hub.docker.com/r/crunchydata/crunchy-upgrade/[PostgreSQL Upgrade Container] version 1.6.0 or later (e.g. centos7-10.1-1.7.0)
* link:https://hub.docker.com/r/crunchydata/crunchy-collect/[PostgreSQL Metrics Collection Container] version 1.6.0 or later (e.g. centos7-10.1-1.7.0)

The Operator project builds and operates with the following containers:

* link:https://hub.docker.com/r/crunchydata/pgo-lspvc/[PVC Listing Container]
* link:https://hub.docker.com/r/crunchydata/pgo-rmdata/[Remove Data Container]
* link:https://hub.docker.com/r/crunchydata/postgres-operator/[postgres-operator Container]
* link:https://hub.docker.com/r/crunchydata/pgo-apiserver/[apiserver Container]
* link:https://hub.docker.com/r/crunchydata/pgo-load/[file load Container]

This Operator is developed on and has also been tested on the following operating systems:

* *CentOS 7*
* *RHEL 7*

=== Kubernetes Environment

To test the *postgres-operator*, it is required to have a Kubernetes cluster
environment.  The Operator is tested on Kubeadm Kubernetes installed clusters.  Other
Kubernetes installation methods have been known to work as well.

link:https://kubernetes.io/docs/setup/independent/install-kubeadm/[Installing kubeadm - Official Kubernetes Documentation]


On kubeadm, you'll need to let non-root users have access to the
kubeconfig admin directory and files as follows:
....
sudo chmod o+rwx /etc/kubernetes/
sudo chmod o+rwx /etc/kubernetes/admin.conf
....

== Installation

=== Create Project and Clone

In your .bashrc file, include the following:
....
export GOPATH=$HOME/odev
export GOBIN=$GOPATH/bin
export PATH=$PATH:$GOBIN
export COROOT=$GOPATH/src/github.com/crunchydata/postgres-operator
export CO_BASEOS=centos7
export CO_VERSION=2.4
export CO_IMAGE_TAG=$CO_BASEOS-$CO_VERSION
export CO_NAMESPACE=demo
export CO_CMD=kubectl
export CO_APISERVER_URL=https://postgres-operator:8443
export PGO_CA_CERT=$COROOT/conf/apiserver/server.crt
export PGO_CLIENT_CERT=$COROOT/conf/apiserver/server.crt
export PGO_CLIENT_KEY=$COROOT/conf/apiserver/server.key
....

The value of CO_APISERVER_URL is used by the *pgo* client to connect
to the postgres-operator *apiserver*.  This URL should include
either a DNS name for the postgres-operator service or it's Service
IP address.

Next, set up a project directory structure and pull down the project:
....
mkdir -p $HOME/odev/src $HOME/odev/bin $HOME/odev/pkg
mkdir -p $GOPATH/src/github.com/crunchydata/
cd $GOPATH/src/github.com/crunchydata
git clone https://github.com/CrunchyData/postgres-operator.git
cd postgres-operator
....

At this point, you can choose one of two options to install the postgres-operator
itself:

* link:https://github.com/CrunchyData/postgres-operator/blob/master/docs/build.asciidoc#get-prebuilt-images[Get Pre-built Images]
* link:https://github.com/CrunchyData/postgres-operator/blob/master/docs/build.asciidoc#build-from-source[Build from source]

=== Pull Postgres Containers

The Operator works with the Crunchy Container Suite
containers, you can pre-pull them as follows:

For PostgreSQL version 10.1:
....
docker pull crunchydata/crunchy-postgres:centos7-10.1-1.7.0
docker pull crunchydata/crunchy-backup:centos7-10.1-1.7.0
docker pull crunchydata/crunchy-upgrade:centos7-10.1-1.7.0
....

For PostgreSQL version 9.6:
....
docker pull crunchydata/crunchy-postgres:centos7-9.6.6-1.7.0
docker pull crunchydata/crunchy-backup:centos7-9.6.6-1.7.0
docker pull crunchydata/crunchy-upgrade:centos7-9.6.6-1.7.0
....

=== Get Prebuilt Images

At this point if you want to avoid building the images and binary
from source, you can pull down the Docker images as follows:
....
docker pull crunchydata/pgo-lspvc:centos7-2.4
docker pull crunchydata/pgo-load:centos7-2.4
docker pull crunchydata/pgo-rmdata:centos7-2.4
docker pull crunchydata/postgres-operator:centos7-2.4
docker pull crunchydata/pgo-apiserver:centos7-2.4
....

Next get the *pgo* client, go to the Releases page and download the tar ball, uncompress it into your $HOME directory:
....
cd $HOME
wget https://github.com/CrunchyData/postgres-operator/releases/download/2.4/postgres-operator.2.4.tar.gz
tar xvzf ./postgres-operator.2.4.tar.gz
....

Lastly, add the *pgo* client into your PATH.

You are now ready to Deploy the operator to your Kube system.

=== Build from Source

Install a golang compiler, this can be done with either
your package manager or by following directions
from https://golang.org/dl/.  The operator is currently built
using golang version 1.8.X but also runs using golang version 1.9.X

Then install the project library dependencies, the godep dependency manager is used
as follows:
....
cd $COROOT
go get github.com/tools/godep
make setup
....

==== Compiling the PostgreSQL Operator
....
cd $COROOT
make all
which pgo
....

=== Create Namespace

This example is based on a kubeadm installation with the admin
user being already created. The example below assumes the cluster name is *kubernetes* and the cluster default user is *kubernetes-admin*.
....
kubectl create -f $COROOT/examples/demo-namespace.json
kubectl get namespaces
....
then set your context to the new demo namespace
....
sudo chmod o+w /etc/kubernetes
sudo chmod o+w /etc/kubernetes/admin.conf
kubectl config set-context demo --namespace=demo --cluster=kubernetes --user=kubernetes-admin
kubectl config use-context demo
kubectl config current-context
....

Add a cluster role binding to allow the new namespace default service
account permissions to run the postgres-operator and create
the Custom Resource Definitions:

....
$CO_CMD create clusterrolebinding serviceaccounts-cluster-admin \
  --clusterrole=cluster-admin \
    --group=system:serviceaccounts
....

*WARNING*:  the above RBAC command is very permissive, adjust this
to a scope that you require for your environment.

See link:https://kubernetes.io/docs/admin/authorization/rbac/[here] for more
details on how to enable RBAC roles and modify the scope of the permissions
to suit your needs.

There are 2 places you will need to update to specify your
namespace:

In the operator configuration file, $COROOT/conf/apiserver/pgo.yaml, you will add
the *demo* value for the *Namespace*:
....
Namespace:  demo
....

likewise, specify your *CO_NAMESPACE* environment variable will specify *demo*;

....
export CO_NAMESPACE=demo
....

=== Openshift Container Platform

To run the Operator on Openshift Container Platform note the following:

 * Openshift Container Platform 3.7 or greater is required since the Operator is based on Custom Resource Definitions which were first supported in OCP starting with version 3.7
 * the Openshift Project is synonymous with the CO_NAMESPACE environment variable setting
 * the OC_CMD environment variable should be set to *oc* when operating in an Openshift environment

=== Configure Persistent Storage

The default Operator configuration is defined to use a HostPath
persistence configuration.  

There are example scripts provided that will create PV and PVC resources
that can be used in your testing. 

These utilize HostPath and NFS volume types. Other types are not currently 
supported, but can be manually defined. 

To create sample HostPath Persistent Volumes and CLaims use the following scripts:
....
cd $COROOT/pv
./create-pv.sh
kubectl create -f ./crunchy-pvc.json
kubectl create -f ./csv-pvc.json
....

Note that this example will create a PVC called *crunchy-pvc* that is
referenced in the examples and *pgo.yaml* configuration file as the
desired PVC to use when databases and clusters are created.

=== Configure Basic Authentication

Starting in Operator version 2.3, Basic Authentication is required by the *apiserver*.
You will configure the *pgo* client to specify a basic authentication
username and password by creating a file in the user's home
directory named *.pgouser* that looks similar to this, containing only a single line:
....
testuser:testpass
....

This example specifies a username of *testuser* and a password of
*testpass*.  These values will be read by the *pgo* client and passed
to the *apiserver* on each REST API call.

For the *apiserver*, a list of usernames and passwords is
specified in the *apiserver-conf* ConfigMap.  The values specified
in a deployment are found in the following location:
....
$COROOT/conf/apiserver/pgouser
....

The sample configuration for *pgouser* is as follows:
....
username:password
testuser:testpass
....

Modify these values to be unique to your environment.  

If the username and password passed by clients to the *apiserver* do
not match, the REST call will fail and a log message will be produced
in the *apiserver* container log.  The client will receive a 401 http
status code if they are not able to authenticate.

If the *pgouser* file is not found in the home directory of the *pgo* user
then the next searched location is */etc/pgo/pgouser*, and if not found
there then lastly the *PGOUSER* environment variable is searched for
a path to the basic authentication file.

=== Configure TLS

As of Operator 2.3, TLS is used to secure communications to
the *apiserver*.  Sample keys/certs used by TLS are found
here:
....
$COROOT/conf/apiserver/server.crt
$COROOT/conf/apiserver/server.key
....

If you want to generate your own keys, you can use the script found in:
....
$COROOT/bin/make-certs.sh
....

The *pgo* client is required to use keys to connect to the *apiserver*.
Specify the keys to *pgo* by setting the following environment
variables:
....
export PGO_CA_CERT=$COROOT/conf/apiserver/server.crt
export PGO_CLIENT_CERT=$COROOT/conf/apiserver/server.crt
export PGO_CLIENT_KEY=$COROOT/conf/apiserver/server.key
....

The sample server keys are used as the client keys, adjust to suit
your requirements.

For the *apiserver* TLS configuration, the keys are included
in the *apiserver-conf* configMap when the *apiserver* is deployed.
See the $COROOT/deploy/deploy.sh script which is where the
configMap is created.

The *apiserver* listens on port 8443 (e.g. https://postgres-operator:8443).

=== Configuration

The *apiserver* uses the following  configuration files found in $COROOT/conf/apiserver to determine how the Operator will provision PostgreSQL containers:
....
$COROOT/conf/apiserver/pgo.yaml
$COROOT/conf/apiserver/pgo.lspvc-template.json
$COROOT/conf/apiserver/pgo.load-template.json
....

Note that the default *pgo.yaml* file assumes you are going to use *HostPath* Persistent Volumes for
your storage configuration.  Adjust this file for NFS or other storage configurations.

Note that the *pgo.yaml* configuration file assumes your Kubernetes configuration file is located in */etc/kubernetes/admin.conf*.  Update this kubeconfig
path to match your local Kubernetes configuration file location. 

The version of PostgreSQL container the Operator will deploy is determined
by the *CCPImageTag* setting in the *$COROOT/conf/apiserver/pgo.yaml* 
configuration file.  By default, this value is set to the latest
release of the Crunchy Container Suite.

More in-depth explanations of postgres operator configurations are available
in the link:config.asciidoc[Configuration] document.

=== Deploy the PostgreSQL Operator
*NOTE*: This will create and use */data* on your
local system as the persistent store for the operator to use
for its persistent volume.
....
cd $COROOT
make deployoperator
kubectl get pod -l 'name=postgres-operator'
....

You should see output similar to:
....
NAME                                 READY     STATUS    RESTARTS   AGE
postgres-operator-7f8db87c7b-4tk52   2/2       Running   0          8s
....

This output shows that both the *apiserver* and *postgres-operator* containers
are in ready state and the pod is running.

You can find the operator service IP address as follows:
....
kubectl get service postgres-operator
NAME                TYPE        CLUSTER-IP      EXTERNAL-IP   PORT(S)             AGE
postgres-operator   ClusterIP   10.105.56.167   <none>        8080/TCP,8443/TCP   1m
....

In this example, the *apiserver* is reachable at *https://10.105.56.167:8443*.


When you first run the operator, it will create the required
CustomResourceDefinitions. You can view these as follows:

....
kubectl get crd
....

Strategies for deploying the operator can be found in the link:design.asciidoc[PostgreSQL Operator Design] document.


=== Verify Installation

When you first run the operator, it will look for the presence of the 
predefined custom resource definitions, and create them if not found. 
The best way to verify a successful deployment of the Operator is by 
viewing these custom resource definitions:

....
kubectl get crd
kubectl get pgclusters
kubectl get pgbackups
kubectl get pgupgrades
kubectl get pgpolicies
kubectl get pgpolicylogs
....

At this point, you should be ready to start using the *pgo* client!  Be
sure to set the environment variable *CO_APISERVER_URL* to the DNS
name of the *postgres-operator* service or to the IP address of the
*postgres-operator* service IP address.  For example:

....
export CO_APISERVER_URL=https://10.105.56.167:8443
....

or if you have DNS configured on your client host:
....
export CO_APISERVER_URL=https://postgres-operator.demo.svc.cluster.local:8443
....


== Performing a Smoke Test

A simple *smoke test* of the postgres operator includes testing
the following:

 * get version information (*pgo version*)
 * create a cluster (*pgo create cluster testcluster*)
 * scale a cluster (*pgo scale testcluster --replica-count=1*)
 * show a cluster (*pgo show cluster testcluster*)
 * show all clusters (*pgo show cluster all*)
 * backup a cluster (*pgo backup testcluster*)
 * show backup of cluster (*pgo show backup testcluster*)
 * show backup pvc of cluster (*pgo show pvc testcluster-backup-pvc*)
 * restore a cluster (*pgo create cluster restoredcluster --backup-pvc=testcluster-backup-pvc --backup-path=testcluster-backups/2017-01-01-01-01-01 --secret-from=testcluster*)
 * test a cluster (*pgo test restoredcluster*)
 * minor upgrade a cluster (*pgo upgrade testcluster*)
 * major upgrade a cluster (*pgo upgrade testcluster --upgrade-type=major*)
 * delete a cluster (*pgo delete cluster testcluster --delete-data --delete-backups*)
 * create a policy from local file (*pgo create policy policy1 --in-file=./examples/policy/policy1.sql*)
 * create a policy from git repo (*pgo create policy gitpolicy --url=https://github.com/CrunchyData/postgres-operator/blob/master/examples/policy/gitpolicy.sql*)
 * repeat testing using emptydir storage type
 * repeat testing using create storage type
 * repeat testing using existing storage type
 * create a series of clusters  (*pgo create cluster myseries --series=2*)
 * apply labels at cluster creation (*pgo create cluster xraydb --series=2 --labels=project=xray*)
 * apply a label to an existing set of clusters (*pgo label --label=env=research --selector=project=xray*)
 * create a user for a given cluster (*pgo user --add-user=user0 --valid-days=30 --managed --db=userdb --selector=name=xraydb0*)
 * load a csv file into a cluster (*pgo load --load-config=./sample-load-config.json --selector=project=xray*)
 * extend a user's password allowed age (*pgo user --change-password=user0 --valid-days=10 --selector=name=xraydb1*)
 * drop user access (*pgo user --delete-user=user2 --selector=project=xray*)
 * check password age (*pgo user --expired=10 --selector=project=xray*)
 * backup an entire project (*pgo backup --selector=project=xray*)
 * delete an entire project (*pgo delete cluster --selector=project=xray*)
 * create a cluster with a crunchy-collect sidecar(*pgo create cluster testcluster --metrics*)

More detailed explanations of the commands can be found in the link:user-guide.asciidoc[User Guide].

== Makefile Targets

The following table describes the Makefile targets:
.Makefile Targets
[width="40%",frame="topbot",options="header,footer"]
|======================
|Target | Description
|all        | compile all binaries and build all images
|setup        | fetch the dependent packages required to build with
|deployoperator        | deploy the Operator (apiserver and postgers-operator) to Kubernetes
|main        | compile the postgres-operator 
|runmain        | locally execute the postgres-operator
|pgo        | build the pgo binary
|runpgo        | run the pgo binary 
|runapiserver        | run the apiserver binary outside of Kube
|clean        | remove binaries and compiled packages, restore dependencies
|operatorimage        | compile and build the postgres-operator Docker image
|apiserverimage        | compile and build the apiserver Docker image
|lsimage        | build the lspvc Docker image
|loadimage        | build the file load Docker image
|rmdataimage        | build the data deletion Docker image
|release        | build the postgres-operator release
|======================


== Operator Configuration

This document describes the configuration options
for the *PostgreSQL operator*.

=== pgo Client Configuration

Starting with Operator version 2.1, the *pgo.yaml* configuration
file is used solely by the *apiserver* and has no effect on the *pgo* client.  With this change, the Operator configuration is centralized to
the *apiserver* container which is deployed alongside the *postgres-operator* container.

Sample Operator configuration files for various storage configurations are located in the $COROOT/examples directory.

To configure the Operator, modify the settings found in
*$COROOT/conf/apiserver/pgo.yaml* to meet your project needs.  Typically
you will modify the storage and namespace settings.

==== pgo Configuration Format

The default pgo configuration file, included in
*$COROOT/conf/apiserver/pgo.yaml*, looks like this:

[source,yaml]
....
Namespace:  demo
Cluster:
  CCPImageTag:  centos7-10.1-1.7.0
  Port:  5432
  PrimaryPassword:  password
  User:  testuser
  Password:  password
  RootPassword:  password
  Database:  userdb
  PasswordAgeDays:  60
  PasswordLength:  8
  Strategy:  1
  Replicas:  0
PrimaryStorage:
  AccessMode:  ReadWriteMany
  Size:  200M
  StorageType:  create
BackupStorage:
  AccessMode:  ReadWriteMany
  Size:  200M
  StorageType:  create
ReplicaStorage:
  AccessMode:  ReadWriteMany
  Size:  200M
  StorageType:  create
Pgo:
  APIServerUrl:  https://localhost:8080
  LSPVCTemplate:  /config/pgo.lspvc-template.json
  CSVLoadTemplate:  /config/pgo.load-template.json
  COImagePrefix:  crunchydata
  COImageTag:  centos7-2.4
  Debug:  true
....

Values in the pgo configuration file have the following meaning:

.pgo Configuration File Definitions
[width="90%",cols="m,2",frame="topbot",options="header"]
|======================
|Setting | Definition
|Namespace        | the namespace the Operator will run within
|Cluster.CCPImageTag        |newly created containers will be based on this image version (e.g. centos7-10.1-1.7.0), unless you override it using the --ccp-image-tag command line flag
|Cluster.Port        | the PostgreSQL port to use for new containers (e.g. 5432)
|Cluster.PrimaryPassword        | the PostgreSQL primary user password, when specified, it will be stored in the secret holding the primary user credentials, if not specified the value will be generated
|Cluster.User        | the PostgreSQL normal user name
|Cluster.Password        | the PostgreSQL normal user password, when specified, it will be stored in the secret holding the normal user credentials, if not specified the value will be generated
|Cluster.RootPassword        | the PostgreSQL *postgres* user password, when specified, it will be stored in the secret holding the root user credentials, if not specified the value will be generated
|Cluster.Strategy        | sets the deployment strategy to be used for deploying a cluster, currently there is only strategy *1*
|Cluster.Replicas        | the number of cluster replicas to create for newly created clusters
|Cluster.Policies        | optional, list of policies to apply to a newly created cluster, comma separated, must be valid policies in the catalog
|Cluster.PasswordAgeDays        | optional, if set, will set the VALID UNTIL date on passwords to this many days in the future when creating users or setting passwords, defaults to 365 days
|Cluster.PasswordLength        | optional, if set, will determine the password length used when creating passwords, defaults to 8
|PrimaryStorage.Name        |for the primary PostgreSQL deployment, if set, the PVC to use for created databases, used when the storage type is *existing*
|PrimaryStorage.StorageClass        |for the primary PostgreSQL deployment, for a dynamic storage type, you can specify the storage class used for storage provisioning(e.g. standard, gold, fast)
|PrimaryStorage.AccessMode        |for the primary PostgreSQL deployment, the access mode for new PVCs (e.g. ReadWriteMany, ReadWriteOnce, ReadOnlyMany). See below for descriptions of these.
|PrimaryStorage.Size        |for the primary PostgreSQL deployment, the size to use when creating new PVCs (e.g. 100M, 1Gi)
|PrimaryStorage.StorageType        |for the primary PostgreSQL deployment, supported values are either *dynamic*, *existing*, *create*, or *emptydir*, if not supplied, *emptydir* is used
|PrimaryStorage.Fsgroup        | optional, if set, will cause a *SecurityContext* and *fsGroup* attributes to be added to generated Pod and Deployment definitions
|PrimaryStorage.SupplementalGroups        | optional, if set, will cause a SecurityContext to be added to generated Pod and Deployment definitions
|ReplicaStorage.Name        |for the replica PostgreSQL deployments, if set, the PVC to use for created databases, used when the storage type is *existing*
|ReplicaStorage.StorageClass        |for the replica PostgreSQL deployment, for a dynamic storage type, you can specify the storage class used for storage provisioning(e.g. standard, gold, fast)
|ReplicaStorage.AccessMode        |for the replica PostgreSQL deployment, the access mode for new PVCs (e.g. ReadWriteMany, ReadWriteOnce, ReadOnlyMany). See below for descriptions of these.
|ReplicaStorage.Size        |for the replica PostgreSQL deployment, the size to use when creating new PVCs (e.g. 100M, 1Gi)
|ReplicaStorage.StorageType        |for the replica PostgreSQL deployment, supported values are either *dynamic*, *existing*, *create*, or *emptydir*, if not supplied, *emptydir* is used
|ReplicaStorage.Fsgroup        | optional, if set, will cause a *SecurityContext* and *fsGroup* attributes to be added to generated Pod and Deployment definitions
|ReplicaStorage.SupplementalGroups        | optional, if set, will cause a SecurityContext to be added to generated Pod and Deployment definitions
|BackupStorage.Name        |for the backup job, if set, the PVC to use for holding backup files, used when the storage type is *existing*
|BackupStorage.StorageClass        |for the backup job, for a dynamic storage type, you can specify the storage class used for storage provisioning(e.g. standard, gold, fast)
|BackupStorage.AccessMode        |for the backup job, the access mode for new PVCs (e.g. ReadWriteMany, ReadWriteOnce, ReadOnlyMany). See below for descriptions of these.
|BackupStorage.Size        |for the backup job, the size to use when creating new PVCs (e.g. 100M, 1Gi)
|BackupStorage.StorageType        |for the backup job , supported values are either *dynamic*, *existing*, *create*, or *emptydir*, if not supplied, *emptydir* is used
|BackupStorage.Fsgroup        | optional, if set, will cause a *SecurityContext* and *fsGroup* attributes to be added to generated Pod and Deployment definitions
|BackupStorage.SupplementalGroups        | optional, if set, will cause a SecurityContext to be added to generated Pod and Deployment definitions
|Pgo.LSPVCTemplate        | the PVC lspvc template file that lists PVC contents
|Pgo.LoadTemplate        | the load template file used for load jobs
|Pgo.COImagePrefix        | image tag prefix to use for the Operator containers
|Pgo.COImageTag        | image tag to use for the Operator containers
|======================

*NOTE*: Regarding the PVC access mode variable; this is automatically set to ReadWriteMany but
you also have the option to set this to ReadWriteOnce or ReadOnlyMany. The definitions of these
are as follows:

* *ReadWriteMany* - mounts the volume as read-write by many nodes
* *ReadWriteOnce* - mounts the PVC as read-write by a single node
* *ReadOnlyMany* - mounts the PVC as read-only by many nodes

=== Operator Configuration (Server)

The operator is run as a Kubernetes Deployment on the Kubernetes cluster
within a namespace.

Execute the Makefile target *deployoperator* to deploy the Operator.

You can also create NFS PV(s) using the create-pv-nfs.sh script.

To enable DEBUG messages from the operator pod, set the *Debug* environment
variable to *true* within its deployment file *deployment.json*.

==== Operator Templates

The database and cluster Kubernetes objects that get created by the operator
are based on json templates that are added into the operator deployment
by means of a mounted volume.

The templates are located in the *$COROOT/conf/postgres-operator* directory
and get added into a config map which is mounted by the operator deployment.

==== Persistence

Different ways of handling storage are specified by a user in
the *.pgo.yaml* configuration file by specifying values within
the ReplicaStorage, PrimaryStorage, and BackupStorage settings.

The following StorageType values are possible:

 * *dynamic* - Currently not implemented, this will allow for dynamic
 provisioning of storage using a StorageClass.
 * *existing* - This setting allows you to use a PVC that already exists.
 For example, if you have a NFS volume mounted to a PVC, all PostgreSQL clusters
 can write to that NFS volume mount via a common PVC. When set, the Name
 setting is used for the PVC.
 * *create* - This setting allows for the creation of a new PVC for
 each PostgreSQL cluster using a naming convention of *clustername*-pvc*.
 When set, the *Size*, *AccessMode* settings are used in
 constructing the new PVC.
 * *emptydir* - If a StorageType value is not defined, *emptydir* is used by default.
 This is a volume type that’s created when a pod is assigned to a node and exists as
 long as that pod remains running on that node; it is deleted as soon as the pod is
 manually deleted or removed from the node.

The operator will create new PVCs using this naming convention:
*dbname-pvc* where *dbname* is the database name you have specified.  For
example, if you run:
....
pgo create cluster example1
....

It will result in a PVC being created named *example1-pvc* and in
the case of a backup job, the pvc is named *example1-backup-pvc*

There are currently 3 sample pgo configuration files provided
for users to use as a starting configuration:

 * pgo.yaml.emptydir - this configuration specifies *emptydir* storage
 to be used for databases
 * pgo.yaml.nfs - this configuration specifies *create* storage to
 be used, this is used for NFS storage for example where you want to
 have a unique PVC created for each database
 * pgo.yaml.dynamic - this configuration specifies *dynamic* storage
 to be used, namely a *storageclass* that refers to a dynamic provisioning
 strorage such as StorageOS or Portworx, or GCE.

== pgo Commands

Prior to using *pgo*, users will need to specify the
*postgres-operator* URL as follows:
....
kubectl get service postgres-operator
NAME                CLUSTER-IP      EXTERNAL-IP   PORT(S)    AGE
postgres-operator   10.104.47.110   <none>        8080/TCP   7m
export APISERVER_URL=https://10.104.47.110:8080
pgo version
....

=== Check Version

To see what version of pgo client and postgres-operator you are
running, use the following:
....
pgo version
....

=== Create Cluster

To create a database, use the following:
....
pgo create cluster mycluster
....

A more complex example is to create a *series* of clusters such
as:
....
pgo create cluster xraydb --series=3 --labels=project=xray --policies=xrayapp,rlspolicy
....

In the example above, we provision 3 clusters that have a number appended
into their resulting cluster name, apply a user defined label to each
cluster, and also apply user defined policies to each cluster after
they are created.

You can then view that database as:
....
pgo show cluster mydatabase
....

The output will give you the current status of the database pod
and the IP address of the database service.  If you have *postgresql*
installed on your test system you can connect to the
database using the service IP address:
....
psql -h 10.105.121.12 -U postgres postgres
....

You can view *all* databases using the special keyword *all*:
....
pgo show cluster all
....

You can filter the results based on the Postgres Version:
....
pgo show cluster all --version=9.6.2
....

You can also add metrics collection to a cluster by using the *--metrics*
command flag as follows:
....
pgo create cluster testcluster --metrics
....

This command flag causes a *crunchy-collect* container to be added to the
database cluster pod and enables metrics collection on that database pod.
For this to work, you will need to configure the Crunchy metrics
example as found in the Crunchy Container Suite.

=== Backup Cluster

You can start a backup job for a cluster as follows:
....
pgo backup mycluster
....

You can view the backup:
....
pgo show backup mycluster
....

You can view the backup along with a PVC listing:
....
pgo show backup mycluster --show-pvc=true
....

The output of the backup will list the backup snapshots
found in the backup PVC, for example:
....
backup job pods for cluster mycluster...
└── backup-mycluster-63fw1
└── mydatabase

database pod mycluster is found

├── mycluster-backups/2017-03-27-13-54-33
├── mycluster-backups/2017-03-27-13-56-49
└── mycluster-backups/2017-03-27-14-02-38
....

This output is important in that it can let you copy/paste
a backup snapshot path and use it for restoring a database or
essentially cloning a database with an existing backup archive.

For example, to restore a database from a backup archive:
....
pgo create cluster restoredb --backup-path=mycluster-backups/2017-03-27-13-56-49 --backup-pvc=crunchy-pvc --secret-from=mycluster
....

This will create a new database called *restoredb* based on the
backup found in *mycluster-backups/2017-03-27-13-56-49* and the
secrets of the *mycluster* cluster.

Selectors can be used to perform backups as well, for example:
....
pgo backup  --selector=project=xray
....

In this example, any cluster that matches the selector will cause
a backup job to be created.


=== Cluster Removal

You can remove a cluster by running:
....
pgo delete cluster restoredb
....

Note, that this command will not remove the PVC associated with
this cluster. 

Selectors also apply to the delete command as follows:
....
pgo delete cluster  --selector=project=xray
....

This command will cause any cluster matching the selector
to be removed. 

You can remove a cluster and it's data files by running:
....
pgo delete cluster restoredb --delete-data
....

You can remove a cluster, it's data files, and all backups by running:
....
pgo delete cluster restoredb --delete-data --delete-backups
....


=== Cluster Replication

When you create a Cluster, you will see in the output a variety of Kubernetes objects were created including:

 * a Deployment holding the master PostgreSQL database
 * a Deployment holding the replica PostgreSQL database
 * a service for the master database
 * a service for the replica databases

Since Postgres is a single-master database by design, the master
Deployment is set to a replica count of 1, it can not scale beyond 1.

The replica Deployment is set to an initial value of 0, you will
see there are 0 replica databases running.  Those replica databases
are in read-only mode, but you can scale up the number of replicas
beyond 0 if you need higher read scaling.  To set the number of 
replicas issue the following command:
....
pgo scale mycluster --replica-count=1
....

There are 2 service connections available to the postgres cluster, one is
to the master database which allows read-write SQL processing, and
the other is to the set of read-only replica databases.  The replica
service performs round-robin load balancing to the replica databases.

You can connect to the master database and verify that it is replicating
to the replica databases as follows:
....
psql -h 10.107.180.159 -U postgres postgres -c 'table pg_stat_replication'
....

You can view *all* clusters using the special keyword *all*:
....
pgo show cluster all
....

You can filter the results by Postgres version:
....
pgo show cluster all --version=9.6.2
....


=== Minor Cluster Upgrade

You can perform a minor Postgres version upgrade
of either a database or cluster as follows:
....
pgo upgrade mycluster
....

When you run this command, it will cause the operator
to delete the existing containers of the database or cluster
and recreate them using the currently defined Postgres
container image specified in your pgo configuration file.

The database data files remain untouched, only the container
is updated, this will upgrade your Postgres server version only.

=== Major Cluster Upgrade

You can perform a major Postgres version upgrade
of either a database or cluster as follows:
....
pgo upgrade mycluster --upgrade-type=major
....

When you run this command, it will cause the operator
to delete the existing containers of the database or cluster
and recreate them using the currently defined Postgres
container image specified in your pgo configuration file.

The database data files are converted to the new major Postgres
version as specified by the current Postgres image version
in your pgo configuration file.  Currently only a Postgres
9.5 to 9.6 upgrade is supported.

In this scenario, the upgrade is performed by the Postgres
pg_upgrade utility which is containerized in the *crunchydata/crunchy-upgrade*
container.  The operator will create a Job which runs the upgrade container,
using the existing Postgres database files as input, and output
the updated database files to a new PVC.

Once the upgrade job is completed, the operator will create the
original database or cluster container mounted with the new PVC
which contains the upgraded database files.

As the upgrade is processed, the status of the *pgupgrade* CRD is
updated to give the user some insight into how the upgrade is
proceeding.  Upgrades like this can take a long time if your
database is large.  The operator creates a watch on the upgrade
job to know when and how to proceed.

=== Viewing PVC Information

You can view the files on a PVC as follows:
....
pgo show pvc crunchy-pvc
....

In this example, the PVC is *crunchy-pvc*.  This command is useful
in some cases to examine what files are on a given PVC.

In the case where you want to list a specific path on a PVC
you can specify the path option as follows:
....
pgo show pvc crunchy-pvc --pvc-root=mycluster-backups
....

=== Viewing Passwords

You can view the passwords used by the cluster as follows:
....
pgo show cluster mycluster --show-secrets=true
....

Passwords are generated if not specified in your *pgo* configuration.

=== Overriding CCP_IMAGE_TAG

New clusters typically pick up the container image version to use
based on the *pgo* configuration file's CCP_IMAGE_TAG setting.  You
can override this value using the *--ccp-image-tag* command line
flag:
....
pgo create cluster mycluster --ccp-image-tag=centos7-9.6.5-1.6.0
....

Likewise, you can upgrade the cluster using a command line flag:
....
pgo upgrade mycluster --ccp-image-tag=centos7-9.6.5-1.6.0
pgo upgrade mycluster --upgrade-type=major --ccp-image-tag=centos7-9.6.5-1.6.0
....

=== Testing Database Connections

You can test the database connections to a cluster:
....
pgo test mycluster
....

This command will test each service defined for the cluster using
the postgres, master, and normal user accounts defined for the
cluster.  The cluster credentials are accessed and used to test
the database connections.  The equivalent *psql* command is printed
out as connections are tried, along with the connection status.

=== Clone Cluster

To create a cluster clone from an existing cluster, use the following:
....
pgo clone mycluster --name=myclone
....

When you execute the *clone*, it will orchestrate the following
actions:

 * create a PgClone CRD to start the cloning workflow
 * create a replica on *mycluster* that will become the clone master
 * watch for cloned replicas to complete their replication
 * copy the original cluster's secrets for the clone to use
 * create a PgCluster CRD for the new clone cluster
 * create the Postgres trigger file on the clone to cause it to recover
   and become a valid read-write master
 * remove the PgClone CRD

NOTE:  if you are cloning a cluster that has replica(s) which 
specify a /pgdata volume using emptyDir volume type, then the cloned
master will inherit that same emptydir volume for the new cloned
master.  If this is not the behavior you want, you will need to specify
a volume type for the replica(s) that is based on either shared
volume types or dynamic volumes.

=== Create and View Policy

To create a policy use the following syntax:
....
pgo create policy policy1 --in-file=/tmp/policy1.sql
pgo create policy policy1 --url=https://someurl/policy1.sql
....

When you execute this command, it will create a policy named *policy1*
using the input file */tmp/policy1.sql* as input.  It will create
on the server a PgPolicy CRD with the name *policy1* that you can 
examine as follows:

....
kubectl get pgpolicies policy1 -o json
....

Policies get automatically applied to any cluster you create if 
you define in your *pgo.yaml* configuration a CLUSTER.POLICIES
value.  Policy SQL is executed as the *postgres* user.  

To view policies:
....
pgo show policy all
....


=== Apply Policy

To apply an existing policy to a set of clusters, issue
a command like this:
....
pgo apply policy1 --selector=name=mycluster
....

When you execute this command, it will look up clusters that
have a label value of *name=mycluster* and then it will apply
the *policy1* label to that cluster and execute the policy
SQL against that cluster using the *postgres* user account.

WARNING:  policies are executed as the superuser in PostgreSQL therefore
take caution when using them.

=== User Management - Add a User

To create a new Postgres user to the *mycluster* cluster, execute:
....
pgo user --add-user=sally --selector=name=mycluster
....

To add that user to a all clusters:
....
pgo user --add-user=sally 
....

=== User Management - Delete a User

To delete a Postgres user in the *mycluster* cluster, execute:
....
pgo user --delete-user=sally --selector=name=mycluster
....

To delete that user in all clusters:
....
pgo user --delete-user=sally 
....

=== User Management - Change Password

To change the password for a user in the *mycluster* cluster:
....
pgo user --change-password=sally --selector=name=mycluster
....

To change the password for the *sally* user in all clusters:
....
pgo user --change-password=sally 
....

The password is generated and printed in cleartext on your screen.

=== User Management - Show Expired Passwords

To see user passwords that have expired past a certain number
of days in the *mycluster* cluster:
....
pgo user --expired=7 --selector=name=mycluster
....

The same command across all clusters:
....
pgo user --expired=7 
....

=== User Management - Update Expired Passwords

To assign users to a cluster:
....
pgo user --add-user=user1 --valid-days=30 --managed --db=userdb --selector=name=xraydb1
....

In this example, a user named *user1* is created with a *valid until* password date set to expire in 30 days.  That user will be granted access to the *userdb* database.  This user account also will have an associated *secret* created to hold the password that was generated for this user.  Any clusters that match the selector value will have this user created on it.

To change a user password:
....
pgo user --change-password=user1 --valid-days=10 --selector=name=xray1
....

In this example, a user named *user1* has its password changed to a generated
value and the *valid until* expiration date set to 10 days from now, this
command will take effect across all clusters that match the selector.  If you
specify *valid-days=-1* it will mean the password will not expire (e.g. infinity).

To drop a user:
....
pgo user --delete-user=user3   --selector=project=xray
....

To see which passwords are set to expire in a given number of days:
....
pgo user --expired=10  --selector=project=xray
....

To check password age:
....
pgo user --expired=10  --selector=project=xray
....

In this example, any clusters that match the selector are queried to see
if any users are set to expire in 10 days.

To update expired passwords in a cluster:
....
pgo user --update-passwords --selector=name=mycluster
....

The same command across all clusters:
....
pgo user --update-passwords
....

=== Label Management

You can apply a user defined label to a cluster as follows:
....
pgo label --label=env=research  --selector=project=xray
....

In this example, we apply a label of *env=research* to any
clusters that have an existing label of *project=xray* applied.

=== Data Loading

A CSV file loading capability is supported currently.  You can
test that by creating a SQL Policy which will create a database
table that will be loaded with the CSV data.  For example:

....
pgo create policy xrayapp --in-file=$COROOT/examples/policy/xrayapp.sql
....

Then you can load a sample CSV file into a database as follows:

....
pgo load --load-config=$COROOT/examples/sample-load-config.json  --selector=name=mycluster
....

The loading is based on a load definition found in the *sample-load-config.json* file.  In that file, the data to be loaded is specified. When the *pgo load* command is executed, Jobs will be created to perform the loading for each cluster that matches the selector filter.

If you include the *--policies* flag, any specified policies will be applied prior to the data being loaded.  For
example:
....
pgo load --policies="rlspolicy,xrayapp" --load-config=$COROOT/examples/sample-load-config.json --selector=name=mycluster 
....

Likewise you can load a sample json file into a database as follows:
....
pgo load --policies=jsonload --load-config=$COROOT/examples/sample-json-load-config.json  --selector=name=mycluster
....

The load configuration file has the following YAML attributes:

.Load Configuration File Definitions
[width="90%",cols="m,2",frame="topbot",options="header"]
|======================
|COImagePrefix|  the pgo-load image prefix to use for the load job
|COImageTag|  the pgo-load image tag to use for the load job
|DbDatabase|  the database schema to use for loading the data
|DbUser|  the database user to use for loading the data
|DbPort|  the database port of the database to load
|TableToLoad|  the PostgreSQL table to load
|FilePath|  the name of the file to be loaded
|FileType|  either csv or json, determines the type of data to be loaded
|PVCName|  the name of the PVC that holds the data file to be loaded
|SecurityContext| either fsGroup or SupplementalGroup values
|======================

=== bash Completion

There is a bash completion file that is included for users to try, this
is located in the repo at *example/pgo-bash-completion*.  To use it, copy
that file to /etc/bash_completion.d/pgo, and log out and back into your
bash shell to try it out.   

=== Label Selectors on Cluster Listing

You can create and assign a label to a *pgcluster* CRD object
as follows:
....
kubectl label pgclusters mycluster name=thebest
kubectl get pgclusters --show-lables
....

This allows the user to define any user metadata they desire
for any set of *pgcluster* objects.  You can then select a
subset of *pgcluster* objects using a command like this:
....
pgo show cluster all --selector=name=thebest
....

This gives user another way to query the clusters using their
own metadata labeling scheme applied to *pgcluster* objects.
