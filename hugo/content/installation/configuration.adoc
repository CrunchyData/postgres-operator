---
title: "Configuration"
date: 2018-04-24T18:26:56-07:00
draft: false
weight: 40
---

:toc:
Latest Release: 3.5.0 {docdate}

== Overview

This document describes how to configure the operator beyond the default configurations in addition to detailing what the configuration settings mean.

== Openshift Container Platform

To run the Operator on Openshift Container Platform note the following requirements -

 * Openshift Container Platform 3.7 or greater is required due to the dependence on Custom Resource Definitions.
 * The `CO_CMD` environment variable should be set to `oc` when operating in an Openshift environment.

== Security Configuration

=== Kube RBAC

The `cluster-rbac.yaml` file is executed a single time when installing
the Operator.  This file, executed by a Kubernetes user with cluster-admin
priviledges, does the following:

 * Creates Customer Resource Definitions
 * Grants `get` access to Kube Node resources to the postgres-operator
   service account.

The `rbac.yaml` file is also executed a single time when installing
the Operator.  This file creates Role scoped privileges which are
granted to the postgres-operator service account.  The postgres-operator
service account is used by the *apiserver* and *postgres-operator* containers
to access Kubernetes resources.

Both of these RBAC files are executed by the `deploy/install-rbac.sh`
script. It can also be installed through running `make installrbac` in the
$CCPROOT directory.

[WARNING]
====
The CO_NAMESPACE environment variable determines the namespace
that is used within the deployment of the operator.  If you
are deploying to the *demo* namespace, the following
should setting should be defined in your .bashrc:
`export CO_NAMESPACE=demo`
====

{{% notice tip %}}
See link:https://kubernetes.io/docs/admin/authorization/rbac/[here] for more
details on how to enable RBAC roles and modify the scope of the permissions
to suit your needs.
{{% /notice %}}

=== Basic Authentication

Basic authentication between the host and the apiserver is required. It will
be necessary to configure the pgo client to specify a basic authentication
username and password through the creation a file in the user's home directory
named `.pgouser`. It will look similar to this, and contain only a single line -
....
username:password
....

The above excerpt specifies a username of *username* and a password of *password*.
These values will be read by the *pgo* client and passed to the *apiserver* on each
REST API call.

For the *apiserver*, a list of usernames and passwords is specified in the
*pgo-auth-secret* Secret.  The values specified in a deployment are found in
the following location -
....
$COROOT/conf/postgres-operator/pgouser
....

The sample configuration for `pgouser` is as follows -
....
username:password:pgoadmin
testuser:testpass:pgoadmin
readonlyuser:testpass:pgoreader
....

Modify these values to be unique to your environment.

If the username and password passed by clients to the *apiserver* do
not match, the REST call will fail and a log message will be produced
in the *apiserver* container log. The client will receive a 401 HTTP
status code if they are not able to authenticate.

If the `pgouser` file is not found in the *home* directory of the pgo user
then the next searched location is `/etc/pgo/pgouser`. If the file is not
found in either of the locations, the pgo client searches for the existence
of a `PGOUSER` environment variable in order to locate a path to the basic
authentication file.

Basic authentication can be entirely disabled by setting the BasicAuth
setting in the `pgo.yaml` configuration file to `false`.

=== Configure TLS

TLS is used to secure communications to the apiserver. Sample keys and
certifications that can be used by TLS are found here -
....
$COROOT/conf/postgres-operator/server.crt
$COROOT/conf/postgres-operator/server.key
....

If you want to generate your own keys, you can use the script found in -
....
$COROOT/bin/make-certs.sh
....

The *pgo* client is required to use keys to connect to the *apiserver*.
Specify the keys for pgo by setting the following environment variables -
....
export PGO_CA_CERT=$COROOT/conf/postgres-operator/server.crt
export PGO_CLIENT_CERT=$COROOT/conf/postgres-operator/server.crt
export PGO_CLIENT_KEY=$COROOT/conf/postgres-operator/server.key
....

You can also specify these credentials using the following
command flags where you can reference they keys from any file path directly:
....
pgo version --pgo-ca-cert=/tmp/server.crt --pgo-client-cert=/tmp/server.crt --pgo-client-key=/tmp/server.key
....

The sample server keys are used as the client keys; adjust to suit
security requirements.

For the *apiserver TLS configuration*, the keys are included in the
*apiserver-conf-secret* Secret when the apiserver is deployed. See the
`$COROOT/deploy/deploy.sh script` which is where the secret is created.

The apiserver listens on port 8443 (e.g. https://postgres-operator:8443)
by default.

You can set `InsecureSkipVerify` to *true* by setting the `NO_TLS_VERIFY`
environment variable in the `deployment.json` file to *true*. By default
this value is set to *false* if you do not specify a value.

=== pgo RBAC

The pgo command line utility talks to the apiserver REST API instead of
the Kubernetes API. It is therefore necessary for the pgo client to make
use of RBAC configuration.

Starting in Release 3.0, the */conf/postgres-operator/pgorole* is used to define some sample pgo roles, *pgadmin* and *pgoreader*.

These roles are meant as examples that you can configure to suit security
requirements as necessary. The *pgadmin* role grants a user authorization to
all pgo commands. The *pgoreader* only grants access to pgo commands that
display information such as `pgo show cluster`.

The `pgorole` file is read at start up time when the operator is deployed to
the Kubernetes cluster.

Also, the `pgouser` file now includes the role that is assigned to a specific
user as follows -
....
username:password:pgoadmin
testuser:testpass:pgoadmin
readonlyuser:testpass:pgoreader
....

The following list shows the current complete list of possible pgo
permissions -

.pgo Permissions
[width="60%",frame="topbot",options="header"]
|======================
|Permission | Description
|ShowSecrets   | allow *pgo show user*
|ShowCluster   | allow *pgo show cluster*
|CreateCluster | allow *pgo create cluster*
|TestCluster   | allow *pgo test mycluster*
|ShowBackup    | allow *pgo show backup*
|CreateBackup  | allow *pgo backup mycluster*
|DeleteBackup  | allow *pgo delete backup mycluster*
|Label         | allow *pgo label*
|Load          | allow *pgo load*
|CreatePolicy  | allow *pgo create policy*
|DeletePolicy  | allow *pgo delete policy*
|ShowPolicy    | allow *pgo show policy*
|ApplyPolicy   | allow *pgo apply policy*
|ShowPVC       | allow *pgo show pvc*
|CreateUpgrade | allow *pgo upgrade*
|ShowUpgrade   | allow *pgo show upgrade*
|DeleteUpgrade | allow *pgo delete upgrade*
|CreateUser    | allow *pgo create user*
|CreateFailover| allow *pgo failover*
|ShowConfig    | allow *pgo show config*
|User          | allow *pgo user*
|Version       | allow *pgo version*
|======================

If the user is unauthorized for a pgo command, the user will
get back this response -
....
FATA[0000] Authentication Failed: 40
....


=== REST API Configuration

The postgres-operator pod includes the apiserver which is a REST API that pgo
users are able to communicate with.

The apiserver uses the following configuration files found in `$COROOT/conf/postgres-operator` to determine how the Operator will provision PostgreSQL containers -
....
$COROOT/conf/postgres-operator/pgo.yaml
$COROOT/conf/postgres-operator/pgo.lspvc-template.json
$COROOT/conf/postgres-operator/pgo.load-template.json
....

Note that the default pgo.yaml file assumes you are going to use *HostPath* Persistent
Volumes for your storage configuration. It will be necessary to adjust this file for NFS
or other storage configurations. Some examples of how are listed in the manual installation
document.

The version of PostgreSQL container the Operator will deploy is determined by the *CCPImageTag*
setting in the `$COROOT/conf/postgres-operator/pgo.yaml` configuration file. By default, this value is
set to the latest release of the Crunchy Container Suite.

The default pgo.yaml configuration file, included in `$COROOT/conf/postgres-operator/pgo.yaml`,
looks like this -

[source,yaml]
....
Cluster:
  PrimaryNodeLabel:
  ReplicaNodeLabel:
  CCPImagePrefix:  crunchydata
  Metrics:  false
  Badger:  false
  CCPImageTag:  centos7-10.6-2.2.0
  LogStatement:  none
  LogMinDurationStatement:  60000
  Port:  5432
  User:  testuser
  Database:  userdb
  PasswordAgeDays:  60
  PasswordLength:  8
  Strategy:  1
  Replicas:  0
  ArchiveMode:  false
  ArchiveTimeout:  60
  ServiceType:  ClusterIP
  Backrest:  false
  Autofail:  false
  AutofailReplaceReplica:  false
PrimaryStorage: hostpathstorage
BackupStorage: hostpathstorage
ReplicaStorage: hostpathstorage
Storage:
  hostpathstorage:
    AccessMode:  ReadWriteMany
    Size:  1G
    StorageType:  create
  nfsstorage:
    AccessMode:  ReadWriteMany
    Size:  1G
    StorageType:  create
    SupplementalGroups:  65534
  storage2:
    AccessMode:  ReadWriteMany
    Size:  1G
    StorageType:  dynamic
    StorageClass:  gluster-heketi
    Fsgroup:  26
  storage3:
    AccessMode:  ReadWriteOnce
    Size:  1G
    StorageType:  dynamic
    StorageClass:  rook-ceph-block
    Fsgroup:  26
DefaultContainerResources:
DefaultLoadResources:
DefaultLspvcResources:
DefaultRmdataResources:
DefaultBackupResources:
DefaultPgbouncerResources:
DefaultPgpoolResources:
ContainerResources:
  small:
    RequestsMemory:  512Mi
    RequestsCPU:  0.1
    LimitsMemory:  512Mi
    LimitsCPU:  0.1
  large:
    RequestsMemory:  2Gi
    RequestsCPU:  2.0
    LimitsMemory:  2Gi
    LimitsCPU:  4.0
Pgo:
  AutofailSleepSeconds:  9
  Audit:  false
  LSPVCTemplate:  /pgo-config/pgo.lspvc-template.json
  LoadTemplate:  /pgo-config/pgo.load-template.json
  COImagePrefix:  crunchydata
  COImageTag:  centos7-3.4.0-rc1
....

Values in the pgo configuration file have the following meaning:

.pgo Configuration File Definitions
[width="90%",cols="m,2",frame="topbot",options="header"]
|======================
|Setting | Definition
|BasicAuth        | if set to *true* will enable Basic Authentication
|Cluster.PrimaryNodeLabel        |newly created primary deployments will specify this node label if specified, unless you override it using the --node-label command line flag, if not set, no node label is specifed
|Cluster.ReplicaNodeLabel        |newly created replica deployments will specify this node label if specified, unless you override it using the --node-label command line flag, if not set, no node label is specifed
|Cluster.CCPImageTag        |newly created containers will be based on this image version (e.g. centos7-10.4-1.8.3), unless you override it using the --ccp-image-tag command line flag
|Cluster.Port        | the PostgreSQL port to use for new containers (e.g. 5432)
|Cluster.LogStatement        | postgresql.conf log_statement value (required field) (works with crunchy-postgres >= 2.2.0)
|Cluster.LogMinDurationStatement        | postgresql.conf log_min_duration_statement value (required field) (works with crunchy-postgres >= 2.2.0)
|Cluster.User        | the PostgreSQL normal user name
|Cluster.Strategy        | sets the deployment strategy to be used for deploying a cluster, currently there is only strategy *1*
|Cluster.Replicas        | the number of cluster replicas to create for newly created clusters
|Cluster.Metrics        | boolean, if set to true will cause each new cluster to include crunchy-collect as a sidecar container for metrics collection, if set to false (default), users can still add metrics on a cluster-by-cluster basis using the pgo command flag --metrics
|Cluster.Badger        | boolean, if set to true will cause each new cluster to include crunchy-pgbadger as a sidecar container for static log analysis, if set to false (default), users can still add pgbadger on a cluster-by-cluster basis using the pgo create cluster command flag --pgbadger
|Cluster.Policies        | optional, list of policies to apply to a newly created cluster, comma separated, must be valid policies in the catalog
|Cluster.PasswordAgeDays        | optional, if set, will set the VALID UNTIL date on passwords to this many days in the future when creating users or setting passwords, defaults to 60 days
|Cluster.PasswordLength        | optional, if set, will determine the password length used when creating passwords, defaults to 8
|Cluster.ArchiveMode        | optional, if set to true will enable archive logging for all clusters created, default is false.
|Cluster.ArchiveTimeout        | optional, if set, will determine the archive timeout setting used when ArchiveMode is true, defaults to 60 seconds
|Cluster.ServiceType        | optional, if set, will determine the service type used when creating primary or replica services, defaults to ClusterIP if not set, can be overridden by the user on the command line as well
|Cluster.Backrest        | optional, if set, will cause clusters to have the pgbackrest volume PVC provisioned during cluster creation
|Cluster.Autofail        | optional, if set, will cause clusters to be checked for auto failover in the event of a non-Ready status
|Cluster.AutofailReplaceReplica        | optional, default is false, if set, will determine whether a replica is created as part of a failover to replace the promoted replica, the AutofailReplaceReplica setting in pgo.yaml is overrode with this command line flag if specified by a user.
|PrimaryStorage    |required, the value of the storage configuration to use for the primary PostgreSQL deployment
|ArchiveStorage    |optional, the value of the storage configuration to use for the pgwal (archive) volume for the Postgres container /pgwal volume, if not set, the PrimaryStorage setting is used
|BackupStorage    |required, the value of the storage configuration to use for backups, including the storage for pgbackrest repo volumes
|ReplicaStorage    |required, the value of the storage configuration to use for the replica PostgreSQL deployments
|Storage.storage1.StorageClass        |for a dynamic storage type, you can specify the storage class used for storage provisioning(e.g. standard, gold, fast)
|Storage.storage1.AccessMode        |the access mode for new PVCs (e.g. ReadWriteMany, ReadWriteOnce, ReadOnlyMany). See below for descriptions of these.
|Storage.storage1.Size        |the size to use when creating new PVCs (e.g. 100M, 1Gi)
|Storage.storage1.StorageType        |supported values are either *dynamic*,  *create*,  if not supplied, *create* is used
|Storage.storage1.Fsgroup        | optional, if set, will cause a *SecurityContext* and *fsGroup* attributes to be added to generated Pod and Deployment definitions
|Storage.storage1.SupplementalGroups        | optional, if set, will cause a SecurityContext to be added to generated Pod and Deployment definitions
|Storage.storage1.MatchLabels        | optional, if set, will cause the PVC to add a *matchlabels* selector in order to match a PV, only useful when the StorageType is *create*, when specified a label of *key=value* is added to the PVC as a match criteria
|DefaultContainerResource    |optional, the value of the container resources configuration to use for all database containers, if not set, no resource limits or requests are added on the database container
|DefaultLoadResource    |optional, the value of the container resources configuration to use for pgo-load containers, if not set, no resource limits or requests are added on the database container
|DefaultLspvcResource    |optional, the value of the container resources configuration to use for pgo-lspvc containers, if not set, no resource limits or requests are added on the database container
|DefaultRmdataResource    |optional, the value of the container resources configuration to use for pgo-rmdata containers, if not set, no resource limits or requests are added on the database container
|DefaultBackupResource    |optional, the value of the container resources configuration to use for crunchy-backup containers, if not set, no resource limits or requests are added on the database container
|DefaultPgbouncerResource    |optional, the value of the container resources configuration to use for crunchy-pgbouncer containers, if not set, no resource limits or requests are added on the database container
|DefaultPgpoolResource    |optional, the value of the container resources configuration to use for crunchy-pgpool containers, if not set, no resource limits or requests are added on the database container
|ContainerResources.small.RequestsMemory        | request size of memory in bytes
|ContainerResources.small.RequestsCPU        | request size of CPU cores
|ContainerResources.small.LimitsMemory        | request size of memory in bytes
|ContainerResources.small.LimitsCPU        | request size of CPU cores
|ContainerResources.large.RequestsMemory        | request size of memory in bytes
|ContainerResources.large.RequestsCPU        | request size of CPU cores
|ContainerResources.large.LimitsMemory        | request size of memory in bytes
|ContainerResources.large.LimitsCPU        | request size of CPU cores
|Pgo.LSPVCTemplate        | the PVC lspvc template file that lists PVC contents
|Pgo.LoadTemplate        | the load template file used for load jobs
|Pgo.COImagePrefix        | image tag prefix to use for the Operator containers
|Pgo.COImageTag        | image tag to use for the Operator containers
|Pgo.Audit        | boolean, if set to true will cause each apiserver call to be logged with an *audit* marking
|======================

==== Storage Configurations

You can define n-number of Storage configurations within the *pgo.yaml* file. Those Storage configurations follow these conventions -

 * they must have lowercase name (e.g. storage1)
 * they must be unique names (e.g. mydrstorage, faststorage, slowstorage)

These Storage configurations are referenced in the BackupStorage, ReplicaStorage, and PrimaryStorage configuration values. However, there are command line
options in the *pgo* client that will let a user override these default global
values to offer you the user a way to specify very targeted storage configurations
when needed (e.g. disaster recovery storage for certain backups).

You can set the storage AccessMode values to the following -

* *ReadWriteMany* - mounts the volume as read-write by many nodes
* *ReadWriteOnce* - mounts the PVC as read-write by a single node
* *ReadOnlyMany* - mounts the PVC as read-only by many nodes

These Storage configurations are validated when the *pgo-apiserver* starts, if a
non-valid configuration is found, the apiserver will abort.  These Storage values are only read at *apiserver* start time.

The following StorageType values are possible -

 * *dynamic* - this will allow for dynamic provisioning of storage using a StorageClass.
 * *create* - This setting allows for the creation of a new PVC for each PostgreSQL cluster using a naming convention of *clustername*.  When set, the *Size*, *AccessMode* settings are used in constructing the new PVC.

The operator will create new PVCs using this naming convention:
*dbname* where *dbname* is the database name you have specified.  For
example, if you run:
....
pgo create cluster example1
....

It will result in a PVC being created named *example1* and in
the case of a backup job, the pvc is named *example1-backup*

There are currently 3 sample pgo configuration files provided
for users to use as a starting configuration -

 * `pgo.yaml.nfs` - this configuration specifies *create* storage to be used, this is used for NFS storage for example where you want to have a unique PVC created for each database
 * `pgo.yaml.storageclass` - this configuration specifies *dynamic* storage to be used, namely a *storageclass* that refers to a dynamic provisioning strorage such as StorageOS or Portworx, or GCE.

Note, when Storage Type is *create*, you can specify a storage
configuration setting of *MatchLabels*, when set, this will cause a
*selector* of *key=value* to be added into the PVC, this will
let you target specific PV(s) to be matched for this cluster. Note, if a
PV does not match the claim request, then the cluster will not start.  Users
that want to use this feature have to place labels on their PV resources
as part of PG cluster creation before creating the PG cluster.  For
example, users would add a label like this to their PV before they
create the PG cluster:
....
kubectl label pv somepv myzone=somezone
....

If you do not specify *MatchLabels* in the storage configuration, then
no match filter is added and any available PV will be used to satisfy
the PVC request.  This option does not apply to *dynamic* storage
types.

Example PV creation scripts are provided that add labels to a set of PVs
and can be used for testing:  $COROOT/pv/create-pv-nfs-labels.sh, in that
example, a label of crunchyzone=red is set on a set of PVs to test with.
The pgo.yaml includes a storage config named nfsstoragered that when used
will demonstrate the label matching.  This feature allows you to support
n-number of NFS storage configurations and supports spreading a PG
cluster across different NFS storage configurations.

==== Overriding Container Resources Configuration Defaults

In the *pgo.yaml* configuration file you have the option to configure a default container resources configuration that when set will add CPU and memory resource limits and requests values into each database container when the container is created.

You can also override the default value using the `--resources-config` command flag when creating a new cluster -
....
pgo create cluster testcluster --resources-config=large
....

Note, if you try to allocate more resources than your
host or Kube cluster has available then you will see your
pods wait in a *Pending* status. The output from a `kubectl describe pod`
command will show output like this in this event -
....
Events:
  Type     Reason            Age               From               Message
  ----     ------            ----              ----               -------
  Warning  FailedScheduling  49s (x8 over 1m)  default-scheduler  No nodes are available that match all of the predicates: Insufficient memory (1).
....

==== Overriding Storage Configuration Defaults

....
pgo create cluster testcluster --storage-config=bigdisk
....

That example will create a cluster and specify a storage configuration
of *bigdisk* to be used for the primary database storage. The replica
storage will default to the value of ReplicaStorage as specified in
*pgo.yaml*.

....
pgo create cluster testcluster2 --storage-config=fastdisk --replica-storage-config=slowdisk
....

That example will create a cluster and specify a storage configuration of
*fastdisk* to be used for the primary database storage, while the replica
storage will use the storage configuration *slowdisk*.

....
pgo backup testcluster --storage-config=offsitestorage
....

That example will create a backup and use the *offsitestorage* storage configuration
for persisting the backup.

==== Disaster Recovery Using Storage Configurations

A simple mechanism for partial disaster recovery can be obtained by leveraging network
storage, Kubernetes storage classes, and the storage configuration options within the
Operator.

For example, if you define a Kubernetes storage class that refers to a storage backend
that is running within your disaster recovery site, and then use that storage class as
a storage configuration for your backups, you essentially have moved your backup files
automatically to your disaster recovery site thanks to network storage.

image::/Operator-DR-Storage.png[Operator Storage]

=== PostgreSQL Operator Container Configuration

To enable *debug* level messages from the operator pod, set the `CRUNCHY_DEBUG` environment
variable to *true* within its deployment file `deployment.json`.

==== Operator Templates

The database and cluster Kubernetes objects that get created by the operator are based on JSON
templates that are added into the operator deployment by means of a mounted volume.

The templates are located in the `$COROOT/conf/postgres-operator` directory and are added into
a config map which is mounted by the operator deployment.

== Bash Completion

There is a bash completion file that is included for users to try
located in the repository at `examples/pgo-bash-completion`. To use it -
....
cp $COROOT/examples/pgo-bash-completion /etc/bash_completion.d/pgo
su - $USER
....

== REST API

Because the *apiserver* implements a REST API, it is possible to integrate with it using your own
application code. To demonstrate this, the following *curl* commands show the API usage -

Note: Some setups may require the user to add '?version=x.x' to the end of the commands.

*pgo version*
....
curl -v -X GET -u readonlyuser:testpass -H "Content-Type: application/json" --insecure https://10.101.155.218:8443/version
....

*pgo show policy <name>*
....
curl -v -X GET -u readonlyuser:testpass -H "Content-Type: application/json" --insecure https://10.101.155.218:8443/policies/<name>
....

*pgo delete policy <name>*
....
curl -v -X GET -u readonlyuser:testpass -H "Content-Type: application/json" --insecure https://10.101.155.218:8443/policiesdelete/<name>
....

*pgo show pvc <name>*
....
curl -v -X GET -u readonlyuser:testpass -H "Content-Type: application/json" --insecure https://10.101.155.218:8443/pvc/<name>
....

*pgo apply policy <name>*
....
curl -v -X POST -u readonlyuser:testpass -H "Content-Type: application/json" --insecure https://10.101.155.218:8443/policies/apply/<name>
....

*pgo label*
....
curl -v -X POST -u readonlyuser:testpass -H "Content-Type: application/json" --insecure https://10.101.155.218:8443/label
....

*pgo load*
....
curl -v -X POST -u readonlyuser:testpass -H "Content-Type: application/json" --insecure https://10.101.155.218:8443/load
....

*pgo user*
....
curl -v -X POST -u readonlyuser:testpass -H "Content-Type: application/json" --insecure https://10.101.155.218:8443/user
....

*pgo users <name>*
....
curl -v -X GET -u readonlyuser:testpass -H "Content-Type: application/json" --insecure https://10.101.155.218:8443/users/<name>
....

*pgo delete user <name>*
....
curl -v -X GET -u readonlyuser:testpass -H "Content-Type: application/json" --insecure https://10.101.155.218:8443/usersdelete/<name>
....

*pgo show upgrade <name>*
....
curl -v -X GET -u readonlyuser:testpass -H "Content-Type: application/json" --insecure https://10.101.155.218:8443/upgrades/<name>
....

*pgo delete upgrade <name>*
....
curl -v -X GET -u readonlyuser:testpass -H "Content-Type: application/json" --insecure https://10.101.155.218:8443/upgradesdelete/<name>
....

*pgo show cluster <name>*
....
curl -v -X GET -u readonlyuser:testpass -H "Content-Type: application/json" --insecure https://10.101.155.218:8443/clusters/<name>
....

*pgo delete cluster*
....
curl -v -X GET -u readonlyuser:testpass -H "Content-Type: application/json" --insecure https://10.101.155.218:8443/clustersdelete/<name>
....

*pgo test <name>*
....
curl -v -X GET -u readonlyuser:testpass -H "Content-Type: application/json" --insecure https://10.101.155.218:8443/clusters/test/<name>
....

*pgo scale <name>*
....
curl -v -X GET -u readonlyuser:testpass -H "Content-Type: application/json" --insecure https://10.101.155.218:8443/clusters/scale/<name>
....

== Deploying pgPool

One option with pgo is enabling the creation of a pgpool deployment in addition to the PostgreSQL cluster.
Running pgpool is a logical inclusion when the Kubernetes cluster includes both a primary database in addition
to some number of replicas deployed. The current pgpool configuration deployed by the operator only works when
both a primary and a replica are running.

When a user creates the cluster a command flag can be passed as follows to enable the creation of the pgpool
deployment.
....
pgo create cluster cluster1 --pgpool
pgo scale cluster1
....

This will cause the operator to create a Deployment that includes the *crunchy-pgpool* container along with a
replica.  That container will create a configuration that will perform SQL routing to your cluster services,
both for the primary and replica services.

Pgpool examines the SQL it receives and routes the SQL statement to either the primary or replica based on
the SQL action. Specifically, it will send writes and updates to only the *primary* service. It will send
read-only statements to the *replica* service.

When the operator deploys the pgpool container, it creates a secret (e.g. mycluster-pgpool-secret) that contains
pgpool configuration files. It fills out templated versions of these configuration files specifically for this
PostgreSQL cluster.

Part of the pgpool deployment also includes creating a `pool_passwd` file that will allow the *testuser* credential
to authenticate to pgpool. Adding additional users to the pgpool configuration currently requires human intervention
specifically creating a new pgpool secret and bouncing the pgpool pod to pick up the updated secret. Future operator
releases will attempt to provide *pgo* commands to let you automate the addition or removal of a pgpool user.

Currently to update a pgpool user within the `pool_passwd` configuration file, it is necessary to copy the existing
files from the secret to your local system, update the credentials in `pool_passwd` with the new user credentials,
recreate the pgpool secret, and finally restart the pgpool pod to pick up the updated configuration files.

As an example -
....
kubectl cp demo/wed10-pgpool-6cc6f6598d-wcnmf:/pgconf/ /tmp/foo
....

That command gets a running set of secret pgpool configuration files and places them locally on your system for you
to edit.

*pgpool* requires a specially formatted password credential to be placed into `pool_passwd`. There is a golang program
included in `$COROOT/golang-examples/gen-pgpool-pass.go` that, when run, will generate the value to use within the
*pgpool_passwd* configuration file.
....
go run $COROOT/golang-examples/gen-pgpool-pass.go
Enter Username: testuser
Enter Password:
Password typed: e99Mjt1dLz
hash of password is [md59c4017667828b33762665dc4558fbd76]
....

The value *md59c4017667828b33762665dc4558fbd76* is what you will use
in the *pool_passwd* file.

Then, create the new secrets file based on those updated files -
....
$COROOT/bin/create-pgpool-secrets.sh
....

Lastly for pgpool to pick up the new secret file, delete the existing
deployment pod -
....
kubectl get deployment wed-pgpool
kubectl delete pod wed10-pgpool-6cc6f6598d-wcnmf
....

The pgpool deployment will spin up another pgpool which will pick up
the updated secret file.

== Storage Configuration

Most users after they try out the operator will want to create a more customized installation and deployment of the operator using specific storage types.

The operator will work with HostPath, NFS, Dynamic, and GKE Storage.

{{%expand "NFS" %}}

=== NFS

To configure the operator to use NFS for storage, a sample *pgo.yaml.nfs* file is provided.  Overlay the default `pgo.yaml` file with that file -
....
cp $COROOT/examples/pgo.yaml.nfs $COROOT/conf/postgres-operator/pgo.yaml
....

Then, in your .bashrc file, set the variable `CO_NFS_IP` to the IP address of your NFS server:
....
export CO_NFS_IP=192.168.2.14
....

Edit the *pgo.yaml* file to specify the NFS GID that is set for the NFS volume mount you will be using. The default value assumed is *nfsnobody* as the GID (65534).  Update the value to meet your NFS security settings.

Finally, run the `$COROOT/pv/create-pv-nfs.sh` script to create persistent volumes based on your NFS settings.

{{% /expand%}}

{{%expand "Dynamic" %}}

=== Dynamic

To configure the operator to use Dynamic Storage classes for storage, a sample *pgo.yaml.storageclass* file is provided.  Overlay the default *pgo.yaml* file with that file -
....
cp $COROOT/examples/pgo.yaml.storageclass $COROOT/conf/postgres-operator/pgo.yaml
....

Edit the *pgo.yaml* file to specify the storage class you will be using, the default value assumed is *standard* which is the name used by default within a GKE Kube cluster deployment.  Update the value to match your storage classes.

Notice that the *FsGroup* setting is required for most block storage and is set to the value of *26* since the PostgreSQL container runs as UID *26*.

{{% /expand%}}

{{%expand "GKE" %}}

=== GKE

Some notes for setting up GKE for the Operator deployment.

==== Install Kubectl

On your host you will be working from, install the kubectl command -

https://kubernetes.io/docs/tasks/tools/install-kubectl/

==== GCP

* Select your project
* Create a Kube cluster in that project

By default a storage class called *standard* is created.

==== Install GCloud

To access the Kubernetes cluster, install the gcloud utility -

....
https://cloud.google.com/sdk/downloads
cd google-cloud-sdk
./install.sh
....

==== Configure Kubectl for Cluster Access

....
gcloud auth login

gcloud container clusters get-credentials jeff-quickstart --zone us-central1-a --project crunchy-dev-test

kubectl get storageclass
....

{{% /expand%}}
