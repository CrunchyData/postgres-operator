[
{
	"uri": "https://crunchydata.github.io/postgres-operator/latest/installation/quick-installation/",
	"title": "Quick Installation",
	"tags": [],
	"description": "",
	"content": "Table of Contents Overview Quickstart GKE/PKS Openshift Container Platform   Next Steps   Latest Release: 3.5.0 2018-12-14\n Overview There are currently quickstart script that seek to automate the deployment to popular Kubernetes environments -\n   quickstart.sh\n   The quickstart script will deploy the operator to a GKE Kube cluster or an Openshift Container Platform cluster. The quickstart script is intended to get you up and running quickly, for a typical more custom installation, the manual installation is recommended.\n The script assumes you have a StorageClass defined for persistence.\n Pre-compiled versions of the Operator pgo client are provided for the x86_64, Mac OSX, and Windows hosts.\n   Quickstart GKE/PKS The quickstart.sh script will allow users to set up the Postgres Operator quickly on GKE, PKS, and Openshift.\n The script requires a few things in order to work -\n   wget utility installed\n  kubectl or oc utility installed\n  StorageClass defined on your GKE instance\n   Executing the script will give you a default Operator deployment that assumes dynamic storage and a storage class named standard, user provided values are also allowed by the script to override these defaults.\n The script performs the following -\n   downloads the Operator configuration files\n  sets the $HOME/.pgouser file to default settings\n  deploys the Operator Deployment\n  sets your .bashrc to include the Operator environment variables\n  sets your $HOME/.bash_completion file to be the pgo bash_completion file\n     Note  You should copy the quickstart.sh script from github rather than cloning the entire github Operator repository!     A tip, if you want to set your Kube context to some particular namespace you can run commands similar to this to set it to a demo namespace if that namespace has already been created on your GKE cluster:\n kubectl create -f $COROOT/examples/demo-namespace.json kubectl config set-context demo --cluster=gke_crunchy-a-test_us-central1-a_usera-quickstart --namespace=demo --user=gke_crunchy-a-test_us-central1-a_usera-quickstart kubectl config use-context demo   For Mac and Windows users, pre-built pgo binaries are included in the operator release tar ball, you would download the pgo CLI binaries from the Releases page to your local machine as part of the quick installation:\n   pgo-mac is the Mac binary\n  pgo.exe is the Windows binary\n  pgo is the Linux binary\n  expenv-mac is the expenv binary for Mac\n  expenv.exe is the expenv binary for Windows\n   Currently the quickstart scripts are meant for Linux installs, you will need to modify this script for Windows or Mac installs until we support and provide Windows and Mac installation scripts.\n  Openshift Container Platform The script also is used for installing the operator on OCP.\n    Next Steps Next, visit the Deployment page to deploy the Operator, verify the installation, and view various storage configurations.\n   "
},
{
	"uri": "https://crunchydata.github.io/postgres-operator/latest/installation/",
	"title": "Installation",
	"tags": [],
	"description": "",
	"content": "Latest Release: 3.5.0 2018-12-14\n Installation For a quick deployment on either a GKE or OpenShift environment, visit the Quick Installation page.\n For a manual installation of the Operator on either a Kubernetes or OpenShift environment, visit the Manual Installation page.\n A Helm Chart is also provided.\n If you\u0026#8217;re looking to upgrade a current PostgreSQL Operator installation, visit the Upgrading the Operator page.\n There are many ways to configure the operator further. Some sample configurations are documented on the Configuration page. This includes setting up security and storage configurations for your environment.\n After completing the installation steps, ensure you visit the Deployment page to deploy the Operator to your environment.\n   Next Steps You may want to find out more information on how the operator is designed to work and deploy. This information can be found in the How It Works page.\n Information can be found on the full scope of commands on the Getting Started page.\n   "
},
{
	"uri": "https://crunchydata.github.io/postgres-operator/latest/installation/manual-installation/",
	"title": "Manual Installation",
	"tags": [],
	"description": "",
	"content": "Table of Contents Project Structure Installation Prerequsites Basic Installation HostPath Persistent Volumes NFS Persistent Volumes   Build Images \u0026amp; Deploy Makefile Targets Next Steps   Latest Release: 3.5.0 2018-12-14\n Project Structure First, define the following environment variables in .bashrc:\n export GOPATH=$HOME/odev export GOBIN=$GOPATH/bin export PATH=$PATH:$GOBIN export CO_NAMESPACE=demo export CO_CMD=kubectl export COROOT=$GOPATH/src/github.com/crunchydata/postgres-operator export CO_IMAGE_PREFIX=crunchydata export CO_BASEOS=centos7 export CO_VERSION=3.5.0 export CO_IMAGE_TAG=$CO_BASEOS-$CO_VERSION # for the pgo CLI auth export PGO_CA_CERT=$COROOT/conf/postgres-operator/server.crt export PGO_CLIENT_CERT=$COROOT/conf/postgres-operator/server.crt export PGO_CLIENT_KEY=$COROOT/conf/postgres-operator/server.key # for crunchy-scheduler startup export CCP_IMAGE_PREFIX=crunchydata export CCP_IMAGE_TAG=centos7-10.6-2.2.0 # useful aliases alias setip='export CO_APISERVER_URL=https://`kubectl get service postgres-operator -o=jsonpath=\"{.spec.clusterIP}\"`:8443' alias alog='kubectl logs `kubectl get pod --selector=name=postgres-operator -o jsonpath=\"{.items[0].metadata.name}\"` -c apiserver' alias olog='kubectl logs `kubectl get pod --selector=name=postgres-operator -o jsonpath=\"{.items[0].metadata.name}\"` -c operator'   If you have access to the Crunchy RHEL images, you would change the above references to centos7 to rhel7.\n When deploying on Openshift Container Platform, the CO_CMD environment variable should be:\n export CO_CMD=oc   To perform an installation of the operator, first create the project structure as follows on your host, here we assume a local directory called odev -\n . .bashrc mkdir -p $HOME/odev/src $HOME/odev/bin $HOME/odev/pkg $GOPATH/src/github.com/crunchydata/   Next, get a tagged release of the source code -\n cd $GOPATH/src/github.com/crunchydata git clone https://github.com/CrunchyData/postgres-operator.git cd postgres-operator git checkout 3.5.0     Installation Prerequsites To run the operator and the pgo client, you will need the following -\n   a running Kubernetes or OpenShift cluster\n  the kubectl or oc clients installed in your PATH and configured to connect to the cluster (e.g. export KUBECONFIG=/etc/kubernetes/admin.conf)\n  a Kubernetes namespace created and set to where you want the operator installed. For this install we assume a namespace of demo has been created.\n   kubectl create -f examples/demo-namespace.json kubectl config set-context $(kubectl config current-context) --namespace=demo kubectl config view -o \"jsonpath={.contexts[?(@.name==\\\"$(kubectl config current-context 2\u0026gt;/dev/null)\\\")].context.namespace}\"   On Openshift Container Platform, you would have a Project and User defined for installing the Operator.\n Run the Makefile setup target to install depedencies.\n make setup   Next, run the Makefile installrbac target as a user with cluster-admin priviledges, not as a normal Kube or Openshift user. This target creates the RBAC roles and CRDs required by the Operator and is only required to be created one time.\n For example, on an Openshift system you would run this target as follows using the system:admin Openshift user:\n $ sudo su - # oc login -u system:admin # cd /home/oper # . .bashrc # export PATH=$PATH:/home/oper/odev/bin # cd odev/src/github.com/crunchydata/postgres-operator # make installrbac   On a Kube system, you would be connected as a cluster-admin user and just issue:\n # cd /home/oper # . .bashrc # export PATH=$PATH:/home/oper/odev/bin # cd odev/src/github.com/crunchydata/postgres-operator make installrbac     Basic Installation The basic pgo.yaml configuration specifies 3 different storage configurations: * hostpath * nfs (default) * storage-class\n Storage configurations are documented here: here.\n The default storage configuration used for creating Primary, Replica, and Backups is set to NFS in the default pgo.yaml file. Adjust this setting to meet your storage requirements.\n Sample PV creation scripts are found in the following directory:\n examples/pv   HostPath Persistent Volumes The default Persistent Volume script assumes a default HostPath directory be created called /data:\n sudo mkdir /data sudo chmod 777 /data   Create some sample Persistent Volumes using the following script:\n $COROOT/pv/create-pv.sh    NFS Persistent Volumes The NFS Persistent Volume script assumes a default directory be created called /nfsfileshare as the NFS mount point on your system:\n sudo ls /nfsfileshare   See the crunchy-containers documentation on how to install NFS on a centos/RHEL system if you want to use NFS for testing the operator.\n Create some sample NFS Persistent Volumes using the following script:\n $COROOT/pv/create-nfs-pv.sh      Build Images \u0026amp; Deploy   Packaged Images   Packaged Images To pull prebuilt versions from Dockerhub of the postgres-operator containers, execute the following Makefile target -\n make pull   To pull down the prebuilt pgo binaries, download the tar.gz release file from the following link -\n   Github Releases\n  extract (e.g. tar xvzf postgres-operator.3.5.0.tar.gz)\n   cd $HOME tar xvzf ./postgres-operator.3.5.0.tar.gz     copy pgo client to somewhere in your path (e.g. cp pgo /usr/local/bin)\n   Next, deploy the operator to your Kubernetes cluster -\n cd $COROOT make deployoperator     Warning  If you make configuration file changes you will need to re-run the deployoperator makefile target to re-deploy the Operator with the new configuration files.\n           Build from Source   Build from Source The purpose of this section is to illustrate how to build the PostgreSQL Operator from source. These are considered advanced installation steps and should be primarily used by developers or those wishing a more precise installation method.\n Requirements The postgres-operator runs on any Kubernetes and Openshift platform that supports Custom Resource Definitions. The Operator is tested on Kubeadm and OpenShift Container Platform environments.\n The operator is developed with the Golang versions greater than or equal to version 1.8. See Golang website for details on installing golang.\n The Operator project builds and operates with the following containers -\n   PVC Listing Container\n  Remove Data Container\n  postgres-operator Container\n  apiserver Container\n  file load Container\n  pgbackrest interface Container\n   This Operator is developed and tested on the following operating systems but is known to run on other operating systems -\n   CentOS 7\n  RHEL 7\n          Makefile Targets The following table describes the Makefile targets -\n Table 1. Makefile Targets     Target Description     macpgo\n build the Mac version of the pgo CLI binary\n   winpgo\n build the Windows version of the pgo CLI binary\n   installrbac\n only run once and by a cluster-admin user, this target creates the Operator CRDs and RBAC resources required by the Operator\n   setupnamespace\n only run once, will create a namespace called demo\n   bounce\n delete the Operator pod only, this is a way to upgrade the operator without a full redeploy, as the operator runs in a Deployment, a new pod will be created to replace the old one, a simple way to bounce the pod\n   deployoperator\n deploy the Operator (apiserver and postgers-operator) to Kubernetes\n   all\n compile all binaries and build all images\n   setup\n fetch the dependent packages required to build with, and create Kube RBAC resources\n   main\n compile the postgres-operator\n   pgo\n build the pgo binary\n   clean\n remove binaries and compiled packages, restore dependencies\n   operatorimage\n compile and build the postgres-operator Docker image\n   apiserverimage\n compile and build the apiserver Docker image\n   lsimage\n build the lspvc Docker image\n   loadimage\n build the file load Docker image\n   rmdataimage\n build the data deletion Docker image\n   pgo-backrest-image\n build the pgbackrest interface Docker image\n   release\n build the postgres-operator release\n      Next Steps Next, visit the Deployment page to deploy the Operator, verify the installation, and view various storage configurations.\n   "
},
{
	"uri": "https://crunchydata.github.io/postgres-operator/latest/getting-started/",
	"title": "Getting Started",
	"tags": [],
	"description": "",
	"content": "Table of Contents First Steps Cluster Names General Operator Version Operator Status Operator Configuration Disk Capacity   Cluster Basics Create Cluster Update Cluster     Latest Release: 3.5.0 2018-12-14\n First Steps Prior to using pgo, users will need to specify the postgres-operator URL as follows:\n kubectl get service postgres-operator NAME CLUSTER-IP EXTERNAL-IP PORT(S) AGE postgres-operator 10.104.47.110 \u0026lt;none\u0026gt; 8443/TCP 7m export CO_APISERVER_URL=https://10.104.47.110:8443 pgo version     Cluster Names Many of the pgo commands take in a cluster name, in some cases the special name of all is accepted which will cause the command to be applied to all PostgreSQL clusters. For example:\n pgo df all     General Operator Version This command makes it possible to see what version of the pgo client and postgres-operator you are running.\n Syntax $ pgo version\n   Operator Status You can use the pgo status command to see overall pgo status. Selective metrics are displayed to provide some insights to the pgo user and administrator as to what is running currently in this namespace related to pgo.\n Syntax $ pgo status [FLAGS]\n  Flags     Name Shorthand Input Usage     --output=json\n -o json\n String\n The output format. Currently, json is the only supported value.\n      Operator Configuration The pgo show config command displays the running operator configuration parameters that dictate the setup and user defined configuration of the operator. This command can be useful for sharing your configuration or verifying the setup is as expected.\n Syntax $ pgo show config\n   Disk Capacity The pgo df command will let you see the disk capacity of a cluster\u0026#8217;s PVC versus that of the PostgreSQL data that has been written to disk. If the capacity is less than 50%, then the output is printed in red in order to alert the user. The listing is broken out by the cluster\u0026#8217;s Pods.\n Syntax $ pgo df NAME [FLAGS]\n  Flags     Name Shorthand Input Usage     --selector\n -s\n String\n The selector to use for cluster filtering.\n     Examples Cluster Selectors The pgo df command can either be run against a single cluster or against all clusters matching a selector:\n pgo df mycluster pgo df --selector=project=xrayapp        Cluster Basics Create Cluster The create cluster command will automatically provision a PostgreSQL cluster within Kubernetes or OpenShift using a Deployment.\n Syntax $ pgo create cluster NAME [FLAGS]\n  Flags     Name Shorthand Input Usage     --archive\n N/A\n N/A\n Enables archive logging for the database cluster.\n   --autofail\n N/A\n N/A\n If set, will cause autofailover to be enabled on this cluster.\n   --autofail-replace-replica\n N/A\n N/A\n If set, will allow you to override the AutofailReplaceReplica setting as specified in the pgo.yaml, this setting lets you control whether a replica is created as part of a failover to replace the promoted replica\n   --backup-pvc\n N/A\n String\n The backup archive PVC to restore from.\n   --backup-path\n N/A\n String\n The backup archive path to restore from.\n   --ccp-image-tag\n N/A\n String\n The CCPImageTag to use for cluster creation. If specified, overrides the pgo.yaml setting.\n   --custom-config\n N/A\n String\n The name of a configMap that holds custom PostgreSQL configuration files used to override defaults.\n   --labels\n N/A\n String\n The labels to apply to this cluster.\n   --metrics\n N/A\n N/A\n Adds the crunchy-collect container to the database pod.\n   --node-label\n N/A\n String\n The node label (key) to use in placing the primary database. If not set, any node is used.\n   --password\n N/A\n String\n The password to use for initial database users.\n   --service-type\n N/A\n String\n The Service type to use for the PostgreSQL cluster. If not set, the pgo.yaml default will be used.\n   --pgbackrest\n N/A\n N/A\n Enables a pgBackRest volume for the database pod.\n   --pgbackrest-restore-from\n N/A\n N/A\n Only applies when creating a cluster from a pgbackrest restored PVC. This is the name of the cluster from which the restored PVC was created from and which the new cluster credentials will be based. This setting is required in the scenario.\n   --pgbadger\n N/A\n N/A\n Adds the crunchy-pgbadger container to the database pod.\n   --pgpool\n N/A\n N/A\n Adds the crunchy-pgpool container to the database pod.\n   --pgpool-secret\n N/A\n String\n The name of a pgpool secret to use for the pgpool configuration.\n   --policies\n N/A\n String\n The policies to apply when creating a cluster, comma separated.\n   --replica-count\n N/A\n Int\n The number of replicas to create as part of this cluster. After a cluster is created, you can also add replicas using the scale command.\n   --replica-storage-config\n N/A\n String\n The name of a Storage config in pgo.yaml to use for the cluster replica storage.\n   --resources-config\n N/A\n String\n The name of a container resource configuration in pgo.yaml that holds CPU and memory requests and limits.\n   --secret-from\n N/A\n String\n The cluster name to use when restoring secrets.\n   --series\n N/A\n Int\n The number of clusters to create in a series (default 1).\n   --storage-config\n N/A\n String\n The name of a Storage config in pgo.yaml to use for the cluster storage.\n     Examples Simple Creation Create a single cluster:\n pgo create cluster mycluster   Create a single cluster with a single replica:\n pgo create cluster mycluster --replica-count=1    Complex Creation Create a series of clusters, specifying it as the xray project, with the xrayapp and rlspolicy policies added:\n pgo create cluster mycluster --series=3 --labels=project=xray --policies=xrayapp,rlspolicy    Image Version New clusters typically pick up the container image version to use based on the pgo configuration file\u0026#8217;s CcpImageTag setting. You can override this value using the --ccp-image-tag command line flag:\n pgo create cluster mycluster --ccp-image-tag=centos7-9.6.5-1.6.0    Metrics Add the crunchy-collect container from the Crunchy Container Suite to the database cluster pod and enable metrics collection on the database:\n pgo create cluster mycluster --metrics   You can connect these containers to a metrics pipeline using Grafana and Prometheus by following the example found in the Crunchy Container Suite documentation.\n  pgBadger Add a pgBadger sidecar into the Postgres pod:\n pgo create cluster mycluster --pgbadger   This command flag adds the crunchy-pgbadger container into the database pod. pgBadger reports can then be accessed through port 10000 at /api/badgergenerate.\n  pgPool II By appending the --pgpool command line flag, you can add pgPool II to the database cluster. The container used for this functionality is the crunchy-pgpool container image from the Crunchy Container Suite.\n pgo create cluster mycluster --pgpool    Auto Failover To enable auto failover on this cluster, use the following flag:\n pgo create cluster mycluster --autofail   This flag, when set on the cluster, informs the operator to look or watch for NotReady events on this cluster. When those occur, it will create a failover state machine which acts as a timer for the cluster. If the timer expires, then a failover is triggered on the cluster turning one of the cluster replica pods into the replacement primary pod. See the How It Works documentation for more details on auto failover.\n  pgBackRest pgbackrest beta integration was implemented in version 3.4.0 of the Operator. NOTE: pgbackrest integration is still subject to change in upcoming releases.\n The backrestrepo PVC, used by pgBackRest, has to be created on a RWX file system type in this release. pgBackRest is a more advanced backup and restore capability exposed by the Operator.\n The pgBackRest support is enabled in a PG cluster by a user specifying the --pgbackrest command flag. To enable this feature for all PG clusters when created, you can specify a pgbackrest setting within the pgo.yaml configuration.\n Create a PG cluster that enables pgBackRest specifically for that cluster:\n pgo create cluster mycluster --pgbackrest   Setting this value will cause the Operator to create a PVC specifically dedicated for holding pgBackRest backups.\n Create a pgBackRest backup:\n pgo backup mycluster --backup-type=pgbackrest   You can also pass in pgbackrest backup command options:\n pgo backup mycluster --backup-type=pgbackrest --pgbackrest-opts=\"--type=incr\"   Note, you can not specify --storage-config flag when specifying a pgbackrest backup.\n List pgBackRest information:\n pgo show backup mycluster --backup-type=pgbackrest   Restore from an existing cluster into a newly created PVC:\n pgo restore mycluster --to-pvc=restored pgo create cluster restored --pgbackrest-restore-from=mycluster --pgbackrest   To do restore based on a point in time:\n pgo restore mycluster --to-pvc=restored --backup-opts=\"--type=time\" --pitr-target=\"2018-12-12 14:45:58 EST\"   The pgBackRest backrestrepo PVCs are created using the pgo.yaml BackupStorage setting. Typically, this will be a RWX file system but if the file system is RWO the PVCs will be created without having write access and a backup and restore will fail. The RWX file system setup will allow you to restore from this PVC without having to shutdown the currently attached PostgreSQL cluster. Note that a cluster based off of the restored PVC has to attach the same pgbackrest repo used by the original cluster the restore was based off of.\n    Update Cluster The update cluster command will let users set the autofail flag on a set of clusters. This can be handy to prevent autofailover logic in certain scenarios like when you want to do a minor upgrade on a primary deployment.\n Syntax $ pgo update cluster NAME|all [FLAGS]\n  Flags     Name Shorthand Input Usage     --autofail\n -b\n N/A\n Sets to true or false the autofail flag on the pgcluster CRD for this cluster.\n==== Examples\nCreate a cluster: \u0026#8230;\u0026#8203;. pgo create cluster mycluster --autofail --replica-count=1 \u0026#8230;\u0026#8203;.\nDisable autofail: \u0026#8230;\u0026#8203;. pgo update cluster mycluster --autofail=false \u0026#8230;\u0026#8203;.\nShutdown the primary pod: \u0026#8230;\u0026#8203;. kubectl scale deployment/one --replicas=0 \u0026#8230;\u0026#8203;.\nDo something to the primary such as copy its pvc or patch the primary deployment.\nBring the primary pod back: \u0026#8230;\u0026#8203;. kubectl scale deployment/one --replicas=1 \u0026#8230;\u0026#8203;.\nSet autofail flag back to true: \u0026#8230;\u0026#8203;. pgo update cluster mycluster --autofail=true \u0026#8230;\u0026#8203;.\n=== Delete Cluster\nThe delete cluster command will by default delete all associated components of the selected cluster, but will not delete the data or the backups unless specified.\n==== Syntax\n$ pgo delete cluster NAME\n    |Name |Shorthand |Input |Usage\n |--delete-backups |-b |N/A | Causes the backups for this cluster to be removed permanently. This only is applicable with pgbasebackup backup volumes and does not remove pgbackrest repo volumes.\n |--delete-configs |-b |N/A | Causes the configuration maps for this cluster to be removed permanently.\n |--delete-data |-d |N/A | Causes the data for this cluster to be removed permanently.\n |--no-prompt |-n |N/A | No command line confirmation.\n |--selector |-s |String | The selector to use for cluster filtering.\n     ==== Examples\n===== Simple Deletion\nDelete a single cluster: \u0026#8230;\u0026#8203;. pgo delete cluster mycluster \u0026#8230;\u0026#8203;.\nNote that this command will not remove the PVC associated with this cluster.\n===== Complex Deletion\nSelectors also apply to the delete command as follows: \u0026#8230;\u0026#8203;. pgo delete cluster --selector=project=xray \u0026#8230;\u0026#8203;.\nThis command will cause any cluster matching the selector to be removed.\n===== Delete Components, Data, \u0026amp; Backups\nYou can remove a cluster, it\u0026#8217;s data files, and all backups by running: \u0026#8230;\u0026#8203;. pgo delete cluster restoredb --delete-data --delete-backups --delete-configs \u0026#8230;\u0026#8203;.\nWhen you specify a destructive delete like above, you will be prompted to make sure this is what you want to do. If you don\u0026#8217;t want to be prompted you can enter the --no-prompt command line flag.\n=== Show Cluster\nThe show cluster command allows you to view all the associated created components of a specific cluster or selection of clusters.\nBy default, you will be able to view the status of the created pod, the PVC, Deployment, Service, and Labels associated with the cluster, and any and all specified options (such as whether crunchy_collect is enabled).\n==== Syntax\n$ pgo show cluster NAME\n all [FLAGS]\n==== Flags\n[width=\"100%\",cols=\"5,1,1, 13\",options=\"header\"]\n    |Name |Shorthand |Input |Usage\n |--output=json |-o json |String | The output format. Currently, json is the only supported value.\n |--selector |-s |String | The selector to use for cluster filtering.\n |--ccp-image-tag |N/A |String | Filter the results based on the PostgreSQL version of the cluster.\n     ==== Examples\n===== Simple Display\nShow a single cluster: \u0026#8230;\u0026#8203;. pgo show cluster mycluster \u0026#8230;\u0026#8203;.\n===== Show All\nShow all clusters available: \u0026#8230;\u0026#8203;. pgo show cluster all \u0026#8230;\u0026#8203;.\n===== Show Secrets\nUser credentials are generated through Kubernetes Secrets automatically for the testuser, primaryuser and postgres accounts. The generated passwords can be viewed by running the pgo show user command. More details are available on user management below.\n\u0026#8230;\u0026#8203;. pgo show user mycluster \u0026#8230;\u0026#8203;.\n===== Viewing Users With Passwords Set to Expire\nTo see user passwords that have expired past a certain number of days in the mycluster cluster: \u0026#8230;\u0026#8203;. pgo show user --expired=7 --selector=name=mycluster \u0026#8230;\u0026#8203;.\n[width=\"100%\",cols=\"5,1,1, 13\",options=\"header\"]\n    |Name |Shorthand |Input |Usage |--expired |N/A |String |\n     ===== PostgreSQL Version\nFilter the results based on the PostgeSQL version of the cluster with the --ccp-image-tag flag: \u0026#8230;\u0026#8203;. pgo show cluster all --ccp-image-tag=centos7-10.5-2.1.0 \u0026#8230;\u0026#8203;.\n=== Test Connection\nThis command will test each service defined for the cluster using the postgres, primary, and normal user accounts defined for the cluster. The cluster credentials are accessed and used to test the database connections. The equivalent psql command is printed out as connections are tried, along with the connection status.\n==== Syntax\n$ pgo test NAME\n all [FLAGS]\n==== Flags\n[width=\"100%\",cols=\"5,1,1, 13\",options=\"header\"]\n    |Name |Shorthand |Input |Usage\n |--output=json |-o json |String | The output format. Currently, json is the only supported value.\n |--selector |-s |String | The selector to use for cluster filtering.\n     ==== Examples\n===== Simple Test\nTest the database connections to a cluster: \u0026#8230;\u0026#8203;. pgo test mycluster \u0026#8230;\u0026#8203;.\n===== Complex Test\nLike other commands, you can use the selector to test a series of clusters or to test all available clusters: \u0026#8230;\u0026#8203;. pgo test --selector=env=research pgo test all \u0026#8230;\u0026#8203;.\n== Administration\n=== Reload\nThe reload command will perform a reload on the specified PostgreSQL cluster.\n==== Syntax\n$ pgo reload NAME [FLAGS]\n==== Flags\n[width=\"100%\",cols=\"5,1,1, 13\",options=\"header\"]\n    |Name |Shorthand |Input |Usage\n |--no-prompt |-n |N/A | No command line confirmation.\n |--selector |-s |String | The selector to use for cluster filtering.\n     ==== Examples\n===== Simple Reload\nReload a single cluster: \u0026#8230;\u0026#8203;. pgo reload mycluster \u0026#8230;\u0026#8203;.\n=== Backups\nThe backup command will utilize the crunchy-backup container to execute a full backup against another database container using the standard pg_basebackup utility that is included with PostgreSQL.\nWhen you request a backup, pgo will prompt you if you want to proceed because this action will delete any existing backup job for this cluster that might exist. The backup files will still be left intact but the actual Kubernetes Job will be removed prior to creating a new Job with the same name.\n==== Syntax\n$ pgo backup NAME [FLAGS]\n==== Flags\n[width=\"100%\",cols=\"5,1,1, 13\",options=\"header\"]\n    |Name |Shorthand |Input |Usage\n |--selector |-s |String | The selector to use for cluster filtering.\n |--pvc-name |N/A |String | The PVC name to use for the backup instead of the default.\n |--backup-type |N/A |String | The backup type to perform. Default is pgbasebackup, and both pgbasebackup and pgbackrest are valid backup types.\n |--backup-opts |N/A |String | The options to pass to pgbasebackup or pgbackrest, use appropriate command options depending on which type of backup you are performing.\n |--storage-config |N/A |String | The name of a Storage config in pgo.yaml to use for the cluster storage.\n     ==== Examples\n===== Simple Backup\nYou can start a backup job for a cluster as follows: \u0026#8230;\u0026#8203;. pgo backup mycluster \u0026#8230;\u0026#8203;.\n===== Show Backup\nView the backup and backup status: \u0026#8230;\u0026#8203;. pgo show backup mycluster \u0026#8230;\u0026#8203;.\n===== Backup PVC Management\nNOTE: 'pgo show pvc' can run into file permission issues if you are trying to view a PVC that is on a RWO (read write once) file system (e.g. cloud storage, ceph, storageos, etc.). If another pod has the PVC mounted you will get timeout errors from the 'pgo lspvc' command in the current 3.4.0 release.\nView the PVC folder and the backups contained therein:\n\u0026#8230;\u0026#8203;. pgo show pvc mycluster-backup pgo show pvc mycluster-backup --pvc-root=mycluster-backups \u0026#8230;\u0026#8203;.\nThe output from this command is important in that it can let you copy/paste a backup snapshot path and use it for restoring a database or essentially cloning a database with an existing backup archive.\nFor example, to restore a database from a backup archive: \u0026#8230;\u0026#8203;. pgo create cluster restoredb --backup-path=mycluster-backups/2017-03-27-13-56-49 --backup-pvc=mycluster-backup --secret-from=mycluster \u0026#8230;\u0026#8203;.\nThis will create a new database called restoredb based on the backup found in mycluster-backups/2017-03-27-13-56-49 and the secrets of the mycluster cluster.\n===== Override PVC\nYou can override the PVC used by the backup job with the following: \u0026#8230;\u0026#8203;. pgo backup mycluster --pvc-name=myremotepvc \u0026#8230;\u0026#8203;.\nThis might be useful for special backup cases such as creating a backup on a disaster recovery PVC.\n===== Delete Backup\nTo delete a backup enter the following: \u0026#8230;\u0026#8203;. pgo delete backup mycluster \u0026#8230;\u0026#8203;.\nWhen run, this command removes the PVC used for the backups, and runs the rmdata Job to physically perform data removal of that PVC\u0026#8217;s contents. It also removes the pgbackup CRD for this cluster that holds the last pg_basebackup results.\n=== Scheduling\nThe schedule command will generate schedule configuration maps that are utitlized by the crunchy-scheduler container. This allows users to create automated, scheduled backups for their PostgreSQL clusters.\nCurrently only two types of backups are supported with the schedule command: * pgBackRest * pgBaseBackup\nCrunchy Scheduler is a cron-like microservice that periodically queries Kubernetes for configuration maps with the label crunchy-scheduler=true in a specific namespace. After finding the schedule configs, the scheduler service will either exec into the container (pgBackRest) or create pgBaseBackup jobs for the configured schedule.\nNOTE: in operator version 3.4.0, you are REQUIRED, a single time, to run a pgbackrest backup PRIOR to creating a pgbackrest schedule. This will not be a requirement in the 3.5.0 version of the Operator.\n==== Syntax\n$ pgo create schedule NAME [FLAGS]\n==== Flags\n[width=\"100%\",cols=\"5,1,1, 13\",options=\"header\"]\n    |Name |Shorthand |Input |Usage\n |--ccp-image-tag |-n |N/A | Image version to use for pgBaseBackup backup jobs. Defaults to what PGO is configured to use.\n |--no-prompt |-n |N/A | No command line confirmation.\n |--pgbackrest-backup-type |N/A |String | The type of pgBackRest backup to perform. There is no default and the following are valid: full, diff, incr\n |--pvc-name |N/A |String | The PVC name to use for the backup. Only used for pgBaseBackup schedule types and must be created prior to using.\n |--schedule |N/A |String | The schedule assigned to the cron task.\n |--schedule-type |N/A |String | The schedule type to perform. There is no default and both pgbasebackup and pgbackrest are valid schedule types.\n |--selector |-s |String | The selector to use for cluster filtering.\n     ==== Examples\n===== Creating pgBackRest Schedules\nCreate a pgBackRest full backup on Sunday at 1 a.m:\n\u0026#8230;\u0026#8203;. pgo create schedule --schedule=\"0 1 * * 7\" --schedule-type=pgbackrest --pgbackrest-backup-type=full mycluster \u0026#8230;\u0026#8203;.\nCreate a pgBackRest diff backup on Monday-Saturday at 1 a.m:\n\u0026#8230;\u0026#8203;. pgo create schedule --schedule=\"0 1 * * 1-6\" --schedule-type=pgbackrest --pgbackrest-backup-type=diff mycluster \u0026#8230;\u0026#8203;.\n===== Creating pgBaseBackup Schedules\nCreate a pgBaseBackup backup every day at 1 a.m:\n\u0026#8230;\u0026#8203;. pgo create schedule --schedule=\"0 1 * * *\" --schedule-type=pgbasebackup --pvc-name=mycluster-backups mycluster \u0026#8230;\u0026#8203;.\n==== Creating Schedules Using Selectors\nUsing the selector flag, we can create schedules for all clusters that match a label:\n\u0026#8230;\u0026#8203;. pgo create schedule --schedule=\"0 1 * * *\" --schedule-type=pgbasebackup --pvc-name=mycluster-backups --selector=env=test \u0026#8230;\u0026#8203;.\n===== Show Schedules\nView the schedules for cluster named mycluster:\n\u0026#8230;\u0026#8203;. pgo show schedule mycluster \u0026#8230;\u0026#8203;.\nView the schedules for all clusters with the label env=test:\n\u0026#8230;\u0026#8203;. pgo show schedule --selector=env=test \u0026#8230;\u0026#8203;.\nor for a particular cluster: \u0026#8230;\u0026#8203;. pgo show schedule --selector=pg-cluster=mycluster \u0026#8230;\u0026#8203;.\n===== Delete Schedules\nTo delete schedules for a specific cluster:\n\u0026#8230;\u0026#8203;. pgo delete schedule mycluster \u0026#8230;\u0026#8203;.\nTo delete a schedule by name:\n\u0026#8230;\u0026#8203;. pgo delete schedule --schedule-name=mycluster-pgbackrest-full \u0026#8230;\u0026#8203;.\nTo delete schedules for all clusters with the label env=test:\n\u0026#8230;\u0026#8203;. pgo delete schedule --selector=env=test \u0026#8230;\u0026#8203;.\n=== Scaling Replicas\nWhen you create a Cluster, you will see in the output a variety of Kubernetes objects were created including:\n* a Deployment holding the primary PostgreSQL database * a Deployment holding the replica PostgreSQL database * a service for the primary database * a service for the replica databases\nSince PostgreSQL is a single-primary database by design, the primary Deployment is set to a replica count of 1 and it can not scale beyond that.\nWith PostgreSQL, you can create any n-number of replicas each of which connect to the primary. This forms a streaming replication PostgreSQL cluster. The PostgreSQL replicas are read-only whereas the primary is read-write.\n==== Syntax\n$ pgo scale NAME [FLAGS]\n==== Flags\n[width=\"100%\",cols=\"5,1,1, 13\",options=\"header\"]\n    |Name |Shorthand |Input |Usage\n |--service-type |N/A |String | The service type to use in the replica Service. If not set, the default in pgo.yaml will be used. Possible values include LoadBalancer, ClusterIP, and NodePort.\n |--ccp-image-tag |N/A |String | The CCPImageTag to use for cluster creation. If specified, overrides the .pgo.yaml setting.\n |--no-prompt |-n |N/A | No command line confirmation.\n |--node-label |N/A |String | The node label (key) to use in placing the primary database. If not set, any node is used.\n |--replica-count |N/A |String | The replica count to apply to the clusters (default 1).\n |--resources-config |N/A |String | The name of a container resource configuration in pgo.yaml that holds CPU and memory requests and limits.\n |--storage-config |N/A |String | The name of a Storage config in pgo.yaml to use for the cluster storage.\n     ==== Examples\n===== Scaling Up\nCreate a Postgres replica: \u0026#8230;\u0026#8203;. pgo scale mycluster \u0026#8230;\u0026#8203;.\nScale a Postgres replica to a certain number of replicas: \u0026#8230;\u0026#8203;. pgo scale mycluster --replica-count=3 \u0026#8230;\u0026#8203;.\nThe pgo scale command is additive, in that each time you execute it, another replica is created which is added to the Postgres cluster.\n===== Scaling Down\nYou can cause a replica to be removed from a Postgres cluster by scaling down the replicas.\n==== Syntax\n$ pgo scaledown NAME [FLAGS]\n==== Flags\n[width=\"100%\",cols=\"5,1,1, 13\",options=\"header\"]\n    |Name |Shorthand |Input |Usage |--query |N/A |N/A | Prints the list of targetable replica candidates.\n |--delete-data |-d |N/A | Causes the data for the scaled down replica to be removed permanently.\n |--target |N/A |String | The name of a replica to delete.\n     List the targetable replicas for a given cluster: \u0026#8230;\u0026#8203;. pgo scaledown mycluster --query \u0026#8230;\u0026#8203;.\nYou can scale down a cluster as follows: \u0026#8230;\u0026#8203;. pgo scaledown mycluster --target=mycluster-replica-xxxx \u0026#8230;\u0026#8203;.\nDelete the PVC and associated data for the scaled down replica by using the --delete-data command flag: \u0026#8230;\u0026#8203;. pgo scaledown mycluster --target=mycluster-replica-xxxx --delete-data \u0026#8230;\u0026#8203;.\n===== Testing Replication\nThere are 2 service connections available to the PostgreSQL cluster. One is to the primary database which allows read-write SQL processing, and the other is to the set of read-only replica databases. The replica service performs round-robin load balancing to the replica databases.\nYou can connect to the primary database and verify that it is replicating to the replica databases as follows: \u0026#8230;\u0026#8203;. psql -h 10.107.180.159 -U postgres postgres -c 'table pg_stat_replication' \u0026#8230;\u0026#8203;.\n===== Specifying Nodes\nThe scale command will let you specify a --node-label flag which can be used to influence what Kube node the replica will be scheduled upon.\n\u0026#8230;\u0026#8203;. pgo scale mycluster --node-label=speed=fast \u0026#8230;\u0026#8203;.\nIf you don\u0026#8217;t specify a --node-label flag, a node affinity rule of NotIn will be specified to prefer that the replica be schedule on a node that the primary is not running on.\n===== Overriding Storage Defaults\nYou can also dictate what container resource and storage configurations will be used for a replica by passing in extra command flags: \u0026#8230;\u0026#8203;. pgo scale mycluster --storage-config=storage1 --resources-config=small \u0026#8230;\u0026#8203;.\n=== Manual Failover\nStarting with Release 2.6, there is a manual failover command which can be used to promote a replica to a primary role in a PostgreSQL cluster.\nThis process includes the following actions:\n* pick a target replica to become the new primary * delete the current primary deployment to avoid user requests from going to multiple primary databases (split brain) * promote the targeted replica using pg_ctl promote, this will cause PostgreSQL to go into read-write mode * re-label the targeted replica to use the primary labels, this will match the primary service selector and cause new requests to the primary to be routed to the new primary (targeted replica)\n==== Syntax\n$ pgo failover NAME [FLAGS]\n==== Flags\n[width=\"100%\",cols=\"5,1,1, 13\",options=\"header\"]\n    |Name |Shorthand |Input |Usage\n |--no-prompt |-n |N/A | No command line confirmation.\n |--query |N/A |N/A | Prints the list of failover candidates.\n |--target |N/A |String | The replica target which the failover will occur on.\n     ==== Examples\n===== Manual Failover\nThe command works like this: \u0026#8230;\u0026#8203;. pgo failover mycluster --query \u0026#8230;\u0026#8203;.\nThat command will show you a list of replica targets you can choose to failover to. You will select one of those for the following command: \u0026#8230;\u0026#8203;. pgo failover mycluster --target=mycluster-abxq \u0026#8230;\u0026#8203;.\nThere is a CRD called pgtask that will hold the failover request and also the status of that request. You can view the status by viewing it: \u0026#8230;\u0026#8203;. kubectl get pgtasks mycluster-failover -o yaml \u0026#8230;\u0026#8203;.\nOnce completed, you will see a new replica has been started to replace the promoted replica, which happens automatically due to the re-label. The Deployment will recreate its pod because of this. The failover typically takes only a few seconds, however, the creation of the replacement replica can take longer depending on how much data is being replicated.\n=== Upgrading PostgreSQL\nThe upgrade command will allow you to upgrade the PostgreSQL version of your cluster with the pg_upgrade utility. Minor or major upgrades are supported. The Crunchy Container Suite crunchy-upgrade container is responsible for performing this task.\nBy default, it will request confirmation for the command as the operator deletes the existing contaniers of the database or cluster and recreates them using the currently defined PostgreSQL contaner image specified in the pgo.yaml configuration file or with a defined --ccp-image-tag flag. The database data files remain untouched throughout the upgrade.\nOnce the upgrade job is completed, the operator will create the original database or cluster container mounted with the new PVC which contains the upgraded database files.\nAs the upgrade is processed, the status of the pgupgrade CRD is updated to give the user some insight into how the upgrade is proceeding. Upgrades like this can take a long time if your database is large. The operator creates a watch on the upgrade job to know when and how to proceed.\n==== Syntax\n$ pgo upgrade NAME [FLAGS]\n==== Flags\n[width=\"100%\",cols=\"5,1,1, 13\",options=\"header\"]\n    |Name |Shorthand |Input |Usage\n |--ccp-image-tag |N/A |String | The CCPImageTag to use for cluster creation. If specified, overrides the pgo.yaml setting.\n     ==== Examples\n===== Minor Upgrade\nPerform a minor PostgreSQL version upgrade: \u0026#8230;\u0026#8203;. pgo upgrade mycluster \u0026#8230;\u0026#8203;.\n===== Overriding Version\nOverride the CcpImageTag variable defined in the pgo.yaml configuration file: \u0026#8230;\u0026#8203;. pgo upgrade mycluster --ccp-image-tag=centos7-9.6.9-1.8.3 pgo upgrade mycluster --ccp-image-tag=centos7-9.6.9-1.8.3 \u0026#8230;\u0026#8203;.\n===== Delete Upgrade\nTo remove an upgrade CRD, issue the following: \u0026#8230;\u0026#8203;. pgo delete upgrade \u0026#8230;\u0026#8203;.\n=== Labels\nLabels can be applied to clusters and nested according to their type, with any string input being valid.\n==== Syntax\n$ pgo label [NAME]\n all [FLAGS]\n==== Flags\n[width=\"100%\",cols=\"5,1,1, 13\",options=\"header\"]\n    |Name |Shorthand |Input |Usage\n |--dry-run |N/A |N/A | Shows the clusters that the label would be applied to, without labelling them.\n |--label |N/A |String | The new label to apply for any selected or specified clusters.\n |--selector |-s |String | The selector to use for cluster filtering.\n     ==== Examples\n===== Applying Labels\nYou can apply a user defined label to a cluster as follows: \u0026#8230;\u0026#8203;. pgo label mycluster --label=env=research \u0026#8230;\u0026#8203;.\nOr if you wanted to apply if to a selection of clusters: \u0026#8230;\u0026#8203;. pgo label --label=env=research --selector=project=xray pgo label all --label=env=research \u0026#8230;\u0026#8203;.\nIn the first example, a label of env=research is applied to the cluster mycluster. The second example will apply the label to any clusters that have an existing label of project=xray applied or to all clusters.\n===== Removing Labels\nYou can delete a user defined label from a cluster as follows: \u0026#8230;\u0026#8203;. pgo delete label mycluster --label=env=research \u0026#8230;\u0026#8203;.\n=== Creating SQL Policies\nPolicies are SQL files that can be applied to a single cluster, a selection of clusters, or to all newly created clusters by default.\nThey are automatically applied to any cluster you create if you define in your pgo.yaml configuration a CLUSTER.POLICIES value.\nPolicies are executed as the superuser or postgres user in PostgreSQL. These should therefore be exercised with caution.\n  ==== Syntax\n$ pgo create policy [NAME] [FLAGS]\n==== Flags\n[width=\"100%\",cols=\"5,1,1, 13\",options=\"header\"]\n    |Name |Shorthand |Input |Usage\n |--in-file |N/A |String | The policy file path to use for adding a policy.\n |--url |N/A |N/A | The url to use for adding a policy.\n     ==== Examples\n===== Creating Policies\nTo create a policy use the following syntax: \u0026#8230;\u0026#8203;. pgo create policy policy1 --in-file=/tmp/policy1.sql pgo create policy policy1 --url=https://someurl/policy1.sql \u0026#8230;\u0026#8203;.\nWhen you execute this command, it will create a policy named policy1 using the input file /tmp/policy1.sql as input. It will create on the server a PgPolicy CRD with the name policy1 that you can examine as follows: \u0026#8230;\u0026#8203;. kubectl get pgpolicies policy1 -o json \u0026#8230;\u0026#8203;.\n===== Apply Policies\nTo apply an existing policy to a set of clusters, issue a command like this: \u0026#8230;\u0026#8203;. pgo apply policy1 --selector=name=mycluster \u0026#8230;\u0026#8203;.\nWhen you execute this command, it will look up clusters that have a label value of name=mycluster and then it will apply the policy1 label to that cluster and execute the policy SQL against that cluster using the postgres user account.\n===== Testing Policy Application\nYou can apply policies with a --dry-run flag applied to test which clusters the policy would be applied to without actually executing the SQL: \u0026#8230;\u0026#8203;. pgo apply policy1 --dry-run --selector=name=mycluster \u0026#8230;\u0026#8203;.\n===== Show Policies\nTo view policies, either all of them or a specific one: \u0026#8230;\u0026#8203;. pgo show policy all pgo show policy somepolicy \u0026#8230;\u0026#8203;.\n===== Show Clusters with a Specific Policy\nIf you want to view the clusters than have a specific policy applied to them, you can use the --selector flag as follows to filter on a policy name (e.g. policy1): \u0026#8230;\u0026#8203;. pgo show cluster --selector=policy1=pgpolicy \u0026#8230;\u0026#8203;.\n===== Delete Policies\nTo delete a policy use the following form: \u0026#8230;\u0026#8203;. pgo delete policy policy1 pgo delete policy all \u0026#8230;\u0026#8203;.\n=== Loading Data\nA CSV file loading capability is supported. This can be tested through creating a SQL Policy which will create a database table that will be loaded with the CSV data. The loading is based on a load definition found in the sample-load-config.yaml file. In that file, the data to be loaded is specified. When the pgo load command is executed, Jobs will be created to perform the loading for each cluster that matches the selector filter.\nThe load configuration file has the following YAML attributes:\n[width=\"100%\",cols=\"m,2\",frame=\"topbot\",options=\"header\"]\n   ======================\n   Attribute\n   Description\n   COImagePrefix\n   the pgo-load image prefix to use for the load job\n   COImageTag\n   the pgo-load image tag to use for the load job\n   DbDatabase\n   the database schema to use for loading the data\n   DbUser\n   the database user to use for loading the data\n   DbPort\n   the database port of the database to load\n   TableToLoad\n   the PostgreSQL table to load\n   FilePath\n   the name of the file to be loaded\n   FileType\n   either csv or json, determines the type of data to be loaded\n   PVCName\n   the name of the PVC that holds the data file to be loaded\n   SecurityContext\n   either fsGroup or SupplementalGroup values\n   ======================\nFor running the pgo load examples, you can create the csv-pvc PVC by running: \u0026#8230;\u0026#8203;. kubectl create -f examples/csv-pvc.json \u0026#8230;\u0026#8203;.\nThen you can copy sample load files as referenced by the examples into that PVC location (e.g. /data or /nfsfileshare).\n==== Syntax\n$ pgo load [FLAGS]\n==== Flags\n[width=\"100%\",cols=\"5,1,1, 13\",options=\"header\"]\n    |Name |Shorthand |Input |Usage\n |--load-config |N/A |String | The load configuration to use that defines the load job.\n |--policies |N/A |String | The policies to apply before loading a file, comma separated.\n |--selector |-s |String | The selector to use for cluster filtering.\n     ==== Examples\n===== Loading CSV Files\nLoad a sample CSV file into a database as follows: \u0026#8230;\u0026#8203;. pgo load --load-config=$COROOT/examples/sample-load-config.yaml --selector=name=mycluster \u0026#8230;\u0026#8203;.\n===== Including Policies\nIf you include the --policies flag, any specified policies will be applied prior to the data being loaded. For example: \u0026#8230;\u0026#8203;. pgo load --policies=\"rlspolicy,xrayapp\" --load-config=$COROOT/examples/sample-load-config.yaml --selector=name=mycluster \u0026#8230;\u0026#8203;.\n== Authentication\n=== Credential Management\nThe pgo user, pgo create user, and pgo delete user commands are used to manage credentials for the PostgreSQL clusters.\n==== Syntax\n$ pgo user [FLAGS]\n==== Flags\n[width=\"100%\",cols=\"5,1,1, 13\",options=\"header\"]\n    |Name |Shorthand |Input |Usage\n |--change-password |N/A |String | Updates the password for a user on selective clusters.\n |--db |N/A |String | Grants the user access to a database.\n |--expired |N/A |String | Specifies number of days to check for expiring passwords when using --update-passwords flag to update passwords.\n |--selector |-s |String | The selector to use for cluster filtering.\n |--update-passwords |N/A |N/A | Performs password updating on expired passwords.\n |--password |N/A |N/A | Allows user to specify a password instead of using a generated password.\n |--valid-days |N/A |Int | Sets passwords for new users to X days (default 30).\n |--password-length |N/A |Int | When no password is provided, generates a password with this number of characters (default 12).\n     ==== Examples\n===== Basic User Creation\nTo create a new Postgres user assigned to the mycluster cluster, execute (password will be auto generated and 12 characters long): \u0026#8230;\u0026#8203;. pgo create user sally --selector=name=mycluster \u0026#8230;\u0026#8203;.\n===== Managed User Creation\nTo create a new Postgres user to the mycluster cluster that has credentials created with Kubernetes Secrets, use the --managed flag: \u0026#8230;\u0026#8203;. pgo create user sally --managed --selector=name=mycluster --password=somepass \u0026#8230;\u0026#8203;.\nA managed account is one that the Operator can manipulate as well; when you run pgo test mycluster the account is tested with the other default accounts, etc.\nWhen you create a managed user, if pgpool is part of your cluster, then pgpool is reconfigured to pick up the new user.\n===== Complex User Creation\nIn this example, a user named user1 is created with a valid until password date set to expire in 30 days. That user will be granted access to the userdb database. This user account also will have an associated Secret created to hold the password that was generated for this user. Any clusters that match the selector value will have this user created on it. \u0026#8230;\u0026#8203;. pgo create user user1 --valid-days=30 --db=userdb --selector=name=xraydb1 \u0026#8230;\u0026#8203;.\n===== Deleting Users\nTo delete a Postgres user in the mycluster cluster, execute: \u0026#8230;\u0026#8203;. pgo delete user sally --selector=name=mycluster \u0026#8230;\u0026#8203;.\nIf pgpool is part of your cluster, deletion of a managed user will cause pgpool to be reconfigured to pick up the user deletion.\n===== Change Password\nTo change the password for a user in the mycluster cluster (password will be auto generated and 12 characters long): \u0026#8230;\u0026#8203;. pgo user --change-password=sally --selector=name=mycluster \u0026#8230;\u0026#8203;.\nOr to change the password and set an expiration date: \u0026#8230;\u0026#8203;. pgo user --change-password=user1 --valid-days=10 --selector=name=xray1 \u0026#8230;\u0026#8203;.\nIn this example, a user named user1 has its password changed to a generated value and the valid until expiration date set to 10 days from now. This command will take effect across all clusters that match the selector. If you specify valid-days=-1 it will mean the password will not expire (e.g. infinity).\nIf pgpool is part of your cluster, changing a managed user password will cause pgpool to be reconfigured to pick up the password change.\n===== Updating Expired Passwords\nTo update expired passwords in a cluster: \u0026#8230;\u0026#8203;. pgo user --update-passwords --selector=name=mycluster --expired=5 \u0026#8230;\u0026#8203;.\n== pgbouncer Basics\nWhen a pgbouncer deployment is added into your cluster, it will cause the creation of a Secret that holds the pgbouncer configuration files: * pg_hba.conf * pgbouncer.ini * users.txt\nEach user that is defined for your cluster is used to define the pgbouncer credentials, using the same password.\nThe pgbouncer configuration includes a connection to a database with the name of your cluster (e.g. mycluster) and also a database that connects to the cluster\u0026#8217;s replicas (e.g. mycluster-replica).\nWhen you add a new user, it will cause the pgbouncer to be reconfigured and a new secret to be generated, the pgbouncer is restarted to pick up the new configuration file.\nAdding a pgbouncer deployment into your PG cluster follows a sequence similar to this:\n\u0026#8230;\u0026#8203;. pgo create cluster mycluster --pgbouncer \u0026#8230;\u0026#8203;.\nYou can also add pgbouncer after a cluster has been created: \u0026#8230;\u0026#8203;. pgo create pgbouncer mycluster \u0026#8230;\u0026#8203;.\nNOTE: currently you are required to have a replica in your PG cluster for the pgbouncer sidecar to effectively work, a replica is currently not automatically created when you create a PG cluster.\n== pgpool Basics\nAdding a pgpool deployment into your PG cluster follows a sequence similar to this:\n\u0026#8230;\u0026#8203;. pgo create cluster mycluster \u0026#8230;\u0026#8203;.\nThen you will scale it up: \u0026#8230;\u0026#8203;. pgo scale mycluster \u0026#8230;\u0026#8203;.\nThen you will add managed users of your choice: \u0026#8230;\u0026#8203;. pgo create user somenewuser mycluster --managed \u0026#8230;\u0026#8203;.\nThen you will create a pgpool for the new cluster: \u0026#8230;\u0026#8203;. pgo create pgpool mycluster \u0026#8230;\u0026#8203;.\nThis will create a pgpool user credential for each pgo managed user you have created.\n=== Create pgpool\nThe create pgpool command will create a pgpool deployment that is part of a cluster.\n==== Syntax\n$ pgo create pgpool CLUSTERNAME [FLAGS]\n==== Flags\n[width=\"100%\",cols=\"5,1,1, 13\",options=\"header\"]\n    |Name |Shorthand |Input |Usage\n |--selector |-s |String | The selector to use for cluster filtering.\n     ==== Examples\n===== Simple Creation\nCreate a pgpool: \u0026#8230;\u0026#8203;. pgo create pgpool mycluster \u0026#8230;\u0026#8203;.\nNOTE: currently you are required to have a replica in your PG cluster for the pgpool sidecar to effectively work, a replica is currently not automatically created when you create a PG cluster.\n=== Delete pgpool\nThe delete pgpool command will by delete the pgpool deployment that is part of a cluster.\n==== Syntax\n$ pgo delete pgpool CLUSTERNAME [FLAGS]\n==== Flags\n[width=\"100%\",cols=\"5,1,1, 13\",options=\"header\"]\n    |Name |Shorthand |Input |Usage\n |--selector |-s |String | The selector to use for cluster filtering.\n     ==== Examples\n===== Simple Deletion\nDelete a pgpool: \u0026#8230;\u0026#8203;. pgo delete pgpool mycluster \u0026#8230;\u0026#8203;.\n=== Workflow\nStarting with Release 3.4, there is a workflow concept that you can use to check the status of a cluster creation. When you create a cluster (e.g. pgo create cluster), you will see in the response a workflow ID. You can use that ID to check the status of the cluster creation.\n==== Syntax\n\u0026#8230;\u0026#8203;. pgo show workflow ID \u0026#8230;\u0026#8203;.\n        "
},
{
	"uri": "https://crunchydata.github.io/postgres-operator/latest/installation/helm-chart/",
	"title": "Helm Chart",
	"tags": [],
	"description": "",
	"content": "Latest Release: 3.5.0 2018-12-14\n Helm Chart First, pull prebuilt versions from Dockerhub of the postgres-operator containers, specify the image versions, and execute the following Makefile target -\n export CO_IMAGE_PREFIX=crunchydata export CO_IMAGE_TAG=centos7-3.5.0 make pull   Then, build and deploy the operator using the provided Helm chart -\n cd $COROOT/chart helm install ./postgres-operator helm ls     Next Steps Next, visit the Deployment page to deploy the Operator, verify the installation, and view various storage configurations.\n   "
},
{
	"uri": "https://crunchydata.github.io/postgres-operator/latest/how-it-works/",
	"title": "How it Works",
	"tags": [],
	"description": "",
	"content": "Table of Contents Reference Architecture Custom Resource Definitions Command Line Interface Operator Deployment CLI Design Verbs   Affinity Debugging Persistent Volumes PostgreSQL Operator Deployment Strategies Strategies Specifying a Strategy Strategy Template Files Default Cluster Deployment Strategy (1) Cluster Deletion Custom Postgres Configurations Metrics Collection Manual Failover Auto Failover     Latest Release: 3.5.0 2018-12-14\n Reference Architecture So, what does the Postgres Operator actually deploy when you create a cluster?\n   On this diagram, objects with dashed lines are components that are optionally deployed as part of a PostgreSQL Cluster by the operator. Objects with solid lines are the fundamental and required components.\n For example, within the Primary Deployment, the metrics container is completely optional. That component can be deployed using either the operator configuration or command line arguments if you want to cause metrics to be collected from the Postgres container.\n Replica deployments are similar to the primary deployment but are optional. A replica is not required to be created unless the capability for one is necessary. As you scale up the Postgres cluster, the standard set of components gets deployed and replication to the primary is started.\n Notice that each cluster deployment gets its own unique Persistent Volumes. Each volume can use different storage configurations which is quite powerful.\n   Custom Resource Definitions Kubernetes Custom Resource Definitions are used in the design of the PostgreSQL Operator to define the following -\n   Cluster - pgclusters\n  Backup - pgbackups\n  Upgrade - pgupgrades\n  Policy - pgpolicies\n  Tasks - pgtasks\n     Command Line Interface The pgo command line interface (CLI) is used by a normal end-user to create databases or clusters, or make changes to existing databases.\n The CLI interacts with the apiserver REST API deployed within the postgres-operator deployment.\n From the CLI, users can view existing clusters that were deployed using the CLI and Operator. Objects that were not previously created by the Crunchy Operator are now viewable from the CLI.\n   Operator Deployment The PostgreSQL Operator runs within a Deployment in the Kubernetes cluster. An administrator will deploy the operator deployment using the provided script. Once installed and running, the Operator pod will start watching for certain defined events.\n The operator watches for create/update/delete actions on the pgcluster custom resource definitions. When the CLI creates for example a new pgcluster custom resource definition, the operator catches that event and creates pods and services for that new cluster request.\n   CLI Design The CLI uses the cobra package to implement CLI functionality like help text, config file processing, and command line parsing.\n The pgo client is essentially a REST client which communicates to the pgo-apiserver REST server running within the Operator pod. In some cases you might want to split the apiserver out into its own Deployment but the default deployment has a consolidated pod that contains both the apiserver and operator containers simply for convenience of deployment and updates.\n Verbs A user works with the CLI by entering verbs to indicate what they want to do, as follows.\n pgo show cluster all pgo delete cluster db1 db2 db3 pgo create cluster mycluster   In the above example, the show, backup, delete, and create verbs are used. The CLI is case sensitive and supports only lowercase.\n    Affinity You can have the Operator add an affinity section to a new Cluster Deployment if you want to cause Kubernetes to attempt to schedule a primary cluster to a specific Kubernetes node.\n You can see the nodes on your Kube cluster by running the following -\n kubectl get nodes   You can then specify one of those names (e.g. kubeadm-node2) when creating a cluster -\n pgo create cluster thatcluster --node-name=kubeadm-node2   The affinity rule inserted in the Deployment will used a preferred strategy so that if the node were down or not available, Kube would go ahead and schedule the Pod on another node.\n You can always view the actual node your cluster pod is scheduled on through the following command.\n kubectl get pod -o wide   When you scale up a Cluster and add a replica, the scaling will take into account the use of --node-name. If it sees that a cluster was created with a specific node name, then the replica Deployment will add an affinity rule to attempt to schedule the replica on a different node than the node the primary is schedule on. This provides a simple version of high availability and causes the primary and replicas to not live on the same Kubernetes node.\n   Debugging To see if the operator pod is running enter the following -\n kubectl get pod -l 'name=postgres-operator'   To verify the operator is running and has deployed the Custom Resources execute the following -\n kubectl get crd   The full list of CRDs that are created over time are shown below.\n NAME KIND pgbackups.cr.client-go.k8s.io CustomResourceDefinition.v1beta1.apiextensions.k8s.io pgclusters.cr.client-go.k8s.io CustomResourceDefinition.v1beta1.apiextensions.k8s.io pgpolicies.cr.client-go.k8s.io CustomResourceDefinition.v1beta1.apiextensions.k8s.io pgpolicylogs.cr.client-go.k8s.io CustomResourceDefinition.v1beta1.apiextensions.k8s.io pgupgrades.cr.client-go.k8s.io CustomResourceDefinition.v1beta1.apiextensions.k8s.io pgtasks.cr.client-go.k8s.io CustomResourceDefinition.v1beta1.apiextensions.k8s.io     Persistent Volumes Currently, the operator does not delete persistent volumes by default. Instead, it deletes the claims on the volumes. Starting with release 2.4, the Operator will create Jobs that actually run rm commands on the data volumes before actually removing the Persistent Volumes if the user passes a `--delete-data ` flag when deleting a database cluster.\n Likewise, if the user passes --delete-backups during cluster deletion a Job is created to remove all the backups for a cluster include the related Persistent Volume.\n   PostgreSQL Operator Deployment Strategies This section describes the various deployment strategies offered by the operator. A deployment in this case is the set of objects created in Kubernetes when a custom resource definition of type pgcluster is created. CRDs are created by the pgo client command and acted upon by the postgres operator.\n Strategies To support different types of deployments, the operator supports multiple strategy implementations. Currently there is only a default cluster strategy.\n In the future, more deployment strategies will be supported to offer users more customization to what they see deployed in their Kubernetes cluster.\n Being open source, users can also write their own strategy!\n  Specifying a Strategy In the pgo client configuration file, there is a `CLUSTER.STRATEGY `setting. The current value of the default strategy is 1. If you don\u0026#8217;t set that value, the default strategy is assumed. If you set that value to something not supported, the operator will log an error.\n  Strategy Template Files Each strategy supplies its set of templates used by the operator to create new pods, services, etc.\n When the operator is deployed, part of the deployment process is to copy the required strategy templates into a ConfigMap (operator-conf) that gets mounted into /operator-conf within the operator pod.\n The directory structure of the strategy templates is as follows -\n |-- backup-job.json |-- cluster | |-- 1 | |-- cluster-deployment-1.json | |-- cluster-replica-deployment-1.json | |-- cluster-service-1.json | |-- pvc.json   In this structure, each strategy\u0026#8217;s templates live in a subdirectory that matches the strategy identifier. The default strategy templates are denoted by the value of 1 in the directory structure above.\n If you add another strategy, the file names must be unique within the entire strategy directory. This is due to the way the templates are stored within the ConfigMap.\n  Default Cluster Deployment Strategy (1) Using the default cluster strategy, a cluster when created by the operator will create the following on a Kubernetes cluster -\n   deployment running a Postgres primary container with replica count of 1\n  service mapped to the primary Postgres database\n  service mapped to the replica Postgres database\n  PVC for the primary will be created if not specified in configuration, this assumes you are using a non-shared volume technology (e.g. Amazon EBS), if the CLUSTER.PVC_NAME value is set in your configuration then a shared volume technology is assumed (e.g. HostPath or NFS), if a PVC is created for the primary, the naming convention is clustername where clustername is the name of your cluster.\n   If you want to add a Postgres replica to a cluster, you will scale the cluster. For each replica-count, a Deployment will be created that acts as a PostgreSQL replica.\n This is very different than using a StatefulSet to scale up PostgreSQL. Why would you do it this way? Imagine a case where you want different parts of your PostgreSQL cluster to use different storage configurations,. With this method, it can be done through using specific placement and deployments of each part of the cluster.\n This same concept applies to node selection for the PostgreSQL cluster components. The Operator will let you define precisely which node that the PostgreSQL component should be placed upon using node affinity rules.\n  Cluster Deletion When you run the following, the cluster and its services will be deleted. However, the data files and backup files will remain as well as the PVCs for this cluster.\n pgo delete cluster mycluster   However, to remove the data files from the PVC you can pass the following flag -\n --delete-data   This causes a workflow to be started to remove the data files on the primary cluster deployment PVC.\n The following flag will cause all of the backup files to be removed.\n --delete-backups   The data removal workflow includes the following steps -\n   create a pgtask CRD to hold the PVC name and cluster name to be removed\n  the CRD is watched, and on an ADD will cause a Job to be created that will run the rmdata container using the PVC name and cluster name as parameters which determine the PVC to mount, and the file path to remove under that PVC\n  the rmdata Job is watched by the Operator, and upon a successful status completion the actual PVC is removed\n   This workflow insures that a PVC is not removed until all the data files are removed. Also, a Job was used for the removal of files since that can be a time consuming task.\n The files are removed by the rmdata container which essentially issues the following command to remove the files -\n rm -rf /pgdata/\u0026lt;some path\u0026gt;    Custom Postgres Configurations Starting in release 2.5, users and administrators can specify a custom set of Postgres configuration files be used when creating a new Postgres cluster. The configuration files you can change include -\n   postgresql.conf\n  pg_hba.conf\n  setup.sql\n   Different configurations for PostgreSQL might be defined for the following -\n   OLTP types of databases\n  OLAP types of databases\n  High Memory\n  Minimal Configuration for Development\n  Project Specific configurations\n  Special Security Requirements\n   Global ConfigMap If you create a configMap called pgo-custom-pg-config with any of the above files within it, new clusters will use those configuration files when setting up a new database instance. You do NOT have to specify all of the configuration files. It is entirely up to your use case to determine which to use.\n This global configmap holds the pgbackrest.conf file, this is required for pgbackrest backups to work! This also applies to ANY custom configuration file you wish to use, it MUST contain a pgbackrest.conf file as a key. See the example for pgo-custom-pg-config for the pgbackrest.conf file and how to add it to your custom configuration ConfigMap.\n An example set of configuration files and a script to create the global configMap is found at -\n $COROOT/examples/custom-config   If you run the create.sh script there, it will create the configMap that will include the PostgreSQL configuration files within that directory.\n  Config Files Purpose The postgresql.conf file is the main Postgresql configuration file that allows the definition of a wide variety of tuning parameters and features.\n The pg_hba.conf file is the way Postgresql secures client access.\n The setup.sql file is a Crunchy Container Suite configuration file used to initially populate the database after the initial initdb is run when the database is first created. Changes would be made to this if you wanted to define which database objects are created by default.\n The pgbackrest.conf file is merely used to tell the Postgres container that it should allocate a pgbackrest configuration directory when initializing the container. The contents of this file do not get inspected but the name has to be pgbackrest.conf. This requirement will change in upcoming operator releases.\n  Granular Config Maps Granular config maps can be defined if it is necessary to use a different set of configuration files for different clusters rather than having a single configuration (e.g. Global Config Map). A specific set of ConfigMaps with their own set of PostgreSQL configuration files can be created. When creating new clusters, a --custom-config flag can be passed along with the name of the ConfigMap which will be used for that specific cluster or set of clusters.\n  Defaults If there\u0026#8217;s no reason to change the default PostgreSQL configuration files that ship with the Crunchy Postgres container, there\u0026#8217;s no requirement to. In this event, continue using the Operator as usual and avoid defining a global configMap.\n  Labeling When a custom configMap is used in cluster creation, the Operator labels the primary Postgres Deployment with a label of custom-config and a value of what configMap was used when creating the database.\n Commands coming in future releases will take advantage of this labeling.\n   Metrics Collection If you add a --metrics flag to pgo create cluster it will cause the crunchy-collect container to be added to your Postgres cluster.\n That container requires you run the crunchy-metrics containers as defined within the crunchy-containers project.\n See the crunchy-containers Metrics example for more details on setting up the crunchy-metrics solution.\n  Manual Failover With manual failover some key features include:\n   when you perform a failover, a new replica is created to replace the replica that was promoted to even out the cluster to the original number of replicas\n  when you perform a failover, the promoted replica is removed from the pgreplica CRD to represent the current truth\n   The pgo failover --query command will return a list of replica targets which you can select from. That list include the Ready status of the database as well as the Kube node name it is running on.\n  Auto Failover Starting with release 3.1, there is an auto failover mechanism that can be leveraged by pgo users if enabled.\n This feature will cause the operator to start a timer on a database primary that has received a NotReady status after the database has started. This can happen if for instance the primary database loses the connection to its database storage (e.g. gluster, NFS).\n Once the timer is started, if the primary database does not get back to a Ready status within that timer period, a failover is triggered for this cluster. The failover target is selected by the auto failover logic.\n The amount of time (in seconds) the auto failover timer will wait before triggering a failover is determined by the following pgo.yaml setting:\n AutofailSleepSeconds: 9   If the above setting is not configured a default value of 30 seconds is chose.\n The logic of auto failover works like this:\n   the readiness probe on the primary database container is executed every few seconds to check the readiness of the database, this is what tells Kubernetes whether or not the container is Ready or NotReady.\n  if a NotReady state is detected then that event is caught by the operator which is watching for database containers created by the operator\n  upon a NotReady event, a timer is started for that database which acts as the final check as to if a failover is required for that database\n  if the timer expires and the state is still Not Ready then the manual failover logic is executed for this cluster which causes a promotion of a replica to primary, and also creates a replacement replica\n  only replica targets with a status of Ready will be used to select the target to promote\n   The readiness probe settings are defined in the following template:\n conf/postgres-operator/cluster/1/cluster-deployment-1.json   The readiness probe settings determine how often the database check is performed. See the Kubernetes documentation on readiness probes for more details on these settings.\n    "
},
{
	"uri": "https://crunchydata.github.io/postgres-operator/latest/contributing/",
	"title": "Contributing",
	"tags": [],
	"description": "",
	"content": "Latest Release: 3.5.0 2018-12-14\n Getting Started Welcome! Thank you for your interest in contributing. Before submitting a new issue or pull request to the Crunchy Data PostgreSQL Operator project on GitHub, please review any open or closed issues here in addition to any existing open pull requests.\n   Documentation The documentation website (located at https://crunchydata.github.io/postgres-operator/) is generated using Hugo and GitHub Pages.\n   Hosting Hugo Locally (Optional) If you would like to build the documentation locally, view the official Installing Hugo guide to set up Hugo locally. You can then start the server by running the following commands -\n cd $CCPROOT/hugo/ vi config.toml hugo server   The local version of the Hugo server is accessible by default from localhost:1313. Once you\u0026#8217;ve run hugo server, that will let you interactively make changes to the documentation as desired and view the updates in real-time.\n   "
},
{
	"uri": "https://crunchydata.github.io/postgres-operator/latest/installation/configuration/",
	"title": "Configuration",
	"tags": [],
	"description": "",
	"content": "Table of Contents Overview Openshift Container Platform Security Configuration Kube RBAC Basic Authentication Configure TLS pgo RBAC REST API Configuration PostgreSQL Operator Container Configuration   Bash Completion REST API Deploying pgPool Storage Configuration   Latest Release: 3.5.0 2018-12-14\n Overview This document describes how to configure the operator beyond the default configurations in addition to detailing what the configuration settings mean.\n   Openshift Container Platform To run the Operator on Openshift Container Platform note the following requirements -\n   Openshift Container Platform 3.7 or greater is required due to the dependence on Custom Resource Definitions.\n  The CO_CMD environment variable should be set to oc when operating in an Openshift environment.\n     Security Configuration Kube RBAC The cluster-rbac.yaml file is executed a single time when installing the Operator. This file, executed by a Kubernetes user with cluster-admin priviledges, does the following:\n   Creates Customer Resource Definitions\n  Grants get access to Kube Node resources to the postgres-operator service account.\n   The rbac.yaml file is also executed a single time when installing the Operator. This file creates Role scoped privileges which are granted to the postgres-operator service account. The postgres-operator service account is used by the apiserver and postgres-operator containers to access Kubernetes resources.\n Both of these RBAC files are executed by the deploy/install-rbac.sh script. It can also be installed through running make installrbac in the $CCPROOT directory.\n   Warning  The CO_NAMESPACE environment variable determines the namespace that is used within the deployment of the operator. If you are deploying to the demo namespace, the following should setting should be defined in your .bashrc: export CO_NAMESPACE=demo\n     See here for more details on how to enable RBAC roles and modify the scope of the permissions to suit your needs.\n    Basic Authentication Basic authentication between the host and the apiserver is required. It will be necessary to configure the pgo client to specify a basic authentication username and password through the creation a file in the user\u0026#8217;s home directory named .pgouser. It will look similar to this, and contain only a single line -\n username:password   The above excerpt specifies a username of username and a password of password. These values will be read by the pgo client and passed to the apiserver on each REST API call.\n For the apiserver, a list of usernames and passwords is specified in the pgo-auth-secret Secret. The values specified in a deployment are found in the following location -\n $COROOT/conf/postgres-operator/pgouser   The sample configuration for pgouser is as follows -\n username:password:pgoadmin testuser:testpass:pgoadmin readonlyuser:testpass:pgoreader   Modify these values to be unique to your environment.\n If the username and password passed by clients to the apiserver do not match, the REST call will fail and a log message will be produced in the apiserver container log. The client will receive a 401 HTTP status code if they are not able to authenticate.\n If the pgouser file is not found in the home directory of the pgo user then the next searched location is /etc/pgo/pgouser. If the file is not found in either of the locations, the pgo client searches for the existence of a PGOUSER environment variable in order to locate a path to the basic authentication file.\n Basic authentication can be entirely disabled by setting the BasicAuth setting in the pgo.yaml configuration file to false.\n  Configure TLS TLS is used to secure communications to the apiserver. Sample keys and certifications that can be used by TLS are found here -\n $COROOT/conf/postgres-operator/server.crt $COROOT/conf/postgres-operator/server.key   If you want to generate your own keys, you can use the script found in -\n $COROOT/bin/make-certs.sh   The pgo client is required to use keys to connect to the apiserver. Specify the keys for pgo by setting the following environment variables -\n export PGO_CA_CERT=$COROOT/conf/postgres-operator/server.crt export PGO_CLIENT_CERT=$COROOT/conf/postgres-operator/server.crt export PGO_CLIENT_KEY=$COROOT/conf/postgres-operator/server.key   You can also specify these credentials using the following command flags where you can reference they keys from any file path directly:\n pgo version --pgo-ca-cert=/tmp/server.crt --pgo-client-cert=/tmp/server.crt --pgo-client-key=/tmp/server.key   The sample server keys are used as the client keys; adjust to suit security requirements.\n For the apiserver TLS configuration, the keys are included in the apiserver-conf-secret Secret when the apiserver is deployed. See the $COROOT/deploy/deploy.sh script which is where the secret is created.\n The apiserver listens on port 8443 (e.g. https://postgres-operator:8443) by default.\n You can set InsecureSkipVerify to true by setting the NO_TLS_VERIFY environment variable in the deployment.json file to true. By default this value is set to false if you do not specify a value.\n  pgo RBAC The pgo command line utility talks to the apiserver REST API instead of the Kubernetes API. It is therefore necessary for the pgo client to make use of RBAC configuration.\n Starting in Release 3.0, the /conf/postgres-operator/pgorole is used to define some sample pgo roles, pgadmin and pgoreader.\n These roles are meant as examples that you can configure to suit security requirements as necessary. The pgadmin role grants a user authorization to all pgo commands. The pgoreader only grants access to pgo commands that display information such as pgo show cluster.\n The pgorole file is read at start up time when the operator is deployed to the Kubernetes cluster.\n Also, the pgouser file now includes the role that is assigned to a specific user as follows -\n username:password:pgoadmin testuser:testpass:pgoadmin readonlyuser:testpass:pgoreader   The following list shows the current complete list of possible pgo permissions -\n Table 1. pgo Permissions     Permission Description     ShowSecrets\n allow pgo show user\n   ShowCluster\n allow pgo show cluster\n   CreateCluster\n allow pgo create cluster\n   TestCluster\n allow pgo test mycluster\n   ShowBackup\n allow pgo show backup\n   CreateBackup\n allow pgo backup mycluster\n   DeleteBackup\n allow pgo delete backup mycluster\n   Label\n allow pgo label\n   Load\n allow pgo load\n   CreatePolicy\n allow pgo create policy\n   DeletePolicy\n allow pgo delete policy\n   ShowPolicy\n allow pgo show policy\n   ApplyPolicy\n allow pgo apply policy\n   ShowPVC\n allow pgo show pvc\n   CreateUpgrade\n allow pgo upgrade\n   ShowUpgrade\n allow pgo show upgrade\n   DeleteUpgrade\n allow pgo delete upgrade\n   CreateUser\n allow pgo create user\n   CreateFailover\n allow pgo failover\n   ShowConfig\n allow pgo show config\n   User\n allow pgo user\n   Version\n allow pgo version\n    If the user is unauthorized for a pgo command, the user will get back this response -\n FATA[0000] Authentication Failed: 40    REST API Configuration The postgres-operator pod includes the apiserver which is a REST API that pgo users are able to communicate with.\n The apiserver uses the following configuration files found in $COROOT/conf/postgres-operator to determine how the Operator will provision PostgreSQL containers -\n $COROOT/conf/postgres-operator/pgo.yaml $COROOT/conf/postgres-operator/pgo.lspvc-template.json $COROOT/conf/postgres-operator/pgo.load-template.json   Note that the default pgo.yaml file assumes you are going to use HostPath Persistent Volumes for your storage configuration. It will be necessary to adjust this file for NFS or other storage configurations. Some examples of how are listed in the manual installation document.\n The version of PostgreSQL container the Operator will deploy is determined by the CCPImageTag setting in the $COROOT/conf/postgres-operator/pgo.yaml configuration file. By default, this value is set to the latest release of the Crunchy Container Suite.\n The default pgo.yaml configuration file, included in $COROOT/conf/postgres-operator/pgo.yaml, looks like this -\n Cluster: PrimaryNodeLabel: ReplicaNodeLabel: CCPImagePrefix: crunchydata Metrics: false Badger: false CCPImageTag: centos7-10.6-2.2.0 LogStatement: none LogMinDurationStatement: 60000 Port: 5432 User: testuser Database: userdb PasswordAgeDays: 60 PasswordLength: 8 Strategy: 1 Replicas: 0 ArchiveMode: false ArchiveTimeout: 60 ServiceType: ClusterIP Backrest: false Autofail: false AutofailReplaceReplica: false PrimaryStorage: hostpathstorage BackupStorage: hostpathstorage ReplicaStorage: hostpathstorage Storage: hostpathstorage: AccessMode: ReadWriteMany Size: 1G StorageType: create nfsstorage: AccessMode: ReadWriteMany Size: 1G StorageType: create SupplementalGroups: 65534 storage2: AccessMode: ReadWriteMany Size: 1G StorageType: dynamic StorageClass: gluster-heketi Fsgroup: 26 storage3: AccessMode: ReadWriteOnce Size: 1G StorageType: dynamic StorageClass: rook-ceph-block Fsgroup: 26 DefaultContainerResources: DefaultLoadResources: DefaultLspvcResources: DefaultRmdataResources: DefaultBackupResources: DefaultPgbouncerResources: DefaultPgpoolResources: ContainerResources: small: RequestsMemory: 512Mi RequestsCPU: 0.1 LimitsMemory: 512Mi LimitsCPU: 0.1 large: RequestsMemory: 2Gi RequestsCPU: 2.0 LimitsMemory: 2Gi LimitsCPU: 4.0 Pgo: AutofailSleepSeconds: 9 Audit: false LSPVCTemplate: /pgo-config/pgo.lspvc-template.json LoadTemplate: /pgo-config/pgo.load-template.json COImagePrefix: crunchydata COImageTag: centos7-3.4.0-rc1   Values in the pgo configuration file have the following meaning:\n Table 2. pgo Configuration File Definitions     Setting Definition     BasicAuth\n if set to true will enable Basic Authentication\n   Cluster.PrimaryNodeLabel\n newly created primary deployments will specify this node label if specified, unless you override it using the --node-label command line flag, if not set, no node label is specifed\n   Cluster.ReplicaNodeLabel\n newly created replica deployments will specify this node label if specified, unless you override it using the --node-label command line flag, if not set, no node label is specifed\n   Cluster.CCPImageTag\n newly created containers will be based on this image version (e.g. centos7-10.4-1.8.3), unless you override it using the --ccp-image-tag command line flag\n   Cluster.Port\n the PostgreSQL port to use for new containers (e.g. 5432)\n   Cluster.LogStatement\n postgresql.conf log_statement value (required field) (works with crunchy-postgres \u0026gt;= 2.2.0)\n   Cluster.LogMinDurationStatement\n postgresql.conf log_min_duration_statement value (required field) (works with crunchy-postgres \u0026gt;= 2.2.0)\n   Cluster.User\n the PostgreSQL normal user name\n   Cluster.Strategy\n sets the deployment strategy to be used for deploying a cluster, currently there is only strategy 1\n   Cluster.Replicas\n the number of cluster replicas to create for newly created clusters\n   Cluster.Metrics\n boolean, if set to true will cause each new cluster to include crunchy-collect as a sidecar container for metrics collection, if set to false (default), users can still add metrics on a cluster-by-cluster basis using the pgo command flag --metrics\n   Cluster.Badger\n boolean, if set to true will cause each new cluster to include crunchy-pgbadger as a sidecar container for static log analysis, if set to false (default), users can still add pgbadger on a cluster-by-cluster basis using the pgo create cluster command flag --pgbadger\n   Cluster.Policies\n optional, list of policies to apply to a newly created cluster, comma separated, must be valid policies in the catalog\n   Cluster.PasswordAgeDays\n optional, if set, will set the VALID UNTIL date on passwords to this many days in the future when creating users or setting passwords, defaults to 60 days\n   Cluster.PasswordLength\n optional, if set, will determine the password length used when creating passwords, defaults to 8\n   Cluster.ArchiveMode\n optional, if set to true will enable archive logging for all clusters created, default is false.\n   Cluster.ArchiveTimeout\n optional, if set, will determine the archive timeout setting used when ArchiveMode is true, defaults to 60 seconds\n   Cluster.ServiceType\n optional, if set, will determine the service type used when creating primary or replica services, defaults to ClusterIP if not set, can be overridden by the user on the command line as well\n   Cluster.Backrest\n optional, if set, will cause clusters to have the pgbackrest volume PVC provisioned during cluster creation\n   Cluster.Autofail\n optional, if set, will cause clusters to be checked for auto failover in the event of a non-Ready status\n   Cluster.AutofailReplaceReplica\n optional, default is false, if set, will determine whether a replica is created as part of a failover to replace the promoted replica, the AutofailReplaceReplica setting in pgo.yaml is overrode with this command line flag if specified by a user.\n   PrimaryStorage\n required, the value of the storage configuration to use for the primary PostgreSQL deployment\n   ArchiveStorage\n optional, the value of the storage configuration to use for the pgwal (archive) volume for the Postgres container /pgwal volume, if not set, the PrimaryStorage setting is used\n   BackupStorage\n required, the value of the storage configuration to use for backups, including the storage for pgbackrest repo volumes\n   ReplicaStorage\n required, the value of the storage configuration to use for the replica PostgreSQL deployments\n   Storage.storage1.StorageClass\n for a dynamic storage type, you can specify the storage class used for storage provisioning(e.g. standard, gold, fast)\n   Storage.storage1.AccessMode\n the access mode for new PVCs (e.g. ReadWriteMany, ReadWriteOnce, ReadOnlyMany). See below for descriptions of these.\n   Storage.storage1.Size\n the size to use when creating new PVCs (e.g. 100M, 1Gi)\n   Storage.storage1.StorageType\n supported values are either dynamic, create, if not supplied, create is used\n   Storage.storage1.Fsgroup\n optional, if set, will cause a SecurityContext and fsGroup attributes to be added to generated Pod and Deployment definitions\n   Storage.storage1.SupplementalGroups\n optional, if set, will cause a SecurityContext to be added to generated Pod and Deployment definitions\n   Storage.storage1.MatchLabels\n optional, if set, will cause the PVC to add a matchlabels selector in order to match a PV, only useful when the StorageType is create, when specified a label of key=value is added to the PVC as a match criteria\n   DefaultContainerResource\n optional, the value of the container resources configuration to use for all database containers, if not set, no resource limits or requests are added on the database container\n   DefaultLoadResource\n optional, the value of the container resources configuration to use for pgo-load containers, if not set, no resource limits or requests are added on the database container\n   DefaultLspvcResource\n optional, the value of the container resources configuration to use for pgo-lspvc containers, if not set, no resource limits or requests are added on the database container\n   DefaultRmdataResource\n optional, the value of the container resources configuration to use for pgo-rmdata containers, if not set, no resource limits or requests are added on the database container\n   DefaultBackupResource\n optional, the value of the container resources configuration to use for crunchy-backup containers, if not set, no resource limits or requests are added on the database container\n   DefaultPgbouncerResource\n optional, the value of the container resources configuration to use for crunchy-pgbouncer containers, if not set, no resource limits or requests are added on the database container\n   DefaultPgpoolResource\n optional, the value of the container resources configuration to use for crunchy-pgpool containers, if not set, no resource limits or requests are added on the database container\n   ContainerResources.small.RequestsMemory\n request size of memory in bytes\n   ContainerResources.small.RequestsCPU\n request size of CPU cores\n   ContainerResources.small.LimitsMemory\n request size of memory in bytes\n   ContainerResources.small.LimitsCPU\n request size of CPU cores\n   ContainerResources.large.RequestsMemory\n request size of memory in bytes\n   ContainerResources.large.RequestsCPU\n request size of CPU cores\n   ContainerResources.large.LimitsMemory\n request size of memory in bytes\n   ContainerResources.large.LimitsCPU\n request size of CPU cores\n   Pgo.LSPVCTemplate\n the PVC lspvc template file that lists PVC contents\n   Pgo.LoadTemplate\n the load template file used for load jobs\n   Pgo.COImagePrefix\n image tag prefix to use for the Operator containers\n   Pgo.COImageTag\n image tag to use for the Operator containers\n   Pgo.Audit\n boolean, if set to true will cause each apiserver call to be logged with an audit marking\n    Storage Configurations You can define n-number of Storage configurations within the pgo.yaml file. Those Storage configurations follow these conventions -\n   they must have lowercase name (e.g. storage1)\n  they must be unique names (e.g. mydrstorage, faststorage, slowstorage)\n   These Storage configurations are referenced in the BackupStorage, ReplicaStorage, and PrimaryStorage configuration values. However, there are command line options in the pgo client that will let a user override these default global values to offer you the user a way to specify very targeted storage configurations when needed (e.g. disaster recovery storage for certain backups).\n You can set the storage AccessMode values to the following -\n   ReadWriteMany - mounts the volume as read-write by many nodes\n  ReadWriteOnce - mounts the PVC as read-write by a single node\n  ReadOnlyMany - mounts the PVC as read-only by many nodes\n   These Storage configurations are validated when the pgo-apiserver starts, if a non-valid configuration is found, the apiserver will abort. These Storage values are only read at apiserver start time.\n The following StorageType values are possible -\n   dynamic - this will allow for dynamic provisioning of storage using a StorageClass.\n  create - This setting allows for the creation of a new PVC for each PostgreSQL cluster using a naming convention of clustername. When set, the Size, AccessMode settings are used in constructing the new PVC.\n   The operator will create new PVCs using this naming convention: dbname where dbname is the database name you have specified. For example, if you run:\n pgo create cluster example1   It will result in a PVC being created named example1 and in the case of a backup job, the pvc is named example1-backup\n There are currently 3 sample pgo configuration files provided for users to use as a starting configuration -\n   pgo.yaml.nfs - this configuration specifies create storage to be used, this is used for NFS storage for example where you want to have a unique PVC created for each database\n  pgo.yaml.storageclass - this configuration specifies dynamic storage to be used, namely a storageclass that refers to a dynamic provisioning strorage such as StorageOS or Portworx, or GCE.\n   Note, when Storage Type is create, you can specify a storage configuration setting of MatchLabels, when set, this will cause a selector of key=value to be added into the PVC, this will let you target specific PV(s) to be matched for this cluster. Note, if a PV does not match the claim request, then the cluster will not start. Users that want to use this feature have to place labels on their PV resources as part of PG cluster creation before creating the PG cluster. For example, users would add a label like this to their PV before they create the PG cluster:\n kubectl label pv somepv myzone=somezone   If you do not specify MatchLabels in the storage configuration, then no match filter is added and any available PV will be used to satisfy the PVC request. This option does not apply to dynamic storage types.\n Example PV creation scripts are provided that add labels to a set of PVs and can be used for testing: $COROOT/pv/create-pv-nfs-labels.sh, in that example, a label of crunchyzone=red is set on a set of PVs to test with. The pgo.yaml includes a storage config named nfsstoragered that when used will demonstrate the label matching. This feature allows you to support n-number of NFS storage configurations and supports spreading a PG cluster across different NFS storage configurations.\n  Overriding Container Resources Configuration Defaults In the pgo.yaml configuration file you have the option to configure a default container resources configuration that when set will add CPU and memory resource limits and requests values into each database container when the container is created.\n You can also override the default value using the --resources-config command flag when creating a new cluster -\n pgo create cluster testcluster --resources-config=large   Note, if you try to allocate more resources than your host or Kube cluster has available then you will see your pods wait in a Pending status. The output from a kubectl describe pod command will show output like this in this event -\n Events: Type Reason Age From Message ---- ------ ---- ---- ------- Warning FailedScheduling 49s (x8 over 1m) default-scheduler No nodes are available that match all of the predicates: Insufficient memory (1).    Overriding Storage Configuration Defaults pgo create cluster testcluster --storage-config=bigdisk   That example will create a cluster and specify a storage configuration of bigdisk to be used for the primary database storage. The replica storage will default to the value of ReplicaStorage as specified in pgo.yaml.\n pgo create cluster testcluster2 --storage-config=fastdisk --replica-storage-config=slowdisk   That example will create a cluster and specify a storage configuration of fastdisk to be used for the primary database storage, while the replica storage will use the storage configuration slowdisk.\n pgo backup testcluster --storage-config=offsitestorage   That example will create a backup and use the offsitestorage storage configuration for persisting the backup.\n  Disaster Recovery Using Storage Configurations A simple mechanism for partial disaster recovery can be obtained by leveraging network storage, Kubernetes storage classes, and the storage configuration options within the Operator.\n For example, if you define a Kubernetes storage class that refers to a storage backend that is running within your disaster recovery site, and then use that storage class as a storage configuration for your backups, you essentially have moved your backup files automatically to your disaster recovery site thanks to network storage.\n     PostgreSQL Operator Container Configuration To enable debug level messages from the operator pod, set the CRUNCHY_DEBUG environment variable to true within its deployment file deployment.json.\n Operator Templates The database and cluster Kubernetes objects that get created by the operator are based on JSON templates that are added into the operator deployment by means of a mounted volume.\n The templates are located in the $COROOT/conf/postgres-operator directory and are added into a config map which is mounted by the operator deployment.\n     Bash Completion There is a bash completion file that is included for users to try located in the repository at examples/pgo-bash-completion. To use it -\n cp $COROOT/examples/pgo-bash-completion /etc/bash_completion.d/pgo su - $USER     REST API Because the apiserver implements a REST API, it is possible to integrate with it using your own application code. To demonstrate this, the following curl commands show the API usage -\n Note: Some setups may require the user to add '?version=x.x' to the end of the commands.\n pgo version\n curl -v -X GET -u readonlyuser:testpass -H \"Content-Type: application/json\" --insecure https://10.101.155.218:8443/version   pgo show policy \u0026lt;name\u0026gt;\n curl -v -X GET -u readonlyuser:testpass -H \"Content-Type: application/json\" --insecure https://10.101.155.218:8443/policies/\u0026lt;name\u0026gt;   pgo delete policy \u0026lt;name\u0026gt;\n curl -v -X GET -u readonlyuser:testpass -H \"Content-Type: application/json\" --insecure https://10.101.155.218:8443/policiesdelete/\u0026lt;name\u0026gt;   pgo show pvc \u0026lt;name\u0026gt;\n curl -v -X GET -u readonlyuser:testpass -H \"Content-Type: application/json\" --insecure https://10.101.155.218:8443/pvc/\u0026lt;name\u0026gt;   pgo apply policy \u0026lt;name\u0026gt;\n curl -v -X POST -u readonlyuser:testpass -H \"Content-Type: application/json\" --insecure https://10.101.155.218:8443/policies/apply/\u0026lt;name\u0026gt;   pgo label\n curl -v -X POST -u readonlyuser:testpass -H \"Content-Type: application/json\" --insecure https://10.101.155.218:8443/label   pgo load\n curl -v -X POST -u readonlyuser:testpass -H \"Content-Type: application/json\" --insecure https://10.101.155.218:8443/load   pgo user\n curl -v -X POST -u readonlyuser:testpass -H \"Content-Type: application/json\" --insecure https://10.101.155.218:8443/user   pgo users \u0026lt;name\u0026gt;\n curl -v -X GET -u readonlyuser:testpass -H \"Content-Type: application/json\" --insecure https://10.101.155.218:8443/users/\u0026lt;name\u0026gt;   pgo delete user \u0026lt;name\u0026gt;\n curl -v -X GET -u readonlyuser:testpass -H \"Content-Type: application/json\" --insecure https://10.101.155.218:8443/usersdelete/\u0026lt;name\u0026gt;   pgo show upgrade \u0026lt;name\u0026gt;\n curl -v -X GET -u readonlyuser:testpass -H \"Content-Type: application/json\" --insecure https://10.101.155.218:8443/upgrades/\u0026lt;name\u0026gt;   pgo delete upgrade \u0026lt;name\u0026gt;\n curl -v -X GET -u readonlyuser:testpass -H \"Content-Type: application/json\" --insecure https://10.101.155.218:8443/upgradesdelete/\u0026lt;name\u0026gt;   pgo show cluster \u0026lt;name\u0026gt;\n curl -v -X GET -u readonlyuser:testpass -H \"Content-Type: application/json\" --insecure https://10.101.155.218:8443/clusters/\u0026lt;name\u0026gt;   pgo delete cluster\n curl -v -X GET -u readonlyuser:testpass -H \"Content-Type: application/json\" --insecure https://10.101.155.218:8443/clustersdelete/\u0026lt;name\u0026gt;   pgo test \u0026lt;name\u0026gt;\n curl -v -X GET -u readonlyuser:testpass -H \"Content-Type: application/json\" --insecure https://10.101.155.218:8443/clusters/test/\u0026lt;name\u0026gt;   pgo scale \u0026lt;name\u0026gt;\n curl -v -X GET -u readonlyuser:testpass -H \"Content-Type: application/json\" --insecure https://10.101.155.218:8443/clusters/scale/\u0026lt;name\u0026gt;     Deploying pgPool One option with pgo is enabling the creation of a pgpool deployment in addition to the PostgreSQL cluster. Running pgpool is a logical inclusion when the Kubernetes cluster includes both a primary database in addition to some number of replicas deployed. The current pgpool configuration deployed by the operator only works when both a primary and a replica are running.\n When a user creates the cluster a command flag can be passed as follows to enable the creation of the pgpool deployment.\n pgo create cluster cluster1 --pgpool pgo scale cluster1   This will cause the operator to create a Deployment that includes the crunchy-pgpool container along with a replica. That container will create a configuration that will perform SQL routing to your cluster services, both for the primary and replica services.\n Pgpool examines the SQL it receives and routes the SQL statement to either the primary or replica based on the SQL action. Specifically, it will send writes and updates to only the primary service. It will send read-only statements to the replica service.\n When the operator deploys the pgpool container, it creates a secret (e.g. mycluster-pgpool-secret) that contains pgpool configuration files. It fills out templated versions of these configuration files specifically for this PostgreSQL cluster.\n Part of the pgpool deployment also includes creating a pool_passwd file that will allow the testuser credential to authenticate to pgpool. Adding additional users to the pgpool configuration currently requires human intervention specifically creating a new pgpool secret and bouncing the pgpool pod to pick up the updated secret. Future operator releases will attempt to provide pgo commands to let you automate the addition or removal of a pgpool user.\n Currently to update a pgpool user within the pool_passwd configuration file, it is necessary to copy the existing files from the secret to your local system, update the credentials in pool_passwd with the new user credentials, recreate the pgpool secret, and finally restart the pgpool pod to pick up the updated configuration files.\n As an example -\n kubectl cp demo/wed10-pgpool-6cc6f6598d-wcnmf:/pgconf/ /tmp/foo   That command gets a running set of secret pgpool configuration files and places them locally on your system for you to edit.\n pgpool requires a specially formatted password credential to be placed into pool_passwd. There is a golang program included in $COROOT/golang-examples/gen-pgpool-pass.go that, when run, will generate the value to use within the pgpool_passwd configuration file.\n go run $COROOT/golang-examples/gen-pgpool-pass.go Enter Username: testuser Enter Password: Password typed: e99Mjt1dLz hash of password is [md59c4017667828b33762665dc4558fbd76]   The value md59c4017667828b33762665dc4558fbd76 is what you will use in the pool_passwd file.\n Then, create the new secrets file based on those updated files -\n $COROOT/bin/create-pgpool-secrets.sh   Lastly for pgpool to pick up the new secret file, delete the existing deployment pod -\n kubectl get deployment wed-pgpool kubectl delete pod wed10-pgpool-6cc6f6598d-wcnmf   The pgpool deployment will spin up another pgpool which will pick up the updated secret file.\n   Storage Configuration Most users after they try out the operator will want to create a more customized installation and deployment of the operator using specific storage types.\n The operator will work with HostPath, NFS, Dynamic, and GKE Storage.\n   NFS   NFS To configure the operator to use NFS for storage, a sample pgo.yaml.nfs file is provided. Overlay the default pgo.yaml file with that file -\n cp $COROOT/examples/pgo.yaml.nfs $COROOT/conf/postgres-operator/pgo.yaml   Then, in your .bashrc file, set the variable CO_NFS_IP to the IP address of your NFS server:\n export CO_NFS_IP=192.168.2.14   Edit the pgo.yaml file to specify the NFS GID that is set for the NFS volume mount you will be using. The default value assumed is nfsnobody as the GID (65534). Update the value to meet your NFS security settings.\n Finally, run the $COROOT/pv/create-pv-nfs.sh script to create persistent volumes based on your NFS settings.\n       Dynamic   Dynamic To configure the operator to use Dynamic Storage classes for storage, a sample pgo.yaml.storageclass file is provided. Overlay the default pgo.yaml file with that file -\n cp $COROOT/examples/pgo.yaml.storageclass $COROOT/conf/postgres-operator/pgo.yaml   Edit the pgo.yaml file to specify the storage class you will be using, the default value assumed is standard which is the name used by default within a GKE Kube cluster deployment. Update the value to match your storage classes.\n Notice that the FsGroup setting is required for most block storage and is set to the value of 26 since the PostgreSQL container runs as UID 26.\n       GKE   GKE Some notes for setting up GKE for the Operator deployment.\n Install Kubectl On your host you will be working from, install the kubectl command -\n https://kubernetes.io/docs/tasks/tools/install-kubectl/\n  GCP   Select your project\n  Create a Kube cluster in that project\n   By default a storage class called standard is created.\n  Install GCloud To access the Kubernetes cluster, install the gcloud utility -\n https://cloud.google.com/sdk/downloads cd google-cloud-sdk ./install.sh    Configure Kubectl for Cluster Access gcloud auth login gcloud container clusters get-credentials jeff-quickstart --zone us-central1-a --project crunchy-dev-test kubectl get storageclass          "
},
{
	"uri": "https://crunchydata.github.io/postgres-operator/latest/installation/deployment/",
	"title": "Deployment",
	"tags": [],
	"description": "",
	"content": "Table of Contents Verify Operator Status Configure pgo Client Verify pgo Client Next Steps   Latest Release: 3.5.0 2018-12-14\n This document details verifying the installation of the PostgreSQL Operator is successful, in addition to detailing some different storage configurations that can be made.\n Verify Operator Status To verify that the operator is deployed and running, run the following:\n kubectl get pod --selector=name=postgres-operator   You should see output similar to this:\n NAME READY STATUS RESTARTS AGE postgres-operator-56598999cd-tbg4w 2/2 Running 0 1m   There are 2 containers in the operator pod, both should be ready as above.\n When you first run the operator, it will create the required CustomResourceDefinitions. You can view these as follows -\n kubectl get crd   The operator creates the following Custom Resource Definitions over time as the associated commands are triggered.\n kubectl get crd NAME AGE pgbackups.cr.client-go.k8s.io 2d pgclusters.cr.client-go.k8s.io 2d pgpolicies.cr.client-go.k8s.io 2d pgreplicas.cr.client-go.k8s.io 2d pgtasks.cr.client-go.k8s.io 2d pgupgrades.cr.client-go.k8s.io 2d   At this point, the server side of the operator is deployed and ready.\n   Configure pgo Client The pgo command line client requires TLS for securing the connection to the operator\u0026#8217;s REST API. This configuration is performed as follows -\n export PGO_CA_CERT=$COROOT/conf/postgres-operator/server.crt export PGO_CLIENT_CERT=$COROOT/conf/postgres-operator/server.crt export PGO_CLIENT_KEY=$COROOT/conf/postgres-operator/server.key   The pgo client uses Basic Authentication to authenticate to the operator REST API. For authentication, add the following .pgouser file to your $HOME -\n echo \"username:password\" \u0026gt; $HOME/.pgouser   Roles are defined in a file called pgorole. This file defines each role and the permissions for that role. By default, two roles are defined as samples -\n pgoadmin pgoreader   This file, moved to your $HOME folder, is optional. These default settings can be adjusted to meet local security requirements.\n The format of this file is as follows -\n rolename: permissionA, permissionB   These are defined in the following file -\n $COROOT/conf/postgres-operator/pgorole   The complete set of permissions is documented in the Configuration document.\n The pgo client needs the URL to connect to the operator.\n Depending on your Kubernetes environment this can be done the following ways.\n   Running Kubernetes Locally   Running Kubernetes Locally If your local host is not set up to resolve Kubernetes Service DNS names, you can specify the operator IP address as follows -\n kubectl get service postgres-operator NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE postgres-operator NodePort 10.109.184.8 \u0026lt;none\u0026gt; 8443:30894/TCP 5m export CO_APISERVER_URL=https://10.109.184.8:8443 pgo version   Alternatively, an alias was set up in the .bashrc file earlier on, as follows:\n alias setip='export CO_APISERVER_URL=https://`kubectl get service postgres-operator -o=jsonpath=\"{.spec.clusterIP}\"`:8443'   This alias (setip) will set the CO_APISERVER_URL IP address for you.\n       Running Kubernetes Remotely   Running Kubernetes Remotely Port forwarding Set up a port-forward tunnel from your host to the Kube remote host, specifying the operator pod -\n kubectl get pod --selector=name=postgres-operator NAME READY STATUS RESTARTS AGE postgres-operator-56598999cd-tbg4w 2/2 Running 0 8m kubectl port-forward postgres-operator-56598999cd-tbg4w 8443:8443   In another terminal -\n export CO_APISERVER_URL=https://127.0.0.1:8443 pgo version    Using an ingress Ingresses allows you to access Kubernetes services throught a controller.\n First you will need to ensure a NGINX Ingress Controller is available in your Kubernetes cluster.\n If you are using Minikube, you can easily deploy one using\n minikube addons enable ingress   If not, please refer to the Nginx Ingress Controller\u0026#8217;s official documentation for its installation.\n Once your controller is running, just deploy the ingress using\n kubectl create -f $COROOT/deploy/ingress.yml   Due to the annotations used, please note this ingress is currently usable only with Nginx Ingress Controller.\n Now you can use the adress IP of the host where the nginx-ingress-controller pod is to connect to the pgo apiserver. The port will be 443 (and not 8443).\n To retrieve the address ip:\n kubectl get ingress postgres-operator -o jsonpath=\"{.status.loadBalancer.ingress[0].ip}\" export CO_APISERVER_URL=https://`kubectl get ingress postgres-operator -o jsonpath=\"{.status.loadBalancer.ingress[0].ip}\"`   If you are using minikube, the address IP displayed is incorrect, just use:\n minikube ip export CO_APISERVER_URL=https://`minikube ip`          Verify pgo Client At this point you should be able to connect to the operator as follows -\n pgo version pgo client version 3.4.0 apiserver version 3.4.0   Operator commands are documented on the Getting Started page.\n   Next Steps There are many ways to configure the operator further. Some sample configurations are documented on the Configuration page.\n You may also want to find out more information on how the operator is designed to work and deploy. This information can be found in the How It Works page.\n Information can be found on the full scope of commands on the Getting Started page.\n   "
},
{
	"uri": "https://crunchydata.github.io/postgres-operator/latest/installation/upgrading-the-operator/",
	"title": "Upgrading the Operator",
	"tags": [],
	"description": "",
	"content": "Latest Release: 3.5.0 2018-12-14\n   Upgrading to v3.5     the pgingest CRD is removed, you will need to manually remove it from any deployments of the operator after upgrading to this version, this includes removing ingest related permissions from the pgorole file, the API server also removes the ingest related API endpoints\n  primary and replica labels are only applicable at cluster creation and not updated after a cluster has executed a failover, a new service-name label is applied to PG cluster components to indidate whether a deployment/pod is a primary or replica, service-name is also the label now used by the cluster services to route with. This scheme allows for an almost immediate failover promotion and avoids the pod having to be bounced as part of a failover. Any existing PG clusters will need to be updated to specify them as a primary or replica using the new service-name labeling scheme. A sample upgrade script is included in the bin directory name upgrade-to-35.sh, you would run this script to upgrade any existing clusters to the new labeling scheme whereby you can run a failover on existing PG clusters deployed prior to 3.5.0.\n  the autofail label was moved from deployments and pods to just the pgcluster CRD to support autofail toggling\n  the storage configurations in pgo.yaml support the MatchLabels attribute for NFS storage, this will allow users to have more than a single NFS backend, when set, this label (key=value) will be used to match the labels on PVs when a PVC is created.\n  the UpdateCluster permission was added to the sample pgorole file to support the new pgo update CLI command, and also added to the pgoperm file\n   \u0026lt;div class=\"expand\"\u0026gt; \u0026lt;div class=\"expand-label\" style=\"cursor: pointer;\" onclick=\"$h = $(this);$h.next('div').slideToggle(100,function () {$h.children('i').attr('class',function () {return $h.next('div').is(':visible') ? 'fa fa-chevron-down' : 'fa fa-chevron-right';});});\"\u0026gt; \u0026lt;i style=\"font-size:x-small;\" class=\"fa fa-chevron-right\"\u0026gt;\u0026lt;/i\u0026gt; \u0026lt;span\u0026gt;\n Upgrading to v3.4    \u0026lt;/span\u0026gt; \u0026lt;/div\u0026gt; \u0026lt;div class=\"expand-content\" style=\"display: none;\"\u0026gt; \u0026lt;div class=\"ulist\"\u0026gt; \u0026lt;ul\u0026gt; \u0026lt;li\u0026gt; \u0026lt;p\u0026gt;The conf/apiserver/ configuration files were moved into the conf/postgres-operator directory to consolidate all config files into a single location. You will need to perform this step manually start with version 3.4.0 if you are running an existing Operator version prior to 3.4.0. The Helm chart is also updated to reflect this change. Starting with 3.4.0 there is a secret, \u0026lt;strong\u0026gt;pgo-auth-secret\u0026lt;/strong\u0026gt; that holds authentication and authorization files used by the operator to authenticate REST clients. Also, the configuration files are stored in a configmap named \u0026lt;strong\u0026gt;pgo-config\u0026lt;/strong\u0026gt;. Existing users will need to update their deploy.sh script and deployment.yaml files to pick up the the new naming conventions.\u0026lt;/p\u0026gt; \u0026lt;/li\u0026gt; \u0026lt;li\u0026gt; \u0026lt;p\u0026gt;new configuration settings were added into pgo.yaml to support resource configuration settings for the various \u0026lt;strong\u0026gt;helper\u0026lt;/strong\u0026gt; containers. The new settings include DefaultLoadResources, DefaultLspvcResources, DefaultRmdataResources, DefaultBackupResources. You will need to add these manually into your existing pgo.yaml file if you want to make use of this feature.\u0026lt;/p\u0026gt; \u0026lt;/li\u0026gt; \u0026lt;li\u0026gt; \u0026lt;p\u0026gt;the pgcluster CRD was changed to remove the password fields, instead secret names are stored in the CRD to avoid having to have passwords in the CRD, the password fields are totally removed starting in this release. No changes are required for existing CRD resources, new CRDs that are created will not have the password fields.\u0026lt;/p\u0026gt; \u0026lt;/li\u0026gt; \u0026lt;li\u0026gt; \u0026lt;p\u0026gt;starting in 3.4, a new operator upgrade process is developed that eventually will handle various forms of automated upgrades depending on user settings and changes to the postgres-operator in between versions. When starting a new Operator it will scan the pgcluster and pgreplica instances and update the pgo-version to match the current operator version, it will also create a user label on the pgreplica/pgcluster to indicate the upgrade date. More advanced upgrade features are planned to be developed.\u0026lt;/p\u0026gt; \u0026lt;/li\u0026gt; \u0026lt;li\u0026gt; \u0026lt;p\u0026gt;in 3.4, the pgo.yaml LogStatement and LogMinDurationStatement settings are present, if not set, defaults are supplied for both. These settings let you define more precisely the degree of Postgres logging for any Postgres clusters created by the Operator. The LogStatement default is 'none' and the LogMinDurationStatement defaults to 60000 (milliseconds). These settings greatly reduce the log file sizes and only will log statements that are longer running that 60000 milliseconds. If you want to see all statements logged, set LogStatement to 'all'.\u0026lt;/p\u0026gt; \u0026lt;/li\u0026gt; \u0026lt;li\u0026gt; \u0026lt;p\u0026gt;in 3.4, the pgbackup CRD includes a new field called BackupOpts, used to hold the pgbasebackup command options which can now be passed in on the CLI backup command, no changes to existing pgbackup CRD resources is required.\u0026lt;/p\u0026gt; \u0026lt;/li\u0026gt; \u0026lt;li\u0026gt; \u0026lt;p\u0026gt;operator 3.3 introduced the alpha version of pgbackrest integration. Now in 3.4, the pgbackrest integration was changed so that users no longer have to specify a custom configuration file using \u0026lt;strong\u0026gt;--custom-config\u0026lt;/strong\u0026gt;.\u0026lt;/p\u0026gt; \u0026lt;/li\u0026gt; \u0026lt;li\u0026gt; \u0026lt;p\u0026gt;the pgo-backrest backup-job is secured with a new service account named pgo-backrest, that SA is now included in the rbac.yaml file and is to be created by an a cluster admin user\u0026lt;/p\u0026gt; \u0026lt;/li\u0026gt; \u0026lt;li\u0026gt; \u0026lt;p\u0026gt;the permission SHOW_WORKFLOW_PERM was created and added to the default pgorole example, this permission lets users view workflow status, workflows are stored as pgtask CRDs\u0026lt;/p\u0026gt; \u0026lt;/li\u0026gt; \u0026lt;/ul\u0026gt; \u0026lt;/div\u0026gt; \u0026lt;div class=\"paragraph\"\u0026gt; \u0026lt;p\u0026gt;With 3.4, there is a global configmap created as part of the deployment process which will serve this same purpose, that is to indicate to the Postgres container that it must allocate the pgbackrest directories within the mounted /backrestrepo volume mount. This means however, that if you specify a global configuration file or specify your own custom configuration that you have to include the pgbackrest.conf file and key within that configmap. The sample global custom configuration map, pgo-custom-pg-config, now includes pgbackrest.conf within it.\u0026lt;/p\u0026gt; \u0026lt;/div\u0026gt; \u0026lt;div class=\"paragraph\"\u0026gt; \u0026lt;p\u0026gt;This new integration only works with pgbackrest v2.6 which is included into crunchy-postgres 2.2.0. This means that to use pgbackrest, you must run crunchy-postgres 2.2.0 or greater which will require users to upgrade the pgo.yaml to use the 2.2.0 CCPImageTag. pgbackrest commands will NOT work with clusters using older Postgres images.\u0026lt;/p\u0026gt; \u0026lt;/div\u0026gt; \u0026lt;div class=\"paragraph\"\u0026gt; \u0026lt;p\u0026gt;\u0026amp;lt;div class=\"expand\"\u0026amp;gt; \u0026amp;lt;div class=\"expand-label\" style=\"cursor: pointer;\" onclick=\"$h = $(this);$h.next('div').slideToggle(100,function () {$h.children('i').attr('class',function () {return $h.next('div').is(':visible') ? 'fa fa-chevron-down' : 'fa fa-chevron-right';});});\"\u0026amp;gt; \u0026amp;lt;i style=\"font-size:x-small;\" class=\"fa fa-chevron-right\"\u0026amp;gt;\u0026amp;lt;/i\u0026amp;gt; \u0026amp;lt;span\u0026amp;gt;\u0026lt;/p\u0026gt; \u0026lt;/div\u0026gt; \u0026lt;div class=\"literalblock\"\u0026gt; \u0026lt;div class=\"content\"\u0026gt; \u0026lt;pre\u0026gt;Upgrading to v3.3\u0026lt;/pre\u0026gt; \u0026lt;/div\u0026gt; \u0026lt;/div\u0026gt; \u0026lt;div class=\"literalblock\"\u0026gt; \u0026lt;div class=\"content\"\u0026gt; \u0026lt;pre\u0026gt; \u0026amp;lt;/span\u0026amp;gt; \u0026amp;lt;/div\u0026amp;gt; \u0026amp;lt;div class=\"expand-content\" style=\"display: none;\"\u0026amp;gt; \u0026amp;lt;div class=\"paragraph\"\u0026amp;gt; \u0026amp;lt;p\u0026amp;gt;The following changes are mandatory when upgrading to 3.3 from previous operator versions:\u0026amp;lt;/p\u0026amp;gt; \u0026amp;lt;/div\u0026amp;gt; \u0026amp;lt;div class=\"ulist\"\u0026amp;gt; \u0026amp;lt;ul\u0026amp;gt; \u0026amp;lt;li\u0026amp;gt; \u0026amp;lt;p\u0026amp;gt;The MatchLabels attribute was added to the pgo.yaml file as an optional storage configuration setting. You do not have to specify this setting, however, the operator-conf ConfigMap now has to include the \u0026amp;lt;code\u0026amp;gt;pvc-matchlabels.json\u0026amp;lt;/code\u0026amp;gt; template file as required by this new feature. If you upgrade to 3.2, you will need to rebuild your \u0026amp;lt;strong\u0026amp;gt;operator-conf\u0026amp;lt;/strong\u0026amp;gt; ConfigMap to include \u0026amp;lt;code\u0026amp;gt;pvc-matchlabels.json\u0026amp;lt;/code\u0026amp;gt; and redeploy the Operator using the new ConfigMap.\u0026amp;lt;/p\u0026amp;gt; \u0026amp;lt;/li\u0026amp;gt; \u0026amp;lt;li\u0026amp;gt; \u0026amp;lt;p\u0026amp;gt;The \u0026amp;lt;code\u0026amp;gt;CCP_IMAGE_PREFIX\u0026amp;lt;/code\u0026amp;gt;, \u0026amp;lt;code\u0026amp;gt;CO_IMAGE_PREFIX\u0026amp;lt;/code\u0026amp;gt;, and \u0026amp;lt;code\u0026amp;gt;CO_IMAGE_TAG\u0026amp;lt;/code\u0026amp;gt; environment variables are now pulled from the \u0026amp;lt;code\u0026amp;gt;pgo.yaml\u0026amp;lt;/code\u0026amp;gt; configuration file that is mounted by both the apiserver and operator containers. To clean up an existing deployment, remove these environment variable definitions from your \u0026amp;lt;code\u0026amp;gt;deployment.yaml\u0026amp;lt;/code\u0026amp;gt; file or Helm chart equivalent.\u0026amp;lt;/p\u0026amp;gt; \u0026amp;lt;/li\u0026amp;gt; \u0026amp;lt;li\u0026amp;gt; \u0026amp;lt;p\u0026amp;gt;The \u0026amp;lt;code\u0026amp;gt;ExternalIP\u0026amp;lt;/code\u0026amp;gt; field was added to the \u0026amp;lt;code\u0026amp;gt;apiservermsgs.ShowClusterService\u0026amp;lt;/code\u0026amp;gt; struct. This field is now passed back to apiserver clients in the REST API when viewing cluster details. For custom clients you might have written, you will see this new field in the REST message.\u0026amp;lt;/p\u0026amp;gt; \u0026amp;lt;/li\u0026amp;gt; \u0026amp;lt;li\u0026amp;gt; \u0026amp;lt;p\u0026amp;gt;Clusters that were created prior to 3.1 will need a new label to be applied. For primary deployments, apply the label \u0026amp;lt;code\u0026amp;gt;primary=true\u0026amp;lt;/code\u0026amp;gt;. For example, \u0026amp;lt;code\u0026amp;gt;kubectl label deploy mycluster primary=true\u0026amp;lt;/code\u0026amp;gt;. For replica deployments, specify \u0026amp;lt;code\u0026amp;gt;primary=false\u0026amp;lt;/code\u0026amp;gt;. For example,\t\u0026amp;lt;code\u0026amp;gt;kubectl label deploy mycluster-xxxx primary=false\u0026amp;lt;/code\u0026amp;gt;.\u0026amp;lt;/p\u0026amp;gt; \u0026amp;lt;/li\u0026amp;gt; \u0026amp;lt;li\u0026amp;gt; \u0026amp;lt;p\u0026amp;gt;The \u0026amp;lt;code\u0026amp;gt;collect.json\u0026amp;lt;/code\u0026amp;gt; template now specifies a pgmonitor credential that must match the \u0026amp;lt;code\u0026amp;gt;PGMONITOR_PASSWORD\u0026amp;lt;/code\u0026amp;gt; environment variable which was added into the \u0026amp;lt;code\u0026amp;gt;cluster-deployment-1.json\u0026amp;lt;/code\u0026amp;gt; template. These changes were required to support crunchy-collect (2.1.0) changes that were introduced. Users should upgrade to \u0026amp;lt;code\u0026amp;gt;crunchy-collect:centos7-10.5-2.1.0\u0026amp;lt;/code\u0026amp;gt; to use this feature. If you do not want to upgrade to this new metrics collector, you will need to retain and reuse the prior version of \u0026amp;lt;code\u0026amp;gt;collect.json\u0026amp;lt;/code\u0026amp;gt; used by the Operator and make sure you deploy that version.\u0026amp;lt;/p\u0026amp;gt; \u0026amp;lt;/li\u0026amp;gt; \u0026amp;lt;/ul\u0026amp;gt; \u0026amp;lt;/div\u0026amp;gt;\u0026lt;/pre\u0026gt; \u0026lt;/div\u0026gt; \u0026lt;/div\u0026gt; \u0026lt;div class=\"literalblock\"\u0026gt; \u0026lt;div class=\"content\"\u0026gt; \u0026lt;pre\u0026gt; \u0026amp;lt;/div\u0026amp;gt; \u0026amp;lt;/div\u0026amp;gt;\u0026lt;/pre\u0026gt; \u0026lt;/div\u0026gt; \u0026lt;/div\u0026gt; \u0026lt;div class=\"paragraph\"\u0026gt; \u0026lt;p\u0026gt;\u0026amp;lt;div class=\"expand\"\u0026amp;gt; \u0026amp;lt;div class=\"expand-label\" style=\"cursor: pointer;\" onclick=\"$h = $(this);$h.next('div').slideToggle(100,function () {$h.children('i').attr('class',function () {return $h.next('div').is(':visible') ? 'fa fa-chevron-down' : 'fa fa-chevron-right';});});\"\u0026amp;gt; \u0026amp;lt;i style=\"font-size:x-small;\" class=\"fa fa-chevron-right\"\u0026amp;gt;\u0026amp;lt;/i\u0026amp;gt; \u0026amp;lt;span\u0026amp;gt;\u0026lt;/p\u0026gt; \u0026lt;/div\u0026gt; \u0026lt;div class=\"literalblock\"\u0026gt; \u0026lt;div class=\"content\"\u0026gt; \u0026lt;pre\u0026gt;Upgrading from v2.6 to v3.1\u0026lt;/pre\u0026gt; \u0026lt;/div\u0026gt; \u0026lt;/div\u0026gt; \u0026lt;div class=\"literalblock\"\u0026gt; \u0026lt;div class=\"content\"\u0026gt; \u0026lt;pre\u0026gt; \u0026amp;lt;/span\u0026amp;gt; \u0026amp;lt;/div\u0026amp;gt; \u0026amp;lt;div class=\"expand-content\" style=\"display: none;\"\u0026amp;gt; \u0026amp;lt;div class=\"paragraph\"\u0026amp;gt; \u0026amp;lt;p\u0026amp;gt;Clusters that were created prior to 3.1 will need a new label to be applied. For primary deployments, apply the label \u0026amp;lt;code\u0026amp;gt;primary=true\u0026amp;lt;/code\u0026amp;gt;. For example, \u0026amp;lt;code\u0026amp;gt;kubectl label deploy mycluster primary=true\u0026amp;lt;/code\u0026amp;gt;. For replica deployments, specify \u0026amp;lt;code\u0026amp;gt;primary=false\u0026amp;lt;/code\u0026amp;gt;. For example,\t\u0026amp;lt;code\u0026amp;gt;kubectl label deploy mycluster-xxxx primary=false\u0026amp;lt;/code\u0026amp;gt;.\u0026amp;lt;/p\u0026amp;gt; \u0026amp;lt;/div\u0026amp;gt;\u0026lt;/pre\u0026gt; \u0026lt;/div\u0026gt; \u0026lt;/div\u0026gt; \u0026lt;div class=\"literalblock\"\u0026gt; \u0026lt;div class=\"content\"\u0026gt; \u0026lt;pre\u0026gt; \u0026amp;lt;/div\u0026amp;gt; \u0026amp;lt;/div\u0026amp;gt;\u0026lt;/pre\u0026gt; \u0026lt;/div\u0026gt; \u0026lt;/div\u0026gt; \u0026lt;div class=\"paragraph\"\u0026gt; \u0026lt;p\u0026gt;\u0026amp;lt;div class=\"expand\"\u0026amp;gt; \u0026amp;lt;div class=\"expand-label\" style=\"cursor: pointer;\" onclick=\"$h = $(this);$h.next('div').slideToggle(100,function () {$h.children('i').attr('class',function () {return $h.next('div').is(':visible') ? 'fa fa-chevron-down' : 'fa fa-chevron-right';});});\"\u0026amp;gt; \u0026amp;lt;i style=\"font-size:x-small;\" class=\"fa fa-chevron-right\"\u0026amp;gt;\u0026amp;lt;/i\u0026amp;gt; \u0026amp;lt;span\u0026amp;gt;\u0026lt;/p\u0026gt; \u0026lt;/div\u0026gt; \u0026lt;div class=\"literalblock\"\u0026gt; \u0026lt;div class=\"content\"\u0026gt; \u0026lt;pre\u0026gt;Upgrading from v2.4 to v2.5\u0026lt;/pre\u0026gt; \u0026lt;/div\u0026gt; \u0026lt;/div\u0026gt; \u0026lt;div class=\"literalblock\"\u0026gt; \u0026lt;div class=\"content\"\u0026gt; \u0026lt;pre\u0026gt; \u0026amp;lt;/span\u0026amp;gt; \u0026amp;lt;/div\u0026amp;gt; \u0026amp;lt;div class=\"expand-content\" style=\"display: none;\"\u0026amp;gt; \u0026amp;lt;div class=\"paragraph\"\u0026amp;gt; \u0026amp;lt;p\u0026amp;gt;For a full list of additions and revisions that occurred in the PostgreSQL Operator v2.5 release, please view the related release page \u0026amp;lt;a href=\"https://github.com/CrunchyData/postgres-operator/releases/tag/2.5\"\u0026amp;gt;here\u0026amp;lt;/a\u0026amp;gt;.\u0026amp;lt;/p\u0026amp;gt; \u0026amp;lt;/div\u0026amp;gt; \u0026amp;lt;div class=\"sect1\"\u0026amp;gt; \u0026amp;lt;h2 id=\"_required_updates\"\u0026amp;gt;Required Updates\u0026amp;lt;/h2\u0026amp;gt; \u0026amp;lt;div class=\"sectionbody\"\u0026amp;gt; \u0026amp;lt;div class=\"paragraph\"\u0026amp;gt; \u0026amp;lt;p\u0026amp;gt;This section notes some required steps that will need to be taken in the process of upgrading from v2.4 to v2.5.\u0026amp;lt;/p\u0026amp;gt; \u0026amp;lt;/div\u0026amp;gt; \u0026amp;lt;div class=\"sect2\"\u0026amp;gt; \u0026amp;lt;h3 id=\"_configuration_file\"\u0026amp;gt;Configuration File\u0026amp;lt;/h3\u0026amp;gt; \u0026amp;lt;div class=\"paragraph\"\u0026amp;gt; \u0026amp;lt;p\u0026amp;gt;It will be necessary to update your existing \u0026amp;lt;code\u0026amp;gt;pgo.yaml\u0026amp;lt;/code\u0026amp;gt; configuration file where the Storage Configuration sections are concerned. The updated file for v2.5 can be found \u0026amp;lt;a href=\"https://github.com/CrunchyData/postgres-operator/blob/2.5/conf/apiserver/pgo.yaml\"\u0026amp;gt;here\u0026amp;lt;/a\u0026amp;gt;. The file contained within the local installation of the Operator is located by default in the following location -\u0026amp;lt;/p\u0026amp;gt; \u0026amp;lt;/div\u0026amp;gt; \u0026amp;lt;div class=\"literalblock\"\u0026amp;gt; \u0026amp;lt;div class=\"content\"\u0026amp;gt; \u0026amp;lt;pre\u0026amp;gt;$COROOT/conf/apiserver/pgo.yaml\u0026amp;lt;/pre\u0026amp;gt; \u0026amp;lt;/div\u0026amp;gt; \u0026amp;lt;/div\u0026amp;gt; \u0026amp;lt;/div\u0026amp;gt; \u0026amp;lt;div class=\"sect2\"\u0026amp;gt; \u0026amp;lt;h3 id=\"_secrets\"\u0026amp;gt;Secrets\u0026amp;lt;/h3\u0026amp;gt; \u0026amp;lt;div class=\"paragraph\"\u0026amp;gt; \u0026amp;lt;p\u0026amp;gt;2.5 changed the names of the database credentials that are created by default in order to be consistent with the way new database credentials are named.\u0026amp;lt;/p\u0026amp;gt; \u0026amp;lt;/div\u0026amp;gt; \u0026amp;lt;div class=\"paragraph\"\u0026amp;gt; \u0026amp;lt;p\u0026amp;gt;It will be necessary to run the following script to update your existing clusters. This script will essentially copy the existing secrets values and create new secrets with those same values but named to the new standard. Run the script by passing in the name of an existing cluster as a parameter.\u0026amp;lt;/p\u0026amp;gt; \u0026amp;lt;/div\u0026amp;gt; \u0026amp;lt;div class=\"literalblock\"\u0026amp;gt; \u0026amp;lt;div class=\"content\"\u0026amp;gt; \u0026amp;lt;pre\u0026amp;gt;$COROOT/bin/upgrade-secret.sh\u0026amp;lt;/pre\u0026amp;gt; \u0026amp;lt;/div\u0026amp;gt; \u0026amp;lt;/div\u0026amp;gt; \u0026amp;lt;/div\u0026amp;gt; \u0026amp;lt;/div\u0026amp;gt; \u0026amp;lt;/div\u0026amp;gt;\u0026lt;/pre\u0026gt; \u0026lt;/div\u0026gt; \u0026lt;/div\u0026gt; \u0026lt;div class=\"literalblock\"\u0026gt; \u0026lt;div class=\"content\"\u0026gt; \u0026lt;pre\u0026gt; \u0026amp;lt;/div\u0026amp;gt; \u0026amp;lt;/div\u0026amp;gt;\u0026lt;/pre\u0026gt; \u0026lt;/div\u0026gt; \u0026lt;/div\u0026gt; \u0026lt;div class=\"paragraph\"\u0026gt; \u0026lt;p\u0026gt;\u0026amp;lt;div class=\"expand\"\u0026amp;gt; \u0026amp;lt;div class=\"expand-label\" style=\"cursor: pointer;\" onclick=\"$h = $(this);$h.next('div').slideToggle(100,function () {$h.children('i').attr('class',function () {return $h.next('div').is(':visible') ? 'fa fa-chevron-down' : 'fa fa-chevron-right';});});\"\u0026amp;gt; \u0026amp;lt;i style=\"font-size:x-small;\" class=\"fa fa-chevron-right\"\u0026amp;gt;\u0026amp;lt;/i\u0026amp;gt; \u0026amp;lt;span\u0026amp;gt;\u0026lt;/p\u0026gt; \u0026lt;/div\u0026gt; \u0026lt;div class=\"literalblock\"\u0026gt; \u0026lt;div class=\"content\"\u0026gt; \u0026lt;pre\u0026gt;Upgrading from v2.5 to v2.6\u0026lt;/pre\u0026gt; \u0026lt;/div\u0026gt; \u0026lt;/div\u0026gt; \u0026lt;div class=\"literalblock\"\u0026gt; \u0026lt;div class=\"content\"\u0026gt; \u0026lt;pre\u0026gt; \u0026amp;lt;/span\u0026amp;gt; \u0026amp;lt;/div\u0026amp;gt; \u0026amp;lt;div class=\"expand-content\" style=\"display: none;\"\u0026amp;gt; \u0026amp;lt;div class=\"paragraph\"\u0026amp;gt; \u0026amp;lt;p\u0026amp;gt;For a full list of additions and revisions that occurred in the PostgreSQL Operator v2.5 release, please view the related release page \u0026amp;lt;a href=\"https://github.com/CrunchyData/postgres-operator/releases/tag/3.3.0\"\u0026amp;gt;here\u0026amp;lt;/a\u0026amp;gt;.\u0026amp;lt;/p\u0026amp;gt; \u0026amp;lt;/div\u0026amp;gt; \u0026amp;lt;div class=\"sect1\"\u0026amp;gt; \u0026amp;lt;h2 id=\"_required_updates\"\u0026amp;gt;Required Updates\u0026amp;lt;/h2\u0026amp;gt; \u0026amp;lt;div class=\"sectionbody\"\u0026amp;gt; \u0026amp;lt;div class=\"paragraph\"\u0026amp;gt; \u0026amp;lt;p\u0026amp;gt;This section notes some required steps that will need to be taken in the process of upgrading from v2.5 to v2.6.\u0026amp;lt;/p\u0026amp;gt; \u0026amp;lt;/div\u0026amp;gt; \u0026amp;lt;div class=\"sect2\"\u0026amp;gt; \u0026amp;lt;h3 id=\"_configuration_file\"\u0026amp;gt;Configuration File\u0026amp;lt;/h3\u0026amp;gt; \u0026amp;lt;div class=\"paragraph\"\u0026amp;gt; \u0026amp;lt;p\u0026amp;gt;One update in v2.6 changed the \u0026amp;lt;code\u0026amp;gt;pgo.yaml\u0026amp;lt;/code\u0026amp;gt; file through removing the Debug flag. The \u0026amp;lt;code\u0026amp;gt;Pgo.Debug\u0026amp;lt;/code\u0026amp;gt; variable can now be removed from the \u0026amp;lt;code\u0026amp;gt;pgo.yaml\u0026amp;lt;/code\u0026amp;gt; file as a result. The debug flag is now called \u0026amp;lt;code\u0026amp;gt;CRUNCHY_DEBUG\u0026amp;lt;/code\u0026amp;gt; and is set in the \u0026amp;lt;code\u0026amp;gt;deployment.json\u0026amp;lt;/code\u0026amp;gt; file as a default environment variable.\u0026amp;lt;/p\u0026amp;gt; \u0026amp;lt;/div\u0026amp;gt; \u0026amp;lt;/div\u0026amp;gt; \u0026amp;lt;div class=\"sect2\"\u0026amp;gt; \u0026amp;lt;h3 id=\"_container_resources\"\u0026amp;gt;Container Resources\u0026amp;lt;/h3\u0026amp;gt; \u0026amp;lt;div class=\"paragraph\"\u0026amp;gt; \u0026amp;lt;p\u0026amp;gt;Release 2.6 added the concept of container resource configurations to the \u0026amp;lt;code\u0026amp;gt;pgo.yaml\u0026amp;lt;/code\u0026amp;gt; file. In order to specify the optional container resource configurations, add a section as follows to your \u0026amp;lt;code\u0026amp;gt;pgo.yaml\u0026amp;lt;/code\u0026amp;gt; file -\u0026amp;lt;/p\u0026amp;gt; \u0026amp;lt;/div\u0026amp;gt; \u0026amp;lt;div class=\"literalblock\"\u0026amp;gt; \u0026amp;lt;div class=\"content\"\u0026amp;gt; \u0026amp;lt;pre\u0026amp;gt;DefaultContainerResource: small ContainerResources: small: RequestsMemory: 2Gi RequestsCPU: 0.5 LimitsMemory: 2Gi LimitsCPU: 1.0 large: RequestsMemory: 8Gi RequestsCPU: 2.0 LimitsMemory: 12Gi LimitsCPU: 4.0\u0026amp;lt;/pre\u0026amp;gt; \u0026amp;lt;/div\u0026amp;gt; \u0026amp;lt;/div\u0026amp;gt; \u0026amp;lt;div class=\"paragraph\"\u0026amp;gt; \u0026amp;lt;p\u0026amp;gt;If these settings are set incorrectly or if the Kubernetes cluster cannot meet the defined memory and CPU requirements, deployments will go into a \u0026amp;lt;strong\u0026amp;gt;pending\u0026amp;lt;/strong\u0026amp;gt; state.\u0026amp;lt;/p\u0026amp;gt; \u0026amp;lt;/div\u0026amp;gt; \u0026amp;lt;/div\u0026amp;gt; \u0026amp;lt;div class=\"sect2\"\u0026amp;gt; \u0026amp;lt;h3 id=\"_kube_rbac\"\u0026amp;gt;Kube RBAC\u0026amp;lt;/h3\u0026amp;gt; \u0026amp;lt;div class=\"paragraph\"\u0026amp;gt; \u0026amp;lt;p\u0026amp;gt;Release 2.6 added a \u0026amp;lt;code\u0026amp;gt;rbac.yaml\u0026amp;lt;/code\u0026amp;gt; file to capture the Kube RBAC rules. These RBAC rules allow the \u0026amp;lt;strong\u0026amp;gt;apiserver\u0026amp;lt;/strong\u0026amp;gt; and \u0026amp;lt;strong\u0026amp;gt;postgres-operator\u0026amp;lt;/strong\u0026amp;gt; containers access to the Kubernetes resources required for the operator to work. As part of the deployment process, it is necessary to execute the \u0026amp;lt;code\u0026amp;gt;rbac.yaml\u0026amp;lt;/code\u0026amp;gt; file to set the roles and bindings required by the operator. Adjust this file to suit local security requirements.\u0026amp;lt;/p\u0026amp;gt; \u0026amp;lt;/div\u0026amp;gt; \u0026amp;lt;/div\u0026amp;gt; \u0026amp;lt;div class=\"sect2\"\u0026amp;gt; \u0026amp;lt;h3 id=\"_application_rbac\"\u0026amp;gt;Application RBAC\u0026amp;lt;/h3\u0026amp;gt; \u0026amp;lt;div class=\"paragraph\"\u0026amp;gt; \u0026amp;lt;p\u0026amp;gt;Release 2.6 added an RBAC capability to secure the \u0026amp;lt;strong\u0026amp;gt;pgo\u0026amp;lt;/strong\u0026amp;gt; application. The \u0026amp;lt;strong\u0026amp;gt;pgouser\u0026amp;lt;/strong\u0026amp;gt; now has a role appended at the end of of each user definition as follows -\u0026amp;lt;/p\u0026amp;gt; \u0026amp;lt;/div\u0026amp;gt; \u0026amp;lt;div class=\"literalblock\"\u0026amp;gt; \u0026amp;lt;div class=\"content\"\u0026amp;gt; \u0026amp;lt;pre\u0026amp;gt;username:password:pgoadmin testuser:testpass:pgoadmin readonlyuser:testpass:pgoreader\u0026amp;lt;/pre\u0026amp;gt; \u0026amp;lt;/div\u0026amp;gt; \u0026amp;lt;/div\u0026amp;gt; \u0026amp;lt;div class=\"paragraph\"\u0026amp;gt; \u0026amp;lt;p\u0026amp;gt;These are defined in the following file -\u0026amp;lt;/p\u0026amp;gt; \u0026amp;lt;/div\u0026amp;gt; \u0026amp;lt;div class=\"literalblock\"\u0026amp;gt; \u0026amp;lt;div class=\"content\"\u0026amp;gt; \u0026amp;lt;pre\u0026amp;gt;$COROOT/conf/apiserver/pgouser\u0026amp;lt;/pre\u0026amp;gt; \u0026amp;lt;/div\u0026amp;gt; \u0026amp;lt;/div\u0026amp;gt; \u0026amp;lt;div class=\"paragraph\"\u0026amp;gt; \u0026amp;lt;p\u0026amp;gt;To match the behavior of the pre 2.6 releases, the \u0026amp;lt;strong\u0026amp;gt;pgadmin\u0026amp;lt;/strong\u0026amp;gt; role is set on the previous user definitions, but a \u0026amp;lt;strong\u0026amp;gt;readonlyuser\u0026amp;lt;/strong\u0026amp;gt; is now defined to test other role definitions. The roles are defined in a new file called \u0026amp;lt;strong\u0026amp;gt;pgorole\u0026amp;lt;/strong\u0026amp;gt;. This file defines each role and the permissions for that role. By default, two roles are defined as samples -\u0026amp;lt;/p\u0026amp;gt; \u0026amp;lt;/div\u0026amp;gt; \u0026amp;lt;div class=\"literalblock\"\u0026amp;gt; \u0026amp;lt;div class=\"content\"\u0026amp;gt; \u0026amp;lt;pre\u0026amp;gt;pgoadmin pgoreader\u0026amp;lt;/pre\u0026amp;gt; \u0026amp;lt;/div\u0026amp;gt; \u0026amp;lt;/div\u0026amp;gt; \u0026amp;lt;div class=\"paragraph\"\u0026amp;gt; \u0026amp;lt;p\u0026amp;gt;Adjust these default settings to meet local security requirements.\u0026amp;lt;/p\u0026amp;gt; \u0026amp;lt;/div\u0026amp;gt; \u0026amp;lt;div class=\"paragraph\"\u0026amp;gt; \u0026amp;lt;p\u0026amp;gt;The format of this file is as follows -\u0026amp;lt;/p\u0026amp;gt; \u0026amp;lt;/div\u0026amp;gt; \u0026amp;lt;div class=\"literalblock\"\u0026amp;gt; \u0026amp;lt;div class=\"content\"\u0026amp;gt; \u0026amp;lt;pre\u0026amp;gt;rolename: permissionA, permissionB\u0026amp;lt;/pre\u0026amp;gt; \u0026amp;lt;/div\u0026amp;gt; \u0026amp;lt;/div\u0026amp;gt; \u0026amp;lt;div class=\"paragraph\"\u0026amp;gt; \u0026amp;lt;p\u0026amp;gt;These are defined in the following file -\u0026amp;lt;/p\u0026amp;gt; \u0026amp;lt;/div\u0026amp;gt; \u0026amp;lt;div class=\"literalblock\"\u0026amp;gt; \u0026amp;lt;div class=\"content\"\u0026amp;gt; \u0026amp;lt;pre\u0026amp;gt;$COROOT/conf/apiserver/pgorole\u0026amp;lt;/pre\u0026amp;gt; \u0026amp;lt;/div\u0026amp;gt; \u0026amp;lt;/div\u0026amp;gt; \u0026amp;lt;div class=\"paragraph\"\u0026amp;gt; \u0026amp;lt;p\u0026amp;gt;The complete set of permissions is documented in the \u0026amp;lt;a href=\"/installation/configuration/\"\u0026amp;gt;Configuration\u0026amp;lt;/a\u0026amp;gt; document.\u0026amp;lt;/p\u0026amp;gt; \u0026amp;lt;/div\u0026amp;gt; \u0026amp;lt;/div\u0026amp;gt; \u0026amp;lt;div class=\"sect2\"\u0026amp;gt; \u0026amp;lt;h3 id=\"_user_creation\"\u0026amp;gt;User Creation\u0026amp;lt;/h3\u0026amp;gt; \u0026amp;lt;div class=\"paragraph\"\u0026amp;gt; \u0026amp;lt;p\u0026amp;gt;Release 2.6 replaced the \u0026amp;lt;code\u0026amp;gt;pgo user --add\u0026amp;lt;/code\u0026amp;gt; command with the \u0026amp;lt;code\u0026amp;gt;pgo create user\u0026amp;lt;/code\u0026amp;gt; command to improve consistency across command usage. Any scripts written using the older style of command require an update to use the new command syntax.\u0026amp;lt;/p\u0026amp;gt; \u0026amp;lt;/div\u0026amp;gt; \u0026amp;lt;/div\u0026amp;gt; \u0026amp;lt;div class=\"sect2\"\u0026amp;gt; \u0026amp;lt;h3 id=\"_replica_crd\"\u0026amp;gt;Replica CRD\u0026amp;lt;/h3\u0026amp;gt; \u0026amp;lt;div class=\"paragraph\"\u0026amp;gt; \u0026amp;lt;p\u0026amp;gt;There is a new Kubernetes Custom Resource Definition that serves the purpose of holding replica information, called \u0026amp;lt;strong\u0026amp;gt;pgreplicas\u0026amp;lt;/strong\u0026amp;gt;. This CRD is populated with the pgo scale command and is used to hold per-replica specific information such as the resource and storage configurations requested at run time.\u0026amp;lt;/p\u0026amp;gt; \u0026amp;lt;/div\u0026amp;gt; \u0026amp;lt;/div\u0026amp;gt; \u0026amp;lt;/div\u0026amp;gt; \u0026amp;lt;/div\u0026amp;gt;\u0026lt;/pre\u0026gt; \u0026lt;/div\u0026gt; \u0026lt;/div\u0026gt; \u0026lt;div class=\"literalblock\"\u0026gt; \u0026lt;div class=\"content\"\u0026gt; \u0026lt;pre\u0026gt; \u0026amp;lt;/div\u0026amp;gt; \u0026amp;lt;/div\u0026amp;gt;\u0026lt;/pre\u0026gt; \u0026lt;/div\u0026gt; \u0026lt;/div\u0026gt;    \u0026lt;/div\u0026gt; \u0026lt;/div\u0026gt;      "
},
{
	"uri": "https://crunchydata.github.io/postgres-operator/latest/",
	"title": "Crunchy Data PostgreSQL Operator",
	"tags": [],
	"description": "",
	"content": "  Latest Release: 3.4.0 2018-12-14\n Documentation Please view the official Crunchy Data PostgreSQL Operator documentation here. If you are interested in contributing or making an update to the documentation, please view the Contributing Guidelines.\n   What is the Operator? The postgres-operator is a controller that runs within a Kubernetes cluster that provides a means to deploy and manage PostgreSQL clusters.\n Use the postgres-operator to -\n   deploy PostgreSQL containers including streaming replication clusters\n  scale up PostgreSQL clusters with extra replicas\n  add pgpool and metrics sidecars to PostgreSQL clusters\n  apply SQL policies to PostgreSQL clusters\n  assign metadata tags to PostgreSQL clusters\n  maintain PostgreSQL users and passwords\n  perform minor and major upgrades to PostgreSQL clusters\n  load simple CSV and JSON files into PostgreSQL clusters\n  perform database backups\n     Design The postgres-operator design incorporates the following concepts -\n   adds Custom Resource Definitions for PostgreSQL to Kubernetes\n  adds controller logic that watches events on PostgreSQL resources\n  provides a command line client (pgo) and REST API for interfacing with the postgres-operator\n  provides for very customized deployments including container resources, storage configurations, and PostgreSQL custom configurations\n   More design information is found on the How It Works page.\n   Requirements The postgres-operator runs on any Kubernetes and Openshift platform that supports Custom Resource Definitions.\n The Operator project builds and operates with the following containers -\n   PVC Listing Container\n  Remove Data Container\n  postgres-operator Container\n  apiserver Container\n  file load Container\n  backrest interface Container\n   This Operator is developed and tested on the following operating systems but is known to run on other operating systems -\n   CentOS 7\n  RHEL 7\n     Installation To build and deploy the Operator on your Kubernetes system, follow the instructions documented on the Installation page.\n If you\u0026#8217;re seeking to upgrade your existing Operator installation, please visit the Upgrading the Operator page.\n   Configuration The operator is template-driven; this makes it simple to configure both the client and the operator. The configuration options are documented on the Configuration page.\n   Getting Started postgres-operator commands are documented on the Getting Started page.\n   "
},
{
	"uri": "https://crunchydata.github.io/postgres-operator/latest/categories/",
	"title": "Categories",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "https://crunchydata.github.io/postgres-operator/latest/tags/",
	"title": "Tags",
	"tags": [],
	"description": "",
	"content": ""
}]